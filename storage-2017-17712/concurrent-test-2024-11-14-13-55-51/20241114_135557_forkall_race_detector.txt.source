vmlinux map is loaded
Waiting for data race records

('Analyed ', 500, 14352, ' data races in total')
('Analyed ', 1000, 14352, ' data races in total')
('Analyed ', 1500, 14352, ' data races in total')
('Analyed ', 2000, 14352, ' data races in total')
('Analyed ', 2500, 14352, ' data races in total')
('Analyed ', 3000, 14352, ' data races in total')
('Analyed ', 3500, 14352, ' data races in total')
('Analyed ', 4000, 14352, ' data races in total')
('Analyed ', 4500, 14352, ' data races in total')
('Analyed ', 5000, 14352, ' data races in total')
('Analyed ', 5500, 14352, ' data races in total')
('Analyed ', 6000, 14352, ' data races in total')
('Analyed ', 6500, 14352, ' data races in total')
('Analyed ', 7000, 14352, ' data races in total')
('Analyed ', 7500, 14352, ' data races in total')
('Analyed ', 8000, 14352, ' data races in total')
('Analyed ', 8500, 14352, ' data races in total')
('Analyed ', 9000, 14352, ' data races in total')
('Analyed ', 9500, 14352, ' data races in total')
('Analyed ', 10000, 14352, ' data races in total')
('Analyed ', 10500, 14352, ' data races in total')
('Analyed ', 11000, 14352, ' data races in total')
('Analyed ', 11500, 14352, ' data races in total')
('Analyed ', 12000, 14352, ' data races in total')
('Analyed ', 12500, 14352, ' data races in total')
('Analyed ', 13000, 14352, ' data races in total')
('Analyed ', 13500, 14352, ' data races in total')
('Analyed ', 14000, 14352, ' data races in total')

====================================================================================================================================================================================
Total: 6	Addresses: c189b4a4 c10e9570 c10e9b6e
1	0xc189b4a4: copy_page_to_iter at /root/2017-17712-i386/linux-4.14/lib/iov_iter.c:701
2	0xc10e955e: finish_wait at /root/2017-17712-i386/linux-4.14/kernel/sched/wait.c:372
3	0xc10e9b59: autoremove_wake_function at /root/2017-17712-i386/linux-4.14/kernel/sched/wait.c:382

1	c189b4a4 copy_page_to_iter T: trace_20241114_141318_3_3_7.txt S: 7 I1: 3 I2: 3 IP1: c189b4a4 IP2: c10e9b6e PMA1: 31629ea4 PMA2: 31629ea4 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 133219 IC2: 125815
2	c10e9570 list_empty_careful T: trace_20241114_141318_3_3_7.txt S: 7 I1: 3 I2: 3 IP1: c10e9570 IP2: c10e9b6e PMA1: 31629ea4 PMA2: 31629ea4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 133157 IC2: 125815
3	c10e9b6e INIT_LIST_HEAD T: trace_20241114_141318_3_3_7.txt S: 7 I1: 3 I2: 3 IP1: c189b4a4 IP2: c10e9b6e PMA1: 31629ea4 PMA2: 31629ea4 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 133219 IC2: 125815

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//lib/iov_iter.c:701
       681		)
       682	
       683		iov_iter_advance(i, bytes);
       684		return true;
       685	}
       686	EXPORT_SYMBOL(_copy_from_iter_full_nocache);
       687	
       688	static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
       689	{
       690		struct page *head = compound_head(page);
       691		size_t v = n + offset + page_address(page) - page_address(head);
       692	
       693		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       694			return true;
       695		WARN_ON(1);
       696		return false;
       697	}
       698	
       699	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       700				 struct iov_iter *i)
==>    701	{       
       702		if (unlikely(!page_copy_sane(page, offset, bytes)))
       703			return 0;
       704		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       705			void *kaddr = kmap_atomic(page);
       706			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       707			kunmap_atomic(kaddr);
       708			return wanted;
       709		} else if (likely(!(i->type & ITER_PIPE)))
       710			return copy_page_to_iter_iovec(page, offset, bytes, i);
       711		else
       712			return copy_page_to_iter_pipe(page, offset, bytes, i);
       713	}
       714	EXPORT_SYMBOL(copy_page_to_iter);
       715	
       716	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       717				 struct iov_iter *i)
       718	{
       719		if (unlikely(!page_copy_sane(page, offset, bytes)))
       720			return 0;
       721		if (unlikely(i->type & ITER_PIPE)) {

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/wait.c:372
       352	
       353		__set_current_state(TASK_RUNNING);
       354		/*
       355		 * We can check for list emptiness outside the lock
       356		 * IFF:
       357		 *  - we use the "careful" check that verifies both
       358		 *    the next and prev pointers, so that there cannot
       359		 *    be any half-pending updates in progress on other
       360		 *    CPU's that we haven't seen yet (and that might
       361		 *    still change the stack area.
       362		 * and
       363		 *  - all other users take the lock (ie we can only
       364		 *    have _one_ other CPU that looks at or modifies
       365		 *    the list).
       366		 */
       367		if (!list_empty_careful(&wq_entry->entry)) {
       368			spin_lock_irqsave(&wq_head->lock, flags);
       369			list_del_init(&wq_entry->entry);
       370			spin_unlock_irqrestore(&wq_head->lock, flags);
       371		}
==>    372	}       
       373	EXPORT_SYMBOL(finish_wait);
       374	
       375	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       376	{
       377		int ret = default_wake_function(wq_entry, mode, sync, key);
       378	
       379		if (ret)
       380			list_del_init(&wq_entry->entry);
       381		return ret;
       382	}
       383	EXPORT_SYMBOL(autoremove_wake_function);
       384	
       385	static inline bool is_kthread_should_stop(void)
       386	{
       387		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       388	}
       389	
       390	/*
       391	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       392	 *

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/wait.c:382
       362		 * and
       363		 *  - all other users take the lock (ie we can only
       364		 *    have _one_ other CPU that looks at or modifies
       365		 *    the list).
       366		 */
       367		if (!list_empty_careful(&wq_entry->entry)) {
       368			spin_lock_irqsave(&wq_head->lock, flags);
       369			list_del_init(&wq_entry->entry);
       370			spin_unlock_irqrestore(&wq_head->lock, flags);
       371		}
       372	}
       373	EXPORT_SYMBOL(finish_wait);
       374	
       375	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       376	{
       377		int ret = default_wake_function(wq_entry, mode, sync, key);
       378	
       379		if (ret)
       380			list_del_init(&wq_entry->entry);
       381		return ret;
==>    382	}       
       383	EXPORT_SYMBOL(autoremove_wake_function);
       384	
       385	static inline bool is_kthread_should_stop(void)
       386	{
       387		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       388	}
       389	
       390	/*
       391	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       392	 *
       393	 * add_wait_queue(&wq_head, &wait);
       394	 * for (;;) {
       395	 *     if (condition)
       396	 *         break;
       397	 *
       398	 *     p->state = mode;				condition = true;
       399	 *     smp_mb(); // A				smp_wmb(); // C
       400	 *     if (!wq_entry->flags & WQ_FLAG_WOKEN)	wq_entry->flags |= WQ_FLAG_WOKEN;
       401	 *         schedule()				try_to_wake_up();
       402	 *     p->state = TASK_RUNNING;		    ~~~~~~~~~~~~~~~~~~


====================================================================================================================================================================================
Total: 6	Addresses: c111570d c1112c36
3	0xc111570d: raw_write_seqcount_end at /root/2017-17712-i386/linux-4.14/./include/linux/seqlock.h:235
3	0xc1112c30: ktime_get at /root/2017-17712-i386/linux-4.14/kernel/time/timekeeping.c:755

3	c111570d raw_write_seqcount_end T: trace_20241114_141519_3_3_53.txt S: 53 I1: 3 I2: 3 IP1: c111570d IP2: c1112c36 PMA1: 328c680 PMA2: 328c680 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 353888 IC2: 353497
3	c1112c36 __read_once_size T: trace_20241114_141519_3_3_53.txt S: 53 I1: 3 I2: 3 IP1: c111570d IP2: c1112c36 PMA1: 328c680 PMA2: 328c680 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 353888 IC2: 353497

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/seqlock.h:235
       215	 * If the critical section was invalid, it must be ignored (and typically
       216	 * retried).
       217	 */
       218	static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
       219	{
       220		smp_rmb();
       221		return __read_seqcount_retry(s, start);
       222	}
       223	
       224	
       225	
       226	static inline void raw_write_seqcount_begin(seqcount_t *s)
       227	{
       228		s->sequence++;
       229		smp_wmb();
       230	}
       231	
       232	static inline void raw_write_seqcount_end(seqcount_t *s)
       233	{
       234		smp_wmb();
==>    235		s->sequence++;       
       236	}
       237	
       238	/**
       239	 * raw_write_seqcount_barrier - do a seq write barrier
       240	 * @s: pointer to seqcount_t
       241	 *
       242	 * This can be used to provide an ordering guarantee instead of the
       243	 * usual consistency guarantee. It is one wmb cheaper, because we can
       244	 * collapse the two back-to-back wmb()s.
       245	 *
       246	 *      seqcount_t seq;
       247	 *      bool X = true, Y = false;
       248	 *
       249	 *      void read(void)
       250	 *      {
       251	 *              bool x, y;
       252	 *
       253	 *              do {
       254	 *                      int s = read_seqcount_begin(&seq);
       255	 *

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/time/timekeeping.c:755
       735	
       736	/**
       737	 * getnstimeofday64 - Returns the time of day in a timespec64.
       738	 * @ts:		pointer to the timespec64 to be set
       739	 *
       740	 * Returns the time of day in a timespec64 (WARN if suspended).
       741	 */
       742	void getnstimeofday64(struct timespec64 *ts)
       743	{
       744		WARN_ON(__getnstimeofday64(ts));
       745	}
       746	EXPORT_SYMBOL(getnstimeofday64);
       747	
       748	ktime_t ktime_get(void)
       749	{
       750		struct timekeeper *tk = &tk_core.timekeeper;
       751		unsigned int seq;
       752		ktime_t base;
       753		u64 nsecs;
       754	
==>    755		WARN_ON(timekeeping_suspended);       
       756	
       757		do {
       758			seq = read_seqcount_begin(&tk_core.seq);
       759			base = tk->tkr_mono.base;
       760			nsecs = timekeeping_get_ns(&tk->tkr_mono);
       761	
       762		} while (read_seqcount_retry(&tk_core.seq, seq));
       763	
       764		return ktime_add_ns(base, nsecs);
       765	}
       766	EXPORT_SYMBOL_GPL(ktime_get);
       767	
       768	u32 ktime_get_resolution_ns(void)
       769	{
       770		struct timekeeper *tk = &tk_core.timekeeper;
       771		unsigned int seq;
       772		u32 nsecs;
       773	
       774		WARN_ON(timekeeping_suspended);
       775	


====================================================================================================================================================================================
Total: 8	Addresses: c1122237 8057cb0 805de35
2	8057cb0 Not found
2	805de35 Not found
4	0xc1122237: get_futex_value_locked at /root/2017-17712-i386/linux-4.14/kernel/futex.c:773

2	8057cb0 Not found T: trace_20241114_141344_3_3_31.txt S: 31 I1: 3 I2: 3 IP1: 8057cb0 IP2: c1122237 PMA1: 6de34000 PMA2: 6de34000 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 149450 IC2: 124379
2	805de35 Not found T: trace_20241114_141344_3_3_31.txt S: 31 I1: 3 I2: 3 IP1: 805de35 IP2: c1122237 PMA1: 6de34000 PMA2: 6de34000 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 149459 IC2: 124379
4	c1122237 get_futex_value_locked T: trace_20241114_141344_3_3_31.txt S: 31 I1: 3 I2: 3 IP1: 805de35 IP2: c1122237 PMA1: 6de34000 PMA2: 6de34000 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 149459 IC2: 124379

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/futex.c:773
       753		return NULL;
       754	}
       755	
       756	static int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,
       757					      u32 uval, u32 newval)
       758	{
       759		int ret;
       760	
       761		pagefault_disable();
       762		ret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);
       763		pagefault_enable();
       764	
       765		return ret;
       766	}
       767	
       768	static int get_futex_value_locked(u32 *dest, u32 __user *from)
       769	{
       770		int ret;
       771	
       772		pagefault_disable();
==>    773		ret = __get_user(*dest, from);       
       774		pagefault_enable();
       775	
       776		return ret ? -EFAULT : 0;
       777	}
       778	
       779	
       780	/*
       781	 * PI code:
       782	 */
       783	static int refill_pi_state_cache(void)
       784	{
       785		struct futex_pi_state *pi_state;
       786	
       787		if (likely(current->pi_state_cache))
       788			return 0;
       789	
       790		pi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);
       791	
       792		if (!pi_state)
       793			return -ENOMEM;


====================================================================================================================================================================================
Total: 10	Addresses: c1122d9a c11c0dbd
5	0xc1122d9a: constant_test_bit at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/bitops.h:325
5	0xc11c0dbd: set_bit at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/bitops.h:76

5	c1122d9a constant_test_bit T: trace_20241114_141325_3_2_60.txt S: 60 I1: 3 I2: 2 IP1: c11c0dbd IP2: c1122d9a PMA1: 36880f50 PMA2: 36880f50 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 143530 IC2: 118001
5	c11c0dbd set_bit T: trace_20241114_141325_3_2_60.txt S: 60 I1: 3 I2: 2 IP1: c11c0dbd IP2: c1122d9a PMA1: 36880f50 PMA2: 36880f50 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 143530 IC2: 118001

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/bitops.h:325
       305	
       306		return oldbit;
       307	}
       308	
       309	/**
       310	 * test_and_change_bit - Change a bit and return its old value
       311	 * @nr: Bit to change
       312	 * @addr: Address to count from
       313	 *
       314	 * This operation is atomic and cannot be reordered.
       315	 * It also implies a memory barrier.
       316	 */
       317	static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
       318	{
       319		GEN_BINARY_RMWcc(LOCK_PREFIX "btc", *addr, "Ir", nr, "%0", c);
       320	}
       321	
       322	static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
       323	{
       324		return ((1UL << (nr & (BITS_PER_LONG-1))) &
==>    325			(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;       
       326	}
       327	
       328	static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
       329	{
       330		bool oldbit;
       331	
       332		asm volatile("bt %2,%1\n\t"
       333			     CC_SET(c)
       334			     : CC_OUT(c) (oldbit)
       335			     : "m" (*(unsigned long *)addr), "Ir" (nr));
       336	
       337		return oldbit;
       338	}
       339	
       340	#if 0 /* Fool kernel-doc since it doesn't do macros yet */
       341	/**
       342	 * test_bit - Determine whether a bit is set
       343	 * @nr: bit number to test
       344	 * @addr: Address to start counting from
       345	 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/bitops.h:76
        56	
        57	/**
        58	 * set_bit - Atomically set a bit in memory
        59	 * @nr: the bit to set
        60	 * @addr: the address to start counting from
        61	 *
        62	 * This function is atomic and may not be reordered.  See __set_bit()
        63	 * if you do not require the atomic guarantees.
        64	 *
        65	 * Note: there are no guarantees that this function will not be reordered
        66	 * on non x86 architectures, so if you are writing portable code,
        67	 * make sure not to rely on its reordering guarantees.
        68	 *
        69	 * Note that @nr may be almost arbitrarily large; this function is not
        70	 * restricted to acting on a single-word quantity.
        71	 */
        72	static __always_inline void
        73	set_bit(long nr, volatile unsigned long *addr)
        74	{
        75		if (IS_IMMEDIATE(nr)) {
==>     76			asm volatile(LOCK_PREFIX "orb %1,%0"       
        77				: CONST_MASK_ADDR(nr, addr)
        78				: "iq" ((u8)CONST_MASK(nr))
        79				: "memory");
        80		} else {
        81			asm volatile(LOCK_PREFIX "bts %1,%0"
        82				: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
        83		}
        84	}
        85	
        86	/**
        87	 * __set_bit - Set a bit in memory
        88	 * @nr: the bit to set
        89	 * @addr: the address to start counting from
        90	 *
        91	 * Unlike set_bit(), this function is non-atomic and may be reordered.
        92	 * If it's called on the same region of memory simultaneously, the effect
        93	 * may be that only one operation succeeds.
        94	 */
        95	static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
        96	{


====================================================================================================================================================================================
Total: 12	Addresses: c10d2ff2 c26c23a7
6	0xc10d2ff2: ttwu_do_wakeup at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:1667
6	0xc26c23a7: sched_submit_work at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:3408

6	c10d2ff2 ttwu_do_wakeup T: trace_20241114_141348_3_2_41.txt S: 41 I1: 3 I2: 2 IP1: c26c23a7 IP2: c10d2ff2 PMA1: 3583ca04 PMA2: 3583ca04 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 180828 IC2: 180823
6	c26c23a7 sched_submit_work T: trace_20241114_141348_3_2_41.txt S: 41 I1: 3 I2: 2 IP1: c26c23a7 IP2: c10d2ff2 PMA1: 3583ca04 PMA2: 3583ca04 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 180828 IC2: 180823

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:1667
      1647			schedstat_inc(p->se.statistics.nr_wakeups_sync);
      1648	}
      1649	
      1650	static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags)
      1651	{
      1652		activate_task(rq, p, en_flags);
      1653		p->on_rq = TASK_ON_RQ_QUEUED;
      1654	
      1655		/* If a worker is waking up, notify the workqueue: */
      1656		if (p->flags & PF_WQ_WORKER)
      1657			wq_worker_waking_up(p, cpu_of(rq));
      1658	}
      1659	
      1660	/*
      1661	 * Mark the task runnable and perform wakeup-preemption.
      1662	 */
      1663	static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
      1664				   struct rq_flags *rf)
      1665	{
      1666		check_preempt_curr(rq, p, wake_flags);
==>   1667		p->state = TASK_RUNNING;       
      1668		trace_sched_wakeup(p);
      1669	
      1670	#ifdef CONFIG_SMP
      1671		if (p->sched_class->task_woken) {
      1672			/*
      1673			 * Our task @p is fully woken up and running; so its safe to
      1674			 * drop the rq->lock, hereafter rq is only used for statistics.
      1675			 */
      1676			rq_unpin_lock(rq, rf);
      1677			p->sched_class->task_woken(rq, p);
      1678			rq_repin_lock(rq, rf);
      1679		}
      1680	
      1681		if (rq->idle_stamp) {
      1682			u64 delta = rq_clock(rq) - rq->idle_stamp;
      1683			u64 max = 2*rq->max_idle_balance_cost;
      1684	
      1685			update_avg(&rq->avg_idle, delta);
      1686	
      1687			if (rq->avg_idle > max)

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:3408
      3388		 */
      3389		raw_spin_lock_irq(&current->pi_lock);
      3390		raw_spin_unlock_irq(&current->pi_lock);
      3391	
      3392		/* Causes final put_task_struct in finish_task_switch(): */
      3393		__set_current_state(TASK_DEAD);
      3394	
      3395		/* Tell freezer to ignore us: */
      3396		current->flags |= PF_NOFREEZE;
      3397	
      3398		__schedule(false);
      3399		BUG();
      3400	
      3401		/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
      3402		for (;;)
      3403			cpu_relax();
      3404	}
      3405	
      3406	static inline void sched_submit_work(struct task_struct *tsk)
      3407	{
==>   3408		if (!tsk->state || tsk_is_pi_blocked(tsk))       
      3409			return;
      3410		/*
      3411		 * If we are going to sleep and we have plugged IO queued,
      3412		 * make sure to submit it to avoid deadlocks.
      3413		 */
      3414		if (blk_needs_flush_plug(tsk))
      3415			blk_schedule_flush_plug(tsk);
      3416	}
      3417	
      3418	asmlinkage __visible void __sched schedule(void)
      3419	{
      3420		struct task_struct *tsk = current;
      3421	
      3422		sched_submit_work(tsk);
      3423		do {
      3424			preempt_disable();
      3425			__schedule(false);
      3426			sched_preempt_enable_no_resched();
      3427		} while (need_resched());
      3428	}


====================================================================================================================================================================================
Total: 14	Addresses: c10d3a85 c26c1cdc
7	0xc10d3a85: try_to_wake_up at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:2012
7	0xc26c1cdc: __schedule at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:3316

7	c10d3a85 try_to_wake_up T: trace_20241114_140219_2_3_8.txt S: 8 I1: 2 I2: 3 IP1: c26c1cdc IP2: c10d3a85 PMA1: 3583ca6c PMA2: 3583ca6c CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 176390 IC2: 175518
7	c26c1cdc __schedule T: trace_20241114_140219_2_3_8.txt S: 8 I1: 2 I2: 3 IP1: c26c1cdc IP2: c10d3a85 PMA1: 3583ca6c PMA2: 3583ca6c CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 176390 IC2: 175518

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:2012
      1992		 * be possible to, falsely, observe p->on_rq == 0 and get stuck
      1993		 * in smp_cond_load_acquire() below.
      1994		 *
      1995		 * sched_ttwu_pending()                 try_to_wake_up()
      1996		 *   [S] p->on_rq = 1;                  [L] P->state
      1997		 *       UNLOCK rq->lock  -----.
      1998		 *                              \
      1999		 *				 +---   RMB
      2000		 * schedule()                   /
      2001		 *       LOCK rq->lock    -----'
      2002		 *       UNLOCK rq->lock
      2003		 *
      2004		 * [task p]
      2005		 *   [S] p->state = UNINTERRUPTIBLE     [L] p->on_rq
      2006		 *
      2007		 * Pairs with the UNLOCK+LOCK on rq->lock from the
      2008		 * last wakeup of our task and the schedule that got our task
      2009		 * current.
      2010		 */
      2011		smp_rmb();
==>   2012		if (p->on_rq && ttwu_remote(p, wake_flags))       
      2013			goto stat;
      2014	
      2015	#ifdef CONFIG_SMP
      2016		/*
      2017		 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
      2018		 * possible to, falsely, observe p->on_cpu == 0.
      2019		 *
      2020		 * One must be running (->on_cpu == 1) in order to remove oneself
      2021		 * from the runqueue.
      2022		 *
      2023		 *  [S] ->on_cpu = 1;	[L] ->on_rq
      2024		 *      UNLOCK rq->lock
      2025		 *			RMB
      2026		 *      LOCK   rq->lock
      2027		 *  [S] ->on_rq = 0;    [L] ->on_cpu
      2028		 *
      2029		 * Pairs with the full barrier implied in the UNLOCK+LOCK on rq->lock
      2030		 * from the consecutive calls to schedule(); the first switching to our
      2031		 * task, the second putting it to sleep.
      2032		 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:3316
      3296		rcu_note_context_switch(preempt);
      3297	
      3298		/*
      3299		 * Make sure that signal_pending_state()->signal_pending() below
      3300		 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
      3301		 * done by the caller to avoid the race with signal_wake_up().
      3302		 */
      3303		rq_lock(rq, &rf);
      3304		smp_mb__after_spinlock();
      3305	
      3306		/* Promote REQ to ACT */
      3307		rq->clock_update_flags <<= 1;
      3308		update_rq_clock(rq);
      3309	
      3310		switch_count = &prev->nivcsw;
      3311		if (!preempt && prev->state) {
      3312			if (unlikely(signal_pending_state(prev->state, prev))) {
      3313				prev->state = TASK_RUNNING;
      3314			} else {
      3315				deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
==>   3316				prev->on_rq = 0;       
      3317	
      3318				if (prev->in_iowait) {
      3319					atomic_inc(&rq->nr_iowait);
      3320					delayacct_blkio_start();
      3321				}
      3322	
      3323				/*
      3324				 * If a worker went to sleep, notify and ask workqueue
      3325				 * whether it wants to wake up a task to maintain
      3326				 * concurrency.
      3327				 */
      3328				if (prev->flags & PF_WQ_WORKER) {
      3329					struct task_struct *to_wakeup;
      3330	
      3331					to_wakeup = wq_worker_sleeping(prev);
      3332					if (to_wakeup)
      3333						try_to_wake_up_local(to_wakeup, &rf);
      3334				}
      3335			}
      3336			switch_count = &prev->nvcsw;


====================================================================================================================================================================================
Total: 16	Addresses: c10d82d2 c10dc6f0
8	0xc10d82d2: update_min_vruntime at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:542
8	0xc10dc6f0: migrate_task_rq_fair at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:6051 (discriminator 1)

8	c10d82d2 update_min_vruntime T: trace_20241114_135835_2_2_37.txt S: 37 I1: 2 I2: 2 IP1: c10dc6f0 IP2: c10d82d2 PMA1: 365ad418 PMA2: 365ad418 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 178265 IC2: 178257
8	c10dc6f0 migrate_task_rq_fair T: trace_20241114_135835_2_2_37.txt S: 37 I1: 2 I2: 2 IP1: c10dc6f0 IP2: c10d82d2 PMA1: 365ad418 PMA2: 365ad418 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 178265 IC2: 178257

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:542
       522			if (curr->on_rq)
       523				vruntime = curr->vruntime;
       524			else
       525				curr = NULL;
       526		}
       527	
       528		if (leftmost) { /* non-empty tree */
       529			struct sched_entity *se;
       530			se = rb_entry(leftmost, struct sched_entity, run_node);
       531	
       532			if (!curr)
       533				vruntime = se->vruntime;
       534			else
       535				vruntime = min_vruntime(vruntime, se->vruntime);
       536		}
       537	
       538		/* ensure we never gain time by being placed backwards. */
       539		cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);
       540	#ifndef CONFIG_64BIT
       541		smp_wmb();
==>    542		cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;       
       543	#endif
       544	}
       545	
       546	/*
       547	 * Enqueue an entity into the rb-tree:
       548	 */
       549	static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
       550	{
       551		struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
       552		struct rb_node *parent = NULL;
       553		struct sched_entity *entry;
       554		bool leftmost = true;
       555	
       556		/*
       557		 * Find the right place in the rbtree:
       558		 */
       559		while (*link) {
       560			parent = *link;
       561			entry = rb_entry(parent, struct sched_entity, run_node);
       562			/*

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:6051
      6031	 * cfs_rq_of(p) references at time of call are still valid and identify the
      6032	 * previous cpu. The caller guarantees p->pi_lock or task_rq(p)->lock is held.
      6033	 */
      6034	static void migrate_task_rq_fair(struct task_struct *p)
      6035	{
      6036		/*
      6037		 * As blocked tasks retain absolute vruntime the migration needs to
      6038		 * deal with this by subtracting the old and adding the new
      6039		 * min_vruntime -- the latter is done by enqueue_entity() when placing
      6040		 * the task on the new runqueue.
      6041		 */
      6042		if (p->state == TASK_WAKING) {
      6043			struct sched_entity *se = &p->se;
      6044			struct cfs_rq *cfs_rq = cfs_rq_of(se);
      6045			u64 min_vruntime;
      6046	
      6047	#ifndef CONFIG_64BIT
      6048			u64 min_vruntime_copy;
      6049	
      6050			do {
==>   6051				min_vruntime_copy = cfs_rq->min_vruntime_copy;       
      6052				smp_rmb();
      6053				min_vruntime = cfs_rq->min_vruntime;
      6054			} while (min_vruntime != min_vruntime_copy);
      6055	#else
      6056			min_vruntime = cfs_rq->min_vruntime;
      6057	#endif
      6058	
      6059			se->vruntime -= min_vruntime;
      6060		}
      6061	
      6062		/*
      6063		 * We are supposed to update the task to "current" time, then its up to date
      6064		 * and ready to go to new CPU/cfs_rq. But we have difficulty in getting
      6065		 * what current time is, so simply throw away the out-of-date time. This
      6066		 * will result in the wakee task is less decayed, but giving the wakee more
      6067		 * load sounds not bad.
      6068		 */
      6069		remove_entity_load_avg(&p->se);
      6070	
      6071		/* Tell new CPU we are migrated */


====================================================================================================================================================================================
Total: 24	Addresses: c10e9b6b c10e952a
12	0xc10e9b59: autoremove_wake_function at /root/2017-17712-i386/linux-4.14/kernel/sched/wait.c:382
12	0xc10e9523: finish_wait at /root/2017-17712-i386/linux-4.14/kernel/sched/wait.c:353

12	c10e9b6b __write_once_size T: trace_20241114_140619_3_2_34.txt S: 34 I1: 3 I2: 2 IP1: c10e9b6b IP2: c10e952a PMA1: 31629ea0 PMA2: 31629ea0 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 2290906 IC2: 2290902
12	c10e952a list_empty_careful T: trace_20241114_140619_3_2_34.txt S: 34 I1: 3 I2: 2 IP1: c10e9b6b IP2: c10e952a PMA1: 31629ea0 PMA2: 31629ea0 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 2290906 IC2: 2290902

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/wait.c:382
       362		 * and
       363		 *  - all other users take the lock (ie we can only
       364		 *    have _one_ other CPU that looks at or modifies
       365		 *    the list).
       366		 */
       367		if (!list_empty_careful(&wq_entry->entry)) {
       368			spin_lock_irqsave(&wq_head->lock, flags);
       369			list_del_init(&wq_entry->entry);
       370			spin_unlock_irqrestore(&wq_head->lock, flags);
       371		}
       372	}
       373	EXPORT_SYMBOL(finish_wait);
       374	
       375	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       376	{
       377		int ret = default_wake_function(wq_entry, mode, sync, key);
       378	
       379		if (ret)
       380			list_del_init(&wq_entry->entry);
       381		return ret;
==>    382	}       
       383	EXPORT_SYMBOL(autoremove_wake_function);
       384	
       385	static inline bool is_kthread_should_stop(void)
       386	{
       387		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       388	}
       389	
       390	/*
       391	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       392	 *
       393	 * add_wait_queue(&wq_head, &wait);
       394	 * for (;;) {
       395	 *     if (condition)
       396	 *         break;
       397	 *
       398	 *     p->state = mode;				condition = true;
       399	 *     smp_mb(); // A				smp_wmb(); // C
       400	 *     if (!wq_entry->flags & WQ_FLAG_WOKEN)	wq_entry->flags |= WQ_FLAG_WOKEN;
       401	 *         schedule()				try_to_wake_up();
       402	 *     p->state = TASK_RUNNING;		    ~~~~~~~~~~~~~~~~~~

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/wait.c:353
       333		spin_unlock_irq(&wq->lock);
       334		schedule();
       335		spin_lock_irq(&wq->lock);
       336		return 0;
       337	}
       338	EXPORT_SYMBOL(do_wait_intr_irq);
       339	
       340	/**
       341	 * finish_wait - clean up after waiting in a queue
       342	 * @wq_head: waitqueue waited on
       343	 * @wq_entry: wait descriptor
       344	 *
       345	 * Sets current thread back to running state and removes
       346	 * the wait descriptor from the given waitqueue if still
       347	 * queued.
       348	 */
       349	void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
       350	{
       351		unsigned long flags;
       352	
==>    353		__set_current_state(TASK_RUNNING);       
       354		/*
       355		 * We can check for list emptiness outside the lock
       356		 * IFF:
       357		 *  - we use the "careful" check that verifies both
       358		 *    the next and prev pointers, so that there cannot
       359		 *    be any half-pending updates in progress on other
       360		 *    CPU's that we haven't seen yet (and that might
       361		 *    still change the stack area.
       362		 * and
       363		 *  - all other users take the lock (ie we can only
       364		 *    have _one_ other CPU that looks at or modifies
       365		 *    the list).
       366		 */
       367		if (!list_empty_careful(&wq_entry->entry)) {
       368			spin_lock_irqsave(&wq_head->lock, flags);
       369			list_del_init(&wq_entry->entry);
       370			spin_unlock_irqrestore(&wq_head->lock, flags);
       371		}
       372	}
       373	EXPORT_SYMBOL(finish_wait);


====================================================================================================================================================================================
Total: 26	Addresses: c10d8e16 c10db84c
13	0xc10d8e16: ___update_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:2992
13	0xc10db84c: cfs_rq_last_update_time at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3487

13	c10d8e16 ___update_load_avg T: trace_20241114_135911_2_3_13.txt S: 13 I1: 2 I2: 3 IP1: c10d8e16 IP2: c10db84c PMA1: 365ad438 PMA2: 365ad438 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 686019 IC2: 685398
13	c10db84c cfs_rq_last_update_time T: trace_20241114_135911_2_3_13.txt S: 13 I1: 2 I2: 3 IP1: c10d8e16 IP2: c10db84c PMA1: 365ad438 PMA2: 365ad438 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 686019 IC2: 685398

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:2992
      2972		u64 delta;
      2973	
      2974		delta = now - sa->last_update_time;
      2975		/*
      2976		 * This should only happen when time goes backwards, which it
      2977		 * unfortunately does during sched clock init when we swap over to TSC.
      2978		 */
      2979		if ((s64)delta < 0) {
      2980			sa->last_update_time = now;
      2981			return 0;
      2982		}
      2983	
      2984		/*
      2985		 * Use 1024ns as the unit of measurement since it's a reasonable
      2986		 * approximation of 1us and fast to compute.
      2987		 */
      2988		delta >>= 10;
      2989		if (!delta)
      2990			return 0;
      2991	
==>   2992		sa->last_update_time += delta << 10;       
      2993	
      2994		/*
      2995		 * running is a subset of runnable (weight) so running can't be set if
      2996		 * runnable is clear. But there are some corner cases where the current
      2997		 * se has been already dequeued but cfs_rq->curr still points to it.
      2998		 * This means that weight will be 0 but not running for a sched_entity
      2999		 * but also for a cfs_rq if the latter becomes idle. As an example,
      3000		 * this happens during idle_balance() which calls
      3001		 * update_blocked_averages()
      3002		 */
      3003		if (!weight)
      3004			running = 0;
      3005	
      3006		/*
      3007		 * Now we know we crossed measurement unit boundaries. The *_avg
      3008		 * accrues by two steps:
      3009		 *
      3010		 * Step 1: accumulate *_sum since last_update_time. If we haven't
      3011		 * crossed period boundaries, finish.
      3012		 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3487
      3467	/* Remove the runnable load generated by se from cfs_rq's runnable load average */
      3468	static inline void
      3469	dequeue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3470	{
      3471		cfs_rq->runnable_load_avg =
      3472			max_t(long, cfs_rq->runnable_load_avg - se->avg.load_avg, 0);
      3473		cfs_rq->runnable_load_sum =
      3474			max_t(s64,  cfs_rq->runnable_load_sum - se->avg.load_sum, 0);
      3475	}
      3476	
      3477	#ifndef CONFIG_64BIT
      3478	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3479	{
      3480		u64 last_update_time_copy;
      3481		u64 last_update_time;
      3482	
      3483		do {
      3484			last_update_time_copy = cfs_rq->load_last_update_time_copy;
      3485			smp_rmb();
      3486			last_update_time = cfs_rq->avg.last_update_time;
==>   3487		} while (last_update_time != last_update_time_copy);       
      3488	
      3489		return last_update_time;
      3490	}
      3491	#else
      3492	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3493	{
      3494		return cfs_rq->avg.last_update_time;
      3495	}
      3496	#endif
      3497	
      3498	/*
      3499	 * Synchronize entity load avg of dequeued entity without locking
      3500	 * the previous rq.
      3501	 */
      3502	void sync_entity_load_avg(struct sched_entity *se)
      3503	{
      3504		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3505		u64 last_update_time;
      3506	
      3507		last_update_time = cfs_rq_last_update_time(cfs_rq);


====================================================================================================================================================================================
Total: 30	Addresses: c10ed57a c10ed65a
15	0xc10ed578: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
15	0xc10ed658: osq_unlock at /root/2017-17712-i386/linux-4.14/kernel/locking/osq_lock.c:223

15	c10ed57a __read_once_size T: trace_20241114_141519_3_3_52.txt S: 52 I1: 3 I2: 3 IP1: c10ed57a IP2: c10ed65a PMA1: 365ada48 PMA2: 365ada48 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 289125 IC2: 289123
15	c10ed65a __write_once_size T: trace_20241114_141519_3_3_52.txt S: 52 I1: 3 I2: 3 IP1: c10ed57a IP2: c10ed65a PMA1: 365ada48 PMA2: 365ada48 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 289125 IC2: 289123

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/osq_lock.c:223
       203		return false;
       204	}
       205	
       206	void osq_unlock(struct optimistic_spin_queue *lock)
       207	{
       208		struct optimistic_spin_node *node, *next;
       209		int curr = encode_cpu(smp_processor_id());
       210	
       211		/*
       212		 * Fast path for the uncontended case.
       213		 */
       214		if (likely(atomic_cmpxchg_release(&lock->tail, curr,
       215						  OSQ_UNLOCKED_VAL) == curr))
       216			return;
       217	
       218		/*
       219		 * Second most likely case.
       220		 */
       221		node = this_cpu_ptr(&osq_node);
       222		next = xchg(&node->next, NULL);
==>    223		if (next) {       
       224			WRITE_ONCE(next->locked, 1);
       225			return;
       226		}
       227	
       228		next = osq_wait_next(lock, node, NULL);
       229		if (next)
       230			WRITE_ONCE(next->locked, 1);
       231	}


====================================================================================================================================================================================
Total: 36	Addresses: c10dc6f6 c10d82c6
18	0xc10dc6f6: migrate_task_rq_fair at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:6054 (discriminator 1)
18	0xc10d82c6: update_min_vruntime at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:539

18	c10dc6f6 migrate_task_rq_fair T: trace_20241114_135823_2_3_9.txt S: 9 I1: 2 I2: 3 IP1: c10d82c6 IP2: c10dc6f6 PMA1: 365ad414 PMA2: 365ad414 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 173778 IC2: 171677
18	c10d82c6 update_min_vruntime T: trace_20241114_135823_2_3_9.txt S: 9 I1: 2 I2: 3 IP1: c10d82c6 IP2: c10dc6f6 PMA1: 365ad414 PMA2: 365ad414 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 173778 IC2: 171677

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:6054
      6034	static void migrate_task_rq_fair(struct task_struct *p)
      6035	{
      6036		/*
      6037		 * As blocked tasks retain absolute vruntime the migration needs to
      6038		 * deal with this by subtracting the old and adding the new
      6039		 * min_vruntime -- the latter is done by enqueue_entity() when placing
      6040		 * the task on the new runqueue.
      6041		 */
      6042		if (p->state == TASK_WAKING) {
      6043			struct sched_entity *se = &p->se;
      6044			struct cfs_rq *cfs_rq = cfs_rq_of(se);
      6045			u64 min_vruntime;
      6046	
      6047	#ifndef CONFIG_64BIT
      6048			u64 min_vruntime_copy;
      6049	
      6050			do {
      6051				min_vruntime_copy = cfs_rq->min_vruntime_copy;
      6052				smp_rmb();
      6053				min_vruntime = cfs_rq->min_vruntime;
==>   6054			} while (min_vruntime != min_vruntime_copy);       
      6055	#else
      6056			min_vruntime = cfs_rq->min_vruntime;
      6057	#endif
      6058	
      6059			se->vruntime -= min_vruntime;
      6060		}
      6061	
      6062		/*
      6063		 * We are supposed to update the task to "current" time, then its up to date
      6064		 * and ready to go to new CPU/cfs_rq. But we have difficulty in getting
      6065		 * what current time is, so simply throw away the out-of-date time. This
      6066		 * will result in the wakee task is less decayed, but giving the wakee more
      6067		 * load sounds not bad.
      6068		 */
      6069		remove_entity_load_avg(&p->se);
      6070	
      6071		/* Tell new CPU we are migrated */
      6072		p->se.avg.last_update_time = 0;
      6073	
      6074		/* We have migrated, no longer consider this task hot */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:539
       519		u64 vruntime = cfs_rq->min_vruntime;
       520	
       521		if (curr) {
       522			if (curr->on_rq)
       523				vruntime = curr->vruntime;
       524			else
       525				curr = NULL;
       526		}
       527	
       528		if (leftmost) { /* non-empty tree */
       529			struct sched_entity *se;
       530			se = rb_entry(leftmost, struct sched_entity, run_node);
       531	
       532			if (!curr)
       533				vruntime = se->vruntime;
       534			else
       535				vruntime = min_vruntime(vruntime, se->vruntime);
       536		}
       537	
       538		/* ensure we never gain time by being placed backwards. */
==>    539		cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);       
       540	#ifndef CONFIG_64BIT
       541		smp_wmb();
       542		cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;
       543	#endif
       544	}
       545	
       546	/*
       547	 * Enqueue an entity into the rb-tree:
       548	 */
       549	static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
       550	{
       551		struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
       552		struct rb_node *parent = NULL;
       553		struct sched_entity *entry;
       554		bool leftmost = true;
       555	
       556		/*
       557		 * Find the right place in the rbtree:
       558		 */
       559		while (*link) {


====================================================================================================================================================================================
Total: 38	Addresses: c10d51b5 c26c1d9f
19	0xc10d51b5: idle_cpu at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:3901
19	0xc26c1d9f: __schedule at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:3345

19	c10d51b5 idle_cpu T: trace_20241114_135616_2_2_59.txt S: 59 I1: 2 I2: 2 IP1: c26c1d9f IP2: c10d51b5 PMA1: 365ad8c0 PMA2: 365ad8c0 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 115570 IC2: 113289
19	c26c1d9f __schedule T: trace_20241114_135616_2_2_59.txt S: 59 I1: 2 I2: 2 IP1: c26c1d9f IP2: c10d51b5 PMA1: 365ad8c0 PMA2: 365ad8c0 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 115570 IC2: 113289

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:3901
      3881	 *
      3882	 * Return: The priority value as seen by users in /proc.
      3883	 * RT tasks are offset by -200. Normal tasks are centered
      3884	 * around 0, value goes from -16 to +15.
      3885	 */
      3886	int task_prio(const struct task_struct *p)
      3887	{
      3888		return p->prio - MAX_RT_PRIO;
      3889	}
      3890	
      3891	/**
      3892	 * idle_cpu - is a given CPU idle currently?
      3893	 * @cpu: the processor in question.
      3894	 *
      3895	 * Return: 1 if the CPU is currently idle. 0 otherwise.
      3896	 */
      3897	int idle_cpu(int cpu)
      3898	{
      3899		struct rq *rq = cpu_rq(cpu);
      3900	
==>   3901		if (rq->curr != rq->idle)       
      3902			return 0;
      3903	
      3904		if (rq->nr_running)
      3905			return 0;
      3906	
      3907	#ifdef CONFIG_SMP
      3908		if (!llist_empty(&rq->wake_list))
      3909			return 0;
      3910	#endif
      3911	
      3912		return 1;
      3913	}
      3914	
      3915	/**
      3916	 * idle_task - return the idle task for a given CPU.
      3917	 * @cpu: the processor in question.
      3918	 *
      3919	 * Return: The idle task for the CPU @cpu.
      3920	 */
      3921	struct task_struct *idle_task(int cpu)

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:3345
      3325				 * whether it wants to wake up a task to maintain
      3326				 * concurrency.
      3327				 */
      3328				if (prev->flags & PF_WQ_WORKER) {
      3329					struct task_struct *to_wakeup;
      3330	
      3331					to_wakeup = wq_worker_sleeping(prev);
      3332					if (to_wakeup)
      3333						try_to_wake_up_local(to_wakeup, &rf);
      3334				}
      3335			}
      3336			switch_count = &prev->nvcsw;
      3337		}
      3338	
      3339		next = pick_next_task(rq, prev, &rf);
      3340		clear_tsk_need_resched(prev);
      3341		clear_preempt_need_resched();
      3342	
      3343		if (likely(prev != next)) {
      3344			rq->nr_switches++;
==>   3345			rq->curr = next;       
      3346			/*
      3347			 * The membarrier system call requires each architecture
      3348			 * to have a full memory barrier after updating
      3349			 * rq->curr, before returning to user-space. For TSO
      3350			 * (e.g. x86), the architecture must provide its own
      3351			 * barrier in switch_mm(). For weakly ordered machines
      3352			 * for which spin_unlock() acts as a full memory
      3353			 * barrier, finish_lock_switch() in common code takes
      3354			 * care of this barrier. For weakly ordered machines for
      3355			 * which spin_unlock() acts as a RELEASE barrier (only
      3356			 * arm64 and PowerPC), arm64 has a full barrier in
      3357			 * switch_to(), and PowerPC has
      3358			 * smp_mb__after_unlock_lock() before
      3359			 * finish_lock_switch().
      3360			 */
      3361			++*switch_count;
      3362	
      3363			trace_sched_switch(preempt, prev, next);
      3364	
      3365			/* Also unlocks the rq: */


====================================================================================================================================================================================
Total: 50	Addresses: c10d8eaf c10d85d0 c10d8590
1	0xc10d8590: update_cfs_rq_h_load at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:7037
24	0xc10d85d0: update_cfs_rq_h_load at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:7033
25	0xc10d8eaf: ___update_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3019

1	c10d8590 update_cfs_rq_h_load T: trace_20241114_135822_3_3_42.txt S: 42 I1: 3 I2: 3 IP1: c10d8590 IP2: c10d8eaf PMA1: 365ad450 PMA2: 365ad450 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 180629 IC2: 180628
24	c10d85d0 update_cfs_rq_h_load T: trace_20241114_135826_2_3_58.txt S: 58 I1: 2 I2: 3 IP1: c10d8eaf IP2: c10d85d0 PMA1: 365ad450 PMA2: 365ad450 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120694 IC2: 119054
25	c10d8eaf ___update_load_avg T: trace_20241114_135826_2_3_58.txt S: 58 I1: 2 I2: 3 IP1: c10d8eaf IP2: c10d85d0 PMA1: 365ad450 PMA2: 365ad450 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120694 IC2: 119054

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:7037
      7017		struct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];
      7018		unsigned long now = jiffies;
      7019		unsigned long load;
      7020	
      7021		if (cfs_rq->last_h_load_update == now)
      7022			return;
      7023	
      7024		cfs_rq->h_load_next = NULL;
      7025		for_each_sched_entity(se) {
      7026			cfs_rq = cfs_rq_of(se);
      7027			cfs_rq->h_load_next = se;
      7028			if (cfs_rq->last_h_load_update == now)
      7029				break;
      7030		}
      7031	
      7032		if (!se) {
      7033			cfs_rq->h_load = cfs_rq_load_avg(cfs_rq);
      7034			cfs_rq->last_h_load_update = now;
      7035		}
      7036	
==>   7037		while ((se = cfs_rq->h_load_next) != NULL) {       
      7038			load = cfs_rq->h_load;
      7039			load = div64_ul(load * se->avg.load_avg,
      7040				cfs_rq_load_avg(cfs_rq) + 1);
      7041			cfs_rq = group_cfs_rq(se);
      7042			cfs_rq->h_load = load;
      7043			cfs_rq->last_h_load_update = now;
      7044		}
      7045	}
      7046	
      7047	static unsigned long task_h_load(struct task_struct *p)
      7048	{
      7049		struct cfs_rq *cfs_rq = task_cfs_rq(p);
      7050	
      7051		update_cfs_rq_h_load(cfs_rq);
      7052		return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
      7053				cfs_rq_load_avg(cfs_rq) + 1);
      7054	}
      7055	#else
      7056	static inline void update_blocked_averages(int cpu)
      7057	{

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:7033
      7013	 */
      7014	static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)
      7015	{
      7016		struct rq *rq = rq_of(cfs_rq);
      7017		struct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];
      7018		unsigned long now = jiffies;
      7019		unsigned long load;
      7020	
      7021		if (cfs_rq->last_h_load_update == now)
      7022			return;
      7023	
      7024		cfs_rq->h_load_next = NULL;
      7025		for_each_sched_entity(se) {
      7026			cfs_rq = cfs_rq_of(se);
      7027			cfs_rq->h_load_next = se;
      7028			if (cfs_rq->last_h_load_update == now)
      7029				break;
      7030		}
      7031	
      7032		if (!se) {
==>   7033			cfs_rq->h_load = cfs_rq_load_avg(cfs_rq);       
      7034			cfs_rq->last_h_load_update = now;
      7035		}
      7036	
      7037		while ((se = cfs_rq->h_load_next) != NULL) {
      7038			load = cfs_rq->h_load;
      7039			load = div64_ul(load * se->avg.load_avg,
      7040				cfs_rq_load_avg(cfs_rq) + 1);
      7041			cfs_rq = group_cfs_rq(se);
      7042			cfs_rq->h_load = load;
      7043			cfs_rq->last_h_load_update = now;
      7044		}
      7045	}
      7046	
      7047	static unsigned long task_h_load(struct task_struct *p)
      7048	{
      7049		struct cfs_rq *cfs_rq = task_cfs_rq(p);
      7050	
      7051		update_cfs_rq_h_load(cfs_rq);
      7052		return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
      7053				cfs_rq_load_avg(cfs_rq) + 1);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3019
      2999		 * but also for a cfs_rq if the latter becomes idle. As an example,
      3000		 * this happens during idle_balance() which calls
      3001		 * update_blocked_averages()
      3002		 */
      3003		if (!weight)
      3004			running = 0;
      3005	
      3006		/*
      3007		 * Now we know we crossed measurement unit boundaries. The *_avg
      3008		 * accrues by two steps:
      3009		 *
      3010		 * Step 1: accumulate *_sum since last_update_time. If we haven't
      3011		 * crossed period boundaries, finish.
      3012		 */
      3013		if (!accumulate_sum(delta, cpu, sa, weight, running, cfs_rq))
      3014			return 0;
      3015	
      3016		/*
      3017		 * Step 2: update *_avg.
      3018		 */
==>   3019		sa->load_avg = div_u64(sa->load_sum, LOAD_AVG_MAX - 1024 + sa->period_contrib);       
      3020		if (cfs_rq) {
      3021			cfs_rq->runnable_load_avg =
      3022				div_u64(cfs_rq->runnable_load_sum, LOAD_AVG_MAX - 1024 + sa->period_contrib);
      3023		}
      3024		sa->util_avg = sa->util_sum / (LOAD_AVG_MAX - 1024 + sa->period_contrib);
      3025	
      3026		return 1;
      3027	}
      3028	
      3029	static int
      3030	__update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)
      3031	{
      3032		return ___update_load_avg(now, cpu, &se->avg, 0, 0, NULL);
      3033	}
      3034	
      3035	static int
      3036	__update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)
      3037	{
      3038		return ___update_load_avg(now, cpu, &se->avg,
      3039					  se->on_rq * scale_load_down(se->load.weight),


====================================================================================================================================================================================
Total: 54	Addresses: c10d8ed7 c10dd1e1 c10dd1f2 c10dc22c
4	0xc10dd1f2: dequeue_entity_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3471
11	0xc10dd1e1: dequeue_entity_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3472
12	0xc10d8ed7: ___update_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3021
27	0xc10dc22c: cfs_rq_runnable_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3536

4	c10dd1f2 dequeue_entity_load_avg T: trace_20241114_135914_2_3_57.txt S: 57 I1: 2 I2: 3 IP1: c10dd1f2 IP2: c10dc22c PMA1: 365ad460 PMA2: 365ad460 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120670 IC2: 120621
11	c10dd1e1 dequeue_entity_load_avg T: trace_20241114_135914_2_3_58.txt S: 58 I1: 2 I2: 3 IP1: c10dd1e1 IP2: c10dc22c PMA1: 365ad460 PMA2: 365ad460 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120665 IC2: 118970
12	c10d8ed7 ___update_load_avg T: trace_20241114_135914_2_3_58.txt S: 58 I1: 2 I2: 3 IP1: c10d8ed7 IP2: c10dc22c PMA1: 365ad460 PMA2: 365ad460 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120621 IC2: 118970
27	c10dc22c cfs_rq_runnable_load_avg T: trace_20241114_135914_2_3_58.txt S: 58 I1: 2 I2: 3 IP1: c10dd1e1 IP2: c10dc22c PMA1: 365ad460 PMA2: 365ad460 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120665 IC2: 118970

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3471
      3451	
      3452	/* Add the load generated by se into cfs_rq's load average */
      3453	static inline void
      3454	enqueue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3455	{
      3456		struct sched_avg *sa = &se->avg;
      3457	
      3458		cfs_rq->runnable_load_avg += sa->load_avg;
      3459		cfs_rq->runnable_load_sum += sa->load_sum;
      3460	
      3461		if (!sa->last_update_time) {
      3462			attach_entity_load_avg(cfs_rq, se);
      3463			update_tg_load_avg(cfs_rq, 0);
      3464		}
      3465	}
      3466	
      3467	/* Remove the runnable load generated by se from cfs_rq's runnable load average */
      3468	static inline void
      3469	dequeue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3470	{
==>   3471		cfs_rq->runnable_load_avg =       
      3472			max_t(long, cfs_rq->runnable_load_avg - se->avg.load_avg, 0);
      3473		cfs_rq->runnable_load_sum =
      3474			max_t(s64,  cfs_rq->runnable_load_sum - se->avg.load_sum, 0);
      3475	}
      3476	
      3477	#ifndef CONFIG_64BIT
      3478	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3479	{
      3480		u64 last_update_time_copy;
      3481		u64 last_update_time;
      3482	
      3483		do {
      3484			last_update_time_copy = cfs_rq->load_last_update_time_copy;
      3485			smp_rmb();
      3486			last_update_time = cfs_rq->avg.last_update_time;
      3487		} while (last_update_time != last_update_time_copy);
      3488	
      3489		return last_update_time;
      3490	}
      3491	#else

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3472
      3452	/* Add the load generated by se into cfs_rq's load average */
      3453	static inline void
      3454	enqueue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3455	{
      3456		struct sched_avg *sa = &se->avg;
      3457	
      3458		cfs_rq->runnable_load_avg += sa->load_avg;
      3459		cfs_rq->runnable_load_sum += sa->load_sum;
      3460	
      3461		if (!sa->last_update_time) {
      3462			attach_entity_load_avg(cfs_rq, se);
      3463			update_tg_load_avg(cfs_rq, 0);
      3464		}
      3465	}
      3466	
      3467	/* Remove the runnable load generated by se from cfs_rq's runnable load average */
      3468	static inline void
      3469	dequeue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3470	{
      3471		cfs_rq->runnable_load_avg =
==>   3472			max_t(long, cfs_rq->runnable_load_avg - se->avg.load_avg, 0);       
      3473		cfs_rq->runnable_load_sum =
      3474			max_t(s64,  cfs_rq->runnable_load_sum - se->avg.load_sum, 0);
      3475	}
      3476	
      3477	#ifndef CONFIG_64BIT
      3478	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3479	{
      3480		u64 last_update_time_copy;
      3481		u64 last_update_time;
      3482	
      3483		do {
      3484			last_update_time_copy = cfs_rq->load_last_update_time_copy;
      3485			smp_rmb();
      3486			last_update_time = cfs_rq->avg.last_update_time;
      3487		} while (last_update_time != last_update_time_copy);
      3488	
      3489		return last_update_time;
      3490	}
      3491	#else
      3492	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3021
      3001		 * update_blocked_averages()
      3002		 */
      3003		if (!weight)
      3004			running = 0;
      3005	
      3006		/*
      3007		 * Now we know we crossed measurement unit boundaries. The *_avg
      3008		 * accrues by two steps:
      3009		 *
      3010		 * Step 1: accumulate *_sum since last_update_time. If we haven't
      3011		 * crossed period boundaries, finish.
      3012		 */
      3013		if (!accumulate_sum(delta, cpu, sa, weight, running, cfs_rq))
      3014			return 0;
      3015	
      3016		/*
      3017		 * Step 2: update *_avg.
      3018		 */
      3019		sa->load_avg = div_u64(sa->load_sum, LOAD_AVG_MAX - 1024 + sa->period_contrib);
      3020		if (cfs_rq) {
==>   3021			cfs_rq->runnable_load_avg =       
      3022				div_u64(cfs_rq->runnable_load_sum, LOAD_AVG_MAX - 1024 + sa->period_contrib);
      3023		}
      3024		sa->util_avg = sa->util_sum / (LOAD_AVG_MAX - 1024 + sa->period_contrib);
      3025	
      3026		return 1;
      3027	}
      3028	
      3029	static int
      3030	__update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)
      3031	{
      3032		return ___update_load_avg(now, cpu, &se->avg, 0, 0, NULL);
      3033	}
      3034	
      3035	static int
      3036	__update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)
      3037	{
      3038		return ___update_load_avg(now, cpu, &se->avg,
      3039					  se->on_rq * scale_load_down(se->load.weight),
      3040					  cfs_rq->curr == se, NULL);
      3041	}

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3536
      3516	{
      3517		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3518	
      3519		/*
      3520		 * tasks cannot exit without having gone through wake_up_new_task() ->
      3521		 * post_init_entity_util_avg() which will have added things to the
      3522		 * cfs_rq, so we can remove unconditionally.
      3523		 *
      3524		 * Similarly for groups, they will have passed through
      3525		 * post_init_entity_util_avg() before unregister_sched_fair_group()
      3526		 * calls this.
      3527		 */
      3528	
      3529		sync_entity_load_avg(se);
      3530		atomic_long_add(se->avg.load_avg, &cfs_rq->removed_load_avg);
      3531		atomic_long_add(se->avg.util_avg, &cfs_rq->removed_util_avg);
      3532	}
      3533	
      3534	static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)
      3535	{
==>   3536		return cfs_rq->runnable_load_avg;       
      3537	}
      3538	
      3539	static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)
      3540	{
      3541		return cfs_rq->avg.load_avg;
      3542	}
      3543	
      3544	static int idle_balance(struct rq *this_rq, struct rq_flags *rf);
      3545	
      3546	#else /* CONFIG_SMP */
      3547	
      3548	static inline int
      3549	update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
      3550	{
      3551		return 0;
      3552	}
      3553	
      3554	#define UPDATE_TG	0x0
      3555	#define SKIP_AGE_LOAD	0x0
      3556	


====================================================================================================================================================================================
Total: 62	Addresses: c10d82c3 c10dc6f9
31	0xc10d82c3: update_min_vruntime at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:539
31	0xc10dc6f9: migrate_task_rq_fair at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:6054 (discriminator 1)

31	c10d82c3 update_min_vruntime T: trace_20241114_135812_3_3_63.txt S: 63 I1: 3 I2: 3 IP1: c10d82c3 IP2: c10dc6f9 PMA1: 365ad410 PMA2: 365ad410 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 191030 IC2: 165932
31	c10dc6f9 migrate_task_rq_fair T: trace_20241114_135812_3_3_63.txt S: 63 I1: 3 I2: 3 IP1: c10d82c3 IP2: c10dc6f9 PMA1: 365ad410 PMA2: 365ad410 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 191030 IC2: 165932

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:539
       519		u64 vruntime = cfs_rq->min_vruntime;
       520	
       521		if (curr) {
       522			if (curr->on_rq)
       523				vruntime = curr->vruntime;
       524			else
       525				curr = NULL;
       526		}
       527	
       528		if (leftmost) { /* non-empty tree */
       529			struct sched_entity *se;
       530			se = rb_entry(leftmost, struct sched_entity, run_node);
       531	
       532			if (!curr)
       533				vruntime = se->vruntime;
       534			else
       535				vruntime = min_vruntime(vruntime, se->vruntime);
       536		}
       537	
       538		/* ensure we never gain time by being placed backwards. */
==>    539		cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);       
       540	#ifndef CONFIG_64BIT
       541		smp_wmb();
       542		cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;
       543	#endif
       544	}
       545	
       546	/*
       547	 * Enqueue an entity into the rb-tree:
       548	 */
       549	static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
       550	{
       551		struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
       552		struct rb_node *parent = NULL;
       553		struct sched_entity *entry;
       554		bool leftmost = true;
       555	
       556		/*
       557		 * Find the right place in the rbtree:
       558		 */
       559		while (*link) {

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:6054
      6034	static void migrate_task_rq_fair(struct task_struct *p)
      6035	{
      6036		/*
      6037		 * As blocked tasks retain absolute vruntime the migration needs to
      6038		 * deal with this by subtracting the old and adding the new
      6039		 * min_vruntime -- the latter is done by enqueue_entity() when placing
      6040		 * the task on the new runqueue.
      6041		 */
      6042		if (p->state == TASK_WAKING) {
      6043			struct sched_entity *se = &p->se;
      6044			struct cfs_rq *cfs_rq = cfs_rq_of(se);
      6045			u64 min_vruntime;
      6046	
      6047	#ifndef CONFIG_64BIT
      6048			u64 min_vruntime_copy;
      6049	
      6050			do {
      6051				min_vruntime_copy = cfs_rq->min_vruntime_copy;
      6052				smp_rmb();
      6053				min_vruntime = cfs_rq->min_vruntime;
==>   6054			} while (min_vruntime != min_vruntime_copy);       
      6055	#else
      6056			min_vruntime = cfs_rq->min_vruntime;
      6057	#endif
      6058	
      6059			se->vruntime -= min_vruntime;
      6060		}
      6061	
      6062		/*
      6063		 * We are supposed to update the task to "current" time, then its up to date
      6064		 * and ready to go to new CPU/cfs_rq. But we have difficulty in getting
      6065		 * what current time is, so simply throw away the out-of-date time. This
      6066		 * will result in the wakee task is less decayed, but giving the wakee more
      6067		 * load sounds not bad.
      6068		 */
      6069		remove_entity_load_avg(&p->se);
      6070	
      6071		/* Tell new CPU we are migrated */
      6072		p->se.avg.last_update_time = 0;
      6073	
      6074		/* We have migrated, no longer consider this task hot */


====================================================================================================================================================================================
Total: 72	Addresses: c10d8e19 c10db849
36	0xc10d8e19: ___update_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:2992
36	0xc10db849: cfs_rq_last_update_time at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3487

36	c10d8e19 ___update_load_avg T: trace_20241114_135827_3_3_13.txt S: 13 I1: 3 I2: 3 IP1: c10d8e19 IP2: c10db849 PMA1: 365ad43c PMA2: 365ad43c CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 192364 IC2: 191734
36	c10db849 cfs_rq_last_update_time T: trace_20241114_135827_3_3_13.txt S: 13 I1: 3 I2: 3 IP1: c10d8e19 IP2: c10db849 PMA1: 365ad43c PMA2: 365ad43c CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 192364 IC2: 191734

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:2992
      2972		u64 delta;
      2973	
      2974		delta = now - sa->last_update_time;
      2975		/*
      2976		 * This should only happen when time goes backwards, which it
      2977		 * unfortunately does during sched clock init when we swap over to TSC.
      2978		 */
      2979		if ((s64)delta < 0) {
      2980			sa->last_update_time = now;
      2981			return 0;
      2982		}
      2983	
      2984		/*
      2985		 * Use 1024ns as the unit of measurement since it's a reasonable
      2986		 * approximation of 1us and fast to compute.
      2987		 */
      2988		delta >>= 10;
      2989		if (!delta)
      2990			return 0;
      2991	
==>   2992		sa->last_update_time += delta << 10;       
      2993	
      2994		/*
      2995		 * running is a subset of runnable (weight) so running can't be set if
      2996		 * runnable is clear. But there are some corner cases where the current
      2997		 * se has been already dequeued but cfs_rq->curr still points to it.
      2998		 * This means that weight will be 0 but not running for a sched_entity
      2999		 * but also for a cfs_rq if the latter becomes idle. As an example,
      3000		 * this happens during idle_balance() which calls
      3001		 * update_blocked_averages()
      3002		 */
      3003		if (!weight)
      3004			running = 0;
      3005	
      3006		/*
      3007		 * Now we know we crossed measurement unit boundaries. The *_avg
      3008		 * accrues by two steps:
      3009		 *
      3010		 * Step 1: accumulate *_sum since last_update_time. If we haven't
      3011		 * crossed period boundaries, finish.
      3012		 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3487
      3467	/* Remove the runnable load generated by se from cfs_rq's runnable load average */
      3468	static inline void
      3469	dequeue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3470	{
      3471		cfs_rq->runnable_load_avg =
      3472			max_t(long, cfs_rq->runnable_load_avg - se->avg.load_avg, 0);
      3473		cfs_rq->runnable_load_sum =
      3474			max_t(s64,  cfs_rq->runnable_load_sum - se->avg.load_sum, 0);
      3475	}
      3476	
      3477	#ifndef CONFIG_64BIT
      3478	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3479	{
      3480		u64 last_update_time_copy;
      3481		u64 last_update_time;
      3482	
      3483		do {
      3484			last_update_time_copy = cfs_rq->load_last_update_time_copy;
      3485			smp_rmb();
      3486			last_update_time = cfs_rq->avg.last_update_time;
==>   3487		} while (last_update_time != last_update_time_copy);       
      3488	
      3489		return last_update_time;
      3490	}
      3491	#else
      3492	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3493	{
      3494		return cfs_rq->avg.last_update_time;
      3495	}
      3496	#endif
      3497	
      3498	/*
      3499	 * Synchronize entity load avg of dequeued entity without locking
      3500	 * the previous rq.
      3501	 */
      3502	void sync_entity_load_avg(struct sched_entity *se)
      3503	{
      3504		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3505		u64 last_update_time;
      3506	
      3507		last_update_time = cfs_rq_last_update_time(cfs_rq);


====================================================================================================================================================================================
Total: 90	Addresses: c1125c3a c11256b8 c112593a
10	0xc1125938: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
35	0xc1125c38: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
45	0xc11256b6: csd_unlock at /root/2017-17712-i386/linux-4.14/kernel/smp.c:126

10	c112593a __read_once_size T: trace_20241114_141414_3_3_44.txt S: 44 I1: 3 I2: 3 IP1: c11256b8 IP2: c112593a PMA1: 31f79edc PMA2: 31f79edc CPU1: 3 CPU2: 2 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 891358 IC2: 890991
35	c1125c3a __read_once_size T: trace_20241114_141619_3_2_15.txt S: 15 I1: 3 I2: 2 IP1: c11256b8 IP2: c1125c3a PMA1: 365ae52c PMA2: 365ae52c CPU1: 0 CPU2: 2 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 775244 IC2: 774620
45	c11256b8 __write_once_size T: trace_20241114_141619_3_2_15.txt S: 15 I1: 3 I2: 2 IP1: c11256b8 IP2: c1125c3a PMA1: 365ae52c PMA2: 365ae52c CPU1: 0 CPU2: 2 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 775244 IC2: 774620

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/smp.c:126
       106	static __always_inline void csd_lock_wait(call_single_data_t *csd)
       107	{
       108		smp_cond_load_acquire(&csd->flags, !(VAL & CSD_FLAG_LOCK));
       109	}
       110	
       111	static __always_inline void csd_lock(call_single_data_t *csd)
       112	{
       113		csd_lock_wait(csd);
       114		csd->flags |= CSD_FLAG_LOCK;
       115	
       116		/*
       117		 * prevent CPU from reordering the above assignment
       118		 * to ->flags with any subsequent assignments to other
       119		 * fields of the specified call_single_data_t structure:
       120		 */
       121		smp_wmb();
       122	}
       123	
       124	static __always_inline void csd_unlock(call_single_data_t *csd)
       125	{
==>    126		WARN_ON(!(csd->flags & CSD_FLAG_LOCK));       
       127	
       128		/*
       129		 * ensure we're all done before releasing data:
       130		 */
       131		smp_store_release(&csd->flags, 0);
       132	}
       133	
       134	static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);
       135	
       136	/*
       137	 * Insert a previously allocated call_single_data_t element
       138	 * for execution on the given CPU. data must already have
       139	 * ->func, ->info, and ->flags set.
       140	 */
       141	static int generic_exec_single(int cpu, call_single_data_t *csd,
       142				       smp_call_func_t func, void *info)
       143	{
       144		if (cpu == smp_processor_id()) {
       145			unsigned long flags;
       146	


====================================================================================================================================================================================
Total: 112	Addresses: c10dd0bd c10dd0c6 c10dc668 c10d966b
14	0xc10dd0ba: update_cfs_rq_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3352
14	0xc10d9664: update_cfs_rq_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3352
28	0xc10dd0c2: update_cfs_rq_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3354
56	0xc10dc65d: remove_entity_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3529

14	c10dd0bd __read_once_size T: trace_20241114_135845_2_2_39.txt S: 39 I1: 2 I2: 2 IP1: c10dd0bd IP2: c10dc668 PMA1: 365ad46c PMA2: 365ad46c CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 178883 IC2: 153430
14	c10d966b __read_once_size T: trace_20241114_135845_2_2_39.txt S: 39 I1: 2 I2: 2 IP1: c10d966b IP2: c10dc668 PMA1: 365ad46c PMA2: 365ad46c CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 179765 IC2: 153430
28	c10dd0c6 atomic_xchg T: trace_20241114_135845_2_2_39.txt S: 39 I1: 2 I2: 2 IP1: c10dd0c6 IP2: c10dc668 PMA1: 365ad46c PMA2: 365ad46c CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 178887 IC2: 153430
56	c10dc668 atomic_add T: trace_20241114_135845_2_2_39.txt S: 39 I1: 2 I2: 2 IP1: c10d966b IP2: c10dc668 PMA1: 365ad46c PMA2: 365ad46c CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 179765 IC2: 153430

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3352
      3332	/**
      3333	 * update_cfs_rq_load_avg - update the cfs_rq's load/util averages
      3334	 * @now: current time, as per cfs_rq_clock_task()
      3335	 * @cfs_rq: cfs_rq to update
      3336	 *
      3337	 * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)
      3338	 * avg. The immediate corollary is that all (fair) tasks must be attached, see
      3339	 * post_init_entity_util_avg().
      3340	 *
      3341	 * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.
      3342	 *
      3343	 * Returns true if the load decayed or we removed load.
      3344	 *
      3345	 * Since both these conditions indicate a changed cfs_rq->avg.load we should
      3346	 * call update_tg_load_avg() when this function returns true.
      3347	 */
      3348	static inline int
      3349	update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
      3350	{
      3351		struct sched_avg *sa = &cfs_rq->avg;
==>   3352		int decayed, removed_load = 0, removed_util = 0;       
      3353	
      3354		if (atomic_long_read(&cfs_rq->removed_load_avg)) {
      3355			s64 r = atomic_long_xchg(&cfs_rq->removed_load_avg, 0);
      3356			sub_positive(&sa->load_avg, r);
      3357			sub_positive(&sa->load_sum, r * LOAD_AVG_MAX);
      3358			removed_load = 1;
      3359			set_tg_cfs_propagate(cfs_rq);
      3360		}
      3361	
      3362		if (atomic_long_read(&cfs_rq->removed_util_avg)) {
      3363			long r = atomic_long_xchg(&cfs_rq->removed_util_avg, 0);
      3364			sub_positive(&sa->util_avg, r);
      3365			sub_positive(&sa->util_sum, r * LOAD_AVG_MAX);
      3366			removed_util = 1;
      3367			set_tg_cfs_propagate(cfs_rq);
      3368		}
      3369	
      3370		decayed = __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
      3371	
      3372	#ifndef CONFIG_64BIT

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3352
      3332	/**
      3333	 * update_cfs_rq_load_avg - update the cfs_rq's load/util averages
      3334	 * @now: current time, as per cfs_rq_clock_task()
      3335	 * @cfs_rq: cfs_rq to update
      3336	 *
      3337	 * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)
      3338	 * avg. The immediate corollary is that all (fair) tasks must be attached, see
      3339	 * post_init_entity_util_avg().
      3340	 *
      3341	 * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.
      3342	 *
      3343	 * Returns true if the load decayed or we removed load.
      3344	 *
      3345	 * Since both these conditions indicate a changed cfs_rq->avg.load we should
      3346	 * call update_tg_load_avg() when this function returns true.
      3347	 */
      3348	static inline int
      3349	update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
      3350	{
      3351		struct sched_avg *sa = &cfs_rq->avg;
==>   3352		int decayed, removed_load = 0, removed_util = 0;       
      3353	
      3354		if (atomic_long_read(&cfs_rq->removed_load_avg)) {
      3355			s64 r = atomic_long_xchg(&cfs_rq->removed_load_avg, 0);
      3356			sub_positive(&sa->load_avg, r);
      3357			sub_positive(&sa->load_sum, r * LOAD_AVG_MAX);
      3358			removed_load = 1;
      3359			set_tg_cfs_propagate(cfs_rq);
      3360		}
      3361	
      3362		if (atomic_long_read(&cfs_rq->removed_util_avg)) {
      3363			long r = atomic_long_xchg(&cfs_rq->removed_util_avg, 0);
      3364			sub_positive(&sa->util_avg, r);
      3365			sub_positive(&sa->util_sum, r * LOAD_AVG_MAX);
      3366			removed_util = 1;
      3367			set_tg_cfs_propagate(cfs_rq);
      3368		}
      3369	
      3370		decayed = __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
      3371	
      3372	#ifndef CONFIG_64BIT

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3354
      3334	 * @now: current time, as per cfs_rq_clock_task()
      3335	 * @cfs_rq: cfs_rq to update
      3336	 *
      3337	 * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)
      3338	 * avg. The immediate corollary is that all (fair) tasks must be attached, see
      3339	 * post_init_entity_util_avg().
      3340	 *
      3341	 * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.
      3342	 *
      3343	 * Returns true if the load decayed or we removed load.
      3344	 *
      3345	 * Since both these conditions indicate a changed cfs_rq->avg.load we should
      3346	 * call update_tg_load_avg() when this function returns true.
      3347	 */
      3348	static inline int
      3349	update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
      3350	{
      3351		struct sched_avg *sa = &cfs_rq->avg;
      3352		int decayed, removed_load = 0, removed_util = 0;
      3353	
==>   3354		if (atomic_long_read(&cfs_rq->removed_load_avg)) {       
      3355			s64 r = atomic_long_xchg(&cfs_rq->removed_load_avg, 0);
      3356			sub_positive(&sa->load_avg, r);
      3357			sub_positive(&sa->load_sum, r * LOAD_AVG_MAX);
      3358			removed_load = 1;
      3359			set_tg_cfs_propagate(cfs_rq);
      3360		}
      3361	
      3362		if (atomic_long_read(&cfs_rq->removed_util_avg)) {
      3363			long r = atomic_long_xchg(&cfs_rq->removed_util_avg, 0);
      3364			sub_positive(&sa->util_avg, r);
      3365			sub_positive(&sa->util_sum, r * LOAD_AVG_MAX);
      3366			removed_util = 1;
      3367			set_tg_cfs_propagate(cfs_rq);
      3368		}
      3369	
      3370		decayed = __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
      3371	
      3372	#ifndef CONFIG_64BIT
      3373		smp_wmb();
      3374		cfs_rq->load_last_update_time_copy = sa->last_update_time;

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3529
      3509	}
      3510	
      3511	/*
      3512	 * Task first catches up with cfs_rq, and then subtract
      3513	 * itself from the cfs_rq (task must be off the queue now).
      3514	 */
      3515	void remove_entity_load_avg(struct sched_entity *se)
      3516	{
      3517		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3518	
      3519		/*
      3520		 * tasks cannot exit without having gone through wake_up_new_task() ->
      3521		 * post_init_entity_util_avg() which will have added things to the
      3522		 * cfs_rq, so we can remove unconditionally.
      3523		 *
      3524		 * Similarly for groups, they will have passed through
      3525		 * post_init_entity_util_avg() before unregister_sched_fair_group()
      3526		 * calls this.
      3527		 */
      3528	
==>   3529		sync_entity_load_avg(se);       
      3530		atomic_long_add(se->avg.load_avg, &cfs_rq->removed_load_avg);
      3531		atomic_long_add(se->avg.util_avg, &cfs_rq->removed_util_avg);
      3532	}
      3533	
      3534	static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)
      3535	{
      3536		return cfs_rq->runnable_load_avg;
      3537	}
      3538	
      3539	static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)
      3540	{
      3541		return cfs_rq->avg.load_avg;
      3542	}
      3543	
      3544	static int idle_balance(struct rq *this_rq, struct rq_flags *rf);
      3545	
      3546	#else /* CONFIG_SMP */
      3547	
      3548	static inline int
      3549	update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)


====================================================================================================================================================================================
Total: 176	Addresses: c10dd197 c10d9732 c10db840
41	0xc10d9732: update_cfs_rq_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3374
47	0xc10dd197: update_cfs_rq_load_avg at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3374
88	0xc10db840: cfs_rq_last_update_time at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:3484

41	c10d9732 update_cfs_rq_load_avg T: trace_20241114_135900_2_3_54.txt S: 54 I1: 2 I2: 3 IP1: c10d9732 IP2: c10db840 PMA1: 365ad474 PMA2: 365ad474 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 175099 IC2: 172050
47	c10dd197 update_cfs_rq_load_avg T: trace_20241114_135900_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c10db840 IP2: c10dd197 PMA1: 365ad474 PMA2: 365ad474 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 185177 IC2: 185175
88	c10db840 cfs_rq_last_update_time T: trace_20241114_135900_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c10db840 IP2: c10dd197 PMA1: 365ad474 PMA2: 365ad474 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 185177 IC2: 185175

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3374
      3354		if (atomic_long_read(&cfs_rq->removed_load_avg)) {
      3355			s64 r = atomic_long_xchg(&cfs_rq->removed_load_avg, 0);
      3356			sub_positive(&sa->load_avg, r);
      3357			sub_positive(&sa->load_sum, r * LOAD_AVG_MAX);
      3358			removed_load = 1;
      3359			set_tg_cfs_propagate(cfs_rq);
      3360		}
      3361	
      3362		if (atomic_long_read(&cfs_rq->removed_util_avg)) {
      3363			long r = atomic_long_xchg(&cfs_rq->removed_util_avg, 0);
      3364			sub_positive(&sa->util_avg, r);
      3365			sub_positive(&sa->util_sum, r * LOAD_AVG_MAX);
      3366			removed_util = 1;
      3367			set_tg_cfs_propagate(cfs_rq);
      3368		}
      3369	
      3370		decayed = __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
      3371	
      3372	#ifndef CONFIG_64BIT
      3373		smp_wmb();
==>   3374		cfs_rq->load_last_update_time_copy = sa->last_update_time;       
      3375	#endif
      3376	
      3377		if (decayed || removed_util)
      3378			cfs_rq_util_change(cfs_rq);
      3379	
      3380		return decayed || removed_load;
      3381	}
      3382	
      3383	/*
      3384	 * Optional action to be done while updating the load average
      3385	 */
      3386	#define UPDATE_TG	0x1
      3387	#define SKIP_AGE_LOAD	0x2
      3388	
      3389	/* Update task and its cfs_rq load average */
      3390	static inline void update_load_avg(struct sched_entity *se, int flags)
      3391	{
      3392		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3393		u64 now = cfs_rq_clock_task(cfs_rq);
      3394		struct rq *rq = rq_of(cfs_rq);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3374
      3354		if (atomic_long_read(&cfs_rq->removed_load_avg)) {
      3355			s64 r = atomic_long_xchg(&cfs_rq->removed_load_avg, 0);
      3356			sub_positive(&sa->load_avg, r);
      3357			sub_positive(&sa->load_sum, r * LOAD_AVG_MAX);
      3358			removed_load = 1;
      3359			set_tg_cfs_propagate(cfs_rq);
      3360		}
      3361	
      3362		if (atomic_long_read(&cfs_rq->removed_util_avg)) {
      3363			long r = atomic_long_xchg(&cfs_rq->removed_util_avg, 0);
      3364			sub_positive(&sa->util_avg, r);
      3365			sub_positive(&sa->util_sum, r * LOAD_AVG_MAX);
      3366			removed_util = 1;
      3367			set_tg_cfs_propagate(cfs_rq);
      3368		}
      3369	
      3370		decayed = __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
      3371	
      3372	#ifndef CONFIG_64BIT
      3373		smp_wmb();
==>   3374		cfs_rq->load_last_update_time_copy = sa->last_update_time;       
      3375	#endif
      3376	
      3377		if (decayed || removed_util)
      3378			cfs_rq_util_change(cfs_rq);
      3379	
      3380		return decayed || removed_load;
      3381	}
      3382	
      3383	/*
      3384	 * Optional action to be done while updating the load average
      3385	 */
      3386	#define UPDATE_TG	0x1
      3387	#define SKIP_AGE_LOAD	0x2
      3388	
      3389	/* Update task and its cfs_rq load average */
      3390	static inline void update_load_avg(struct sched_entity *se, int flags)
      3391	{
      3392		struct cfs_rq *cfs_rq = cfs_rq_of(se);
      3393		u64 now = cfs_rq_clock_task(cfs_rq);
      3394		struct rq *rq = rq_of(cfs_rq);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:3484
      3464		}
      3465	}
      3466	
      3467	/* Remove the runnable load generated by se from cfs_rq's runnable load average */
      3468	static inline void
      3469	dequeue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
      3470	{
      3471		cfs_rq->runnable_load_avg =
      3472			max_t(long, cfs_rq->runnable_load_avg - se->avg.load_avg, 0);
      3473		cfs_rq->runnable_load_sum =
      3474			max_t(s64,  cfs_rq->runnable_load_sum - se->avg.load_sum, 0);
      3475	}
      3476	
      3477	#ifndef CONFIG_64BIT
      3478	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3479	{
      3480		u64 last_update_time_copy;
      3481		u64 last_update_time;
      3482	
      3483		do {
==>   3484			last_update_time_copy = cfs_rq->load_last_update_time_copy;       
      3485			smp_rmb();
      3486			last_update_time = cfs_rq->avg.last_update_time;
      3487		} while (last_update_time != last_update_time_copy);
      3488	
      3489		return last_update_time;
      3490	}
      3491	#else
      3492	static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)
      3493	{
      3494		return cfs_rq->avg.last_update_time;
      3495	}
      3496	#endif
      3497	
      3498	/*
      3499	 * Synchronize entity load avg of dequeued entity without locking
      3500	 * the previous rq.
      3501	 */
      3502	void sync_entity_load_avg(struct sched_entity *se)
      3503	{
      3504		struct cfs_rq *cfs_rq = cfs_rq_of(se);


====================================================================================================================================================================================
Total: 568	Addresses: c11c0d71 c11c0d57 c1122ed4
142	0xc11c0d54: compound_head at /root/2017-17712-i386/linux-4.14/./include/linux/page-flags.h:150
142	0xc1122ed1: compound_head at /root/2017-17712-i386/linux-4.14/./include/linux/page-flags.h:150
284	0xc11c0d54: compound_head at /root/2017-17712-i386/linux-4.14/./include/linux/page-flags.h:150

142	c11c0d71 atomic_try_cmpxchg T: trace_20241114_141400_2_3_55.txt S: 55 I1: 2 I2: 3 IP1: c11c0d71 IP2: c11c0d57 PMA1: 36880f60 PMA2: 36880f60 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120836 IC2: 117997
142	c1122ed4 atomic_dec_and_test T: trace_20241114_141400_2_3_55.txt S: 55 I1: 2 I2: 3 IP1: c1122ed4 IP2: c11c0d57 PMA1: 36880f60 PMA2: 36880f60 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120979 IC2: 117997
284	c11c0d57 __read_once_size T: trace_20241114_141400_2_3_55.txt S: 55 I1: 2 I2: 3 IP1: c1122ed4 IP2: c11c0d57 PMA1: 36880f60 PMA2: 36880f60 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 120979 IC2: 117997

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/page-flags.h:150
       130	
       131		/* SLOB */
       132		PG_slob_free = PG_private,
       133	
       134		/* Compound pages. Stored in first tail page's flags */
       135		PG_double_map = PG_private_2,
       136	
       137		/* non-lru isolated movable page */
       138		PG_isolated = PG_reclaim,
       139	};
       140	
       141	#ifndef __GENERATING_BOUNDS_H
       142	
       143	struct page;	/* forward declaration */
       144	
       145	static inline struct page *compound_head(struct page *page)
       146	{
       147		unsigned long head = READ_ONCE(page->compound_head);
       148	
       149		if (unlikely(head & 1))
==>    150			return (struct page *) (head - 1);       
       151		return page;
       152	}
       153	
       154	static __always_inline int PageTail(struct page *page)
       155	{
       156		return READ_ONCE(page->compound_head) & 1;
       157	}
       158	
       159	static __always_inline int PageCompound(struct page *page)
       160	{
       161		return test_bit(PG_head, &page->flags) || PageTail(page);
       162	}
       163	
       164	/*
       165	 * Page flags policies wrt compound pages
       166	 *
       167	 * PF_ANY:
       168	 *     the page flag is relevant for small, head and tail pages.
       169	 *
       170	 * PF_HEAD:

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/page-flags.h:150
       130	
       131		/* SLOB */
       132		PG_slob_free = PG_private,
       133	
       134		/* Compound pages. Stored in first tail page's flags */
       135		PG_double_map = PG_private_2,
       136	
       137		/* non-lru isolated movable page */
       138		PG_isolated = PG_reclaim,
       139	};
       140	
       141	#ifndef __GENERATING_BOUNDS_H
       142	
       143	struct page;	/* forward declaration */
       144	
       145	static inline struct page *compound_head(struct page *page)
       146	{
       147		unsigned long head = READ_ONCE(page->compound_head);
       148	
       149		if (unlikely(head & 1))
==>    150			return (struct page *) (head - 1);       
       151		return page;
       152	}
       153	
       154	static __always_inline int PageTail(struct page *page)
       155	{
       156		return READ_ONCE(page->compound_head) & 1;
       157	}
       158	
       159	static __always_inline int PageCompound(struct page *page)
       160	{
       161		return test_bit(PG_head, &page->flags) || PageTail(page);
       162	}
       163	
       164	/*
       165	 * Page flags policies wrt compound pages
       166	 *
       167	 * PF_ANY:
       168	 *     the page flag is relevant for small, head and tail pages.
       169	 *
       170	 * PF_HEAD:

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/page-flags.h:150
       130	
       131		/* SLOB */
       132		PG_slob_free = PG_private,
       133	
       134		/* Compound pages. Stored in first tail page's flags */
       135		PG_double_map = PG_private_2,
       136	
       137		/* non-lru isolated movable page */
       138		PG_isolated = PG_reclaim,
       139	};
       140	
       141	#ifndef __GENERATING_BOUNDS_H
       142	
       143	struct page;	/* forward declaration */
       144	
       145	static inline struct page *compound_head(struct page *page)
       146	{
       147		unsigned long head = READ_ONCE(page->compound_head);
       148	
       149		if (unlikely(head & 1))
==>    150			return (struct page *) (head - 1);       
       151		return page;
       152	}
       153	
       154	static __always_inline int PageTail(struct page *page)
       155	{
       156		return READ_ONCE(page->compound_head) & 1;
       157	}
       158	
       159	static __always_inline int PageCompound(struct page *page)
       160	{
       161		return test_bit(PG_head, &page->flags) || PageTail(page);
       162	}
       163	
       164	/*
       165	 * Page flags policies wrt compound pages
       166	 *
       167	 * PF_ANY:
       168	 *     the page flag is relevant for small, head and tail pages.
       169	 *
       170	 * PF_HEAD:


====================================================================================================================================================================================
Total: 940	Addresses: c10d0ef7 c10d3aa2 c10d3a90
3	0xc10d3a8a: try_to_wake_up at /root/2017-17712-i386/linux-4.14/kernel/sched/core.c:2012
467	0xc10d3aa0: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
470	0xc10d0ef5: finish_lock_switch at /root/2017-17712-i386/linux-4.14/kernel/sched/sched.h:1336

3	c10d3a90 __read_once_size T: trace_20241114_141340_2_2_8.txt S: 8 I1: 2 I2: 2 IP1: c10d0ef7 IP2: c10d3a90 PMA1: 3583ca54 PMA2: 3583ca54 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 169293 IC2: 169287
467	c10d3aa2 __read_once_size T: trace_20241114_141406_3_2_29.txt S: 29 I1: 3 I2: 2 IP1: c10d0ef7 IP2: c10d3aa2 PMA1: 32f2c194 PMA2: 32f2c194 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 223976 IC2: 223118
470	c10d0ef7 __write_once_size T: trace_20241114_141406_3_2_29.txt S: 29 I1: 3 I2: 2 IP1: c10d0ef7 IP2: c10d3aa2 PMA1: 32f2c194 PMA2: 32f2c194 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 223976 IC2: 223118

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/core.c:2012
      1992		 * be possible to, falsely, observe p->on_rq == 0 and get stuck
      1993		 * in smp_cond_load_acquire() below.
      1994		 *
      1995		 * sched_ttwu_pending()                 try_to_wake_up()
      1996		 *   [S] p->on_rq = 1;                  [L] P->state
      1997		 *       UNLOCK rq->lock  -----.
      1998		 *                              \
      1999		 *				 +---   RMB
      2000		 * schedule()                   /
      2001		 *       LOCK rq->lock    -----'
      2002		 *       UNLOCK rq->lock
      2003		 *
      2004		 * [task p]
      2005		 *   [S] p->state = UNINTERRUPTIBLE     [L] p->on_rq
      2006		 *
      2007		 * Pairs with the UNLOCK+LOCK on rq->lock from the
      2008		 * last wakeup of our task and the schedule that got our task
      2009		 * current.
      2010		 */
      2011		smp_rmb();
==>   2012		if (p->on_rq && ttwu_remote(p, wake_flags))       
      2013			goto stat;
      2014	
      2015	#ifdef CONFIG_SMP
      2016		/*
      2017		 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
      2018		 * possible to, falsely, observe p->on_cpu == 0.
      2019		 *
      2020		 * One must be running (->on_cpu == 1) in order to remove oneself
      2021		 * from the runqueue.
      2022		 *
      2023		 *  [S] ->on_cpu = 1;	[L] ->on_rq
      2024		 *      UNLOCK rq->lock
      2025		 *			RMB
      2026		 *      LOCK   rq->lock
      2027		 *  [S] ->on_rq = 0;    [L] ->on_cpu
      2028		 *
      2029		 * Pairs with the full barrier implied in the UNLOCK+LOCK on rq->lock
      2030		 * from the consecutive calls to schedule(); the first switching to our
      2031		 * task, the second putting it to sleep.
      2032		 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/sched.h:1336
      1316		 * finished.
      1317		 *
      1318		 * In particular, the load of prev->state in finish_task_switch() must
      1319		 * happen before this.
      1320		 *
      1321		 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
      1322		 */
      1323		smp_store_release(&prev->on_cpu, 0);
      1324	#endif
      1325	#ifdef CONFIG_DEBUG_SPINLOCK
      1326		/* this is a valid case when another task releases the spinlock */
      1327		rq->lock.owner = current;
      1328	#endif
      1329		/*
      1330		 * If we are tracking spinlock dependencies then we have to
      1331		 * fix up the runqueue lock - which gets 'carried over' from
      1332		 * prev into current:
      1333		 */
      1334		spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
      1335	
==>   1336		raw_spin_unlock_irq(&rq->lock);       
      1337	}
      1338	
      1339	/*
      1340	 * wake flags
      1341	 */
      1342	#define WF_SYNC		0x01		/* waker goes to sleep after wakeup */
      1343	#define WF_FORK		0x02		/* child wakeup after fork */
      1344	#define WF_MIGRATED	0x4		/* internal use, task got migrated */
      1345	
      1346	/*
      1347	 * To aid in avoiding the subversion of "niceness" due to uneven distribution
      1348	 * of tasks with abnormal "nice" values across CPUs the contribution that
      1349	 * each task makes to its run queue's load is weighted according to its
      1350	 * scheduling class and "nice" value. For SCHED_NORMAL tasks this is just a
      1351	 * scaled version of the new time slice allocation that they receive on time
      1352	 * slice expiry etc.
      1353	 */
      1354	
      1355	#define WEIGHT_IDLEPRIO                3
      1356	#define WMULT_IDLEPRIO         1431655765


====================================================================================================================================================================================
Total: 4348	Addresses: c26c654f c10ed6ee c26c659d c26c688d c26c6499 c10ed6e8
138	0xc26c659d: pv_queued_spin_unlock at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/paravirt.h:674
176	0xc10ed6ec: virt_spin_lock at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/qspinlock.h:63
434	0xc26c688d: pv_queued_spin_unlock at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/paravirt.h:674
959	0xc26c6489: __preempt_count_add at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/preempt.h:76
1112	0xc26c654f: pv_queued_spin_unlock at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/paravirt.h:674
1529	0xc10ed6db: _static_cpu_has at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/cpufeature.h:147

138	c26c659d pv_queued_spin_unlock T: trace_20241114_141400_2_3_59.txt S: 59 I1: 2 I2: 3 IP1: c26c659d IP2: c10ed6e8 PMA1: 365ad3c0 PMA2: 365ad3c0 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 206456 IC2: 204749
176	c10ed6ee atomic_cmpxchg T: trace_20241114_141404_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c10ed6e8 IP2: c10ed6ee PMA1: 32eb86f4 PMA2: 32eb86f4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 347875 IC2: 347846
434	c26c688d pv_queued_spin_unlock T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c688d IP2: c10ed6e8 PMA1: 32eb86e4 PMA2: 32eb86e4 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 1370335 IC2: 1370312
959	c26c6499 atomic_cmpxchg T: trace_20241114_141404_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c26c6499 IP2: c10ed6ee PMA1: 32eb86f4 PMA2: 32eb86f4 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 347850 IC2: 347846
1112	c26c654f pv_queued_spin_unlock T: trace_20241114_141413_3_3_32.txt S: 32 I1: 3 I2: 3 IP1: c26c654f IP2: c10ed6e8 PMA1: 31ceb740 PMA2: 31ceb740 CPU1: 2 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 668515 IC2: 668468
1529	c10ed6e8 __read_once_size T: trace_20241114_141413_3_3_32.txt S: 32 I1: 3 I2: 3 IP1: c26c654f IP2: c10ed6e8 PMA1: 31ceb740 PMA2: 31ceb740 CPU1: 2 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 668515 IC2: 668468

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/paravirt.h:674
       654	{
       655		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       656	}
       657	
       658	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       659					phys_addr_t phys, pgprot_t flags)
       660	{
       661		pv_mmu_ops.set_fixmap(idx, phys, flags);
       662	}
       663	
       664	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       665	
       666	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       667								u32 val)
       668	{
       669		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       670	}
       671	
       672	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       673	{
==>    674		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       675	}
       676	
       677	static __always_inline void pv_wait(u8 *ptr, u8 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       680	}
       681	
       682	static __always_inline void pv_kick(int cpu)
       683	{
       684		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       685	}
       686	
       687	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       688	{
       689		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       690	}
       691	
       692	#endif /* SMP && PARAVIRT_SPINLOCKS */
       693	
       694	#ifdef CONFIG_X86_32

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/qspinlock.h:63
        43	static inline void queued_spin_unlock(struct qspinlock *lock)
        44	{
        45		native_queued_spin_unlock(lock);
        46	}
        47	#endif
        48	
        49	#ifdef CONFIG_PARAVIRT
        50	#define virt_spin_lock virt_spin_lock
        51	static inline bool virt_spin_lock(struct qspinlock *lock)
        52	{
        53		if (!static_cpu_has(X86_FEATURE_HYPERVISOR))
        54			return false;
        55	
        56		/*
        57		 * On hypervisors without PARAVIRT_SPINLOCKS support we fall
        58		 * back to a Test-and-Set spinlock, because fair locks have
        59		 * horrible lock 'holder' preemption issues.
        60		 */
        61	
        62		do {
==>     63			while (atomic_read(&lock->val) != 0)       
        64				cpu_relax();
        65		} while (atomic_cmpxchg(&lock->val, 0, _Q_LOCKED_VAL) != 0);
        66	
        67		return true;
        68	}
        69	#endif /* CONFIG_PARAVIRT */
        70	
        71	#include <asm-generic/qspinlock.h>
        72	
        73	#endif /* _ASM_X86_QSPINLOCK_H */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/paravirt.h:674
       654	{
       655		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       656	}
       657	
       658	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       659					phys_addr_t phys, pgprot_t flags)
       660	{
       661		pv_mmu_ops.set_fixmap(idx, phys, flags);
       662	}
       663	
       664	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       665	
       666	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       667								u32 val)
       668	{
       669		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       670	}
       671	
       672	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       673	{
==>    674		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       675	}
       676	
       677	static __always_inline void pv_wait(u8 *ptr, u8 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       680	}
       681	
       682	static __always_inline void pv_kick(int cpu)
       683	{
       684		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       685	}
       686	
       687	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       688	{
       689		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       690	}
       691	
       692	#endif /* SMP && PARAVIRT_SPINLOCKS */
       693	
       694	#ifdef CONFIG_X86_32

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/paravirt.h:674
       654	{
       655		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       656	}
       657	
       658	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       659					phys_addr_t phys, pgprot_t flags)
       660	{
       661		pv_mmu_ops.set_fixmap(idx, phys, flags);
       662	}
       663	
       664	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       665	
       666	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       667								u32 val)
       668	{
       669		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       670	}
       671	
       672	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       673	{
==>    674		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       675	}
       676	
       677	static __always_inline void pv_wait(u8 *ptr, u8 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       680	}
       681	
       682	static __always_inline void pv_kick(int cpu)
       683	{
       684		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       685	}
       686	
       687	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       688	{
       689		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       690	}
       691	
       692	#endif /* SMP && PARAVIRT_SPINLOCKS */
       693	
       694	#ifdef CONFIG_X86_32

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/cpufeature.h:147
       127	
       128	#define set_cpu_cap(c, bit)	set_bit(bit, (unsigned long *)((c)->x86_capability))
       129	#define clear_cpu_cap(c, bit)	clear_bit(bit, (unsigned long *)((c)->x86_capability))
       130	#define setup_clear_cpu_cap(bit) do { \
       131		clear_cpu_cap(&boot_cpu_data, bit);	\
       132		set_bit(bit, (unsigned long *)cpu_caps_cleared); \
       133	} while (0)
       134	#define setup_force_cpu_cap(bit) do { \
       135		set_cpu_cap(&boot_cpu_data, bit);	\
       136		set_bit(bit, (unsigned long *)cpu_caps_set);	\
       137	} while (0)
       138	
       139	#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_X86_FAST_FEATURE_TESTS)
       140	/*
       141	 * Static testing of CPU features.  Used the same as boot_cpu_has().
       142	 * These will statically patch the target code for additional
       143	 * performance.
       144	 */
       145	static __always_inline __pure bool _static_cpu_has(u16 bit)
       146	{
==>    147			asm_volatile_goto("1: jmp 6f\n"       
       148				 "2:\n"
       149				 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
       150				         "((5f-4f) - (2b-1b)),0x90\n"
       151				 "3:\n"
       152				 ".section .altinstructions,\"a\"\n"
       153				 " .long 1b - .\n"		/* src offset */
       154				 " .long 4f - .\n"		/* repl offset */
       155				 " .word %P1\n"			/* always replace */
       156				 " .byte 3b - 1b\n"		/* src len */
       157				 " .byte 5f - 4f\n"		/* repl len */
       158				 " .byte 3b - 2b\n"		/* pad len */
       159				 ".previous\n"
       160				 ".section .altinstr_replacement,\"ax\"\n"
       161				 "4: jmp %l[t_no]\n"
       162				 "5:\n"
       163				 ".previous\n"
       164				 ".section .altinstructions,\"a\"\n"
       165				 " .long 1b - .\n"		/* src offset */
       166				 " .long 0\n"			/* no replacement */
       167				 " .word %P0\n"			/* feature bit */


====================================================================================================================================================================================
Total: 5620	Addresses: c26c3fd1 c26c3f5e c26c3499 c26c330d c26c33ed c26c346f c26c3f07 c26c3ef5 c26c33b9 c26c3388 c26c3530 c26c3456 c26c3349 c26c3226 c26c366e c26c3192 c26c3540 c10ecf7a c26c34b9 c26c31c2 c10ecfcd c26c32a0
1	0xc26c3f59: __mutex_handoff at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:196
2	0xc26c3497: __mutex_trylock_or_owner at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:110
3	0xc26c3eee: __mutex_unlock_slowpath at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:1015
4	0xc26c330a: __mutex_lock_common at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:845
4	0xc26c33eb: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
5	0xc26c352a: __mutex_lock_common at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:764
5	0xc26c34b7: __mutex_trylock_or_owner at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:110
6	0xc26c33b7: __mutex_trylock_or_owner at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:110
9	0xc26c3221: spin_lock at /root/2017-17712-i386/linux-4.14/./include/linux/spinlock.h:317
10	0xc26c329d: signal_pending_state at /root/2017-17712-i386/linux-4.14/./include/linux/sched/signal.h:335
11	0xc26c3f04: __owner_flags at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:74
12	0xc26c3386: __mutex_lock_common at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:862
31	0xc26c353d: __mutex_lock_common at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:840
127	0xc26c3347: __mutex_trylock_or_owner at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:110
163	0xc10ecf75: rcu_read_lock at /root/2017-17712-i386/linux-4.14/./include/linux/rcupdate.h:629
198	0xc26c31bd: rcu_read_lock at /root/2017-17712-i386/linux-4.14/./include/linux/rcupdate.h:629
254	0xc26c318b: __preempt_count_add at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/preempt.h:76
271	0xc10ecfcb: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
324	0xc26c3454: rep_nop at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/processor.h:635
364	0xc26c346d: __mutex_trylock_or_owner at /root/2017-17712-i386/linux-4.14/kernel/locking/mutex.c:110
1902	0xc26c3667: get_current at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/current.h:15
1914	0xc26c3fc8: get_current at /root/2017-17712-i386/linux-4.14/./arch/x86/include/asm/current.h:15

1	c26c3f5e atomic_cmpxchg T: trace_20241114_141406_3_2_45.txt S: 45 I1: 3 I2: 2 IP1: c10ecfcd IP2: c26c3f5e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 246797 IC2: 246796
2	c26c3499 atomic_cmpxchg T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c3499 IP2: c26c3f07 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1370271 IC2: 1370247
3	c26c3ef5 __read_once_size T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c3ef5 IP2: c26c3540 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1370239 IC2: 1369634
4	c26c330d __read_once_size T: trace_20241114_141410_3_3_38.txt S: 38 I1: 3 I2: 3 IP1: c26c330d IP2: c26c366e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 359523 IC2: 359513
4	c26c33ed __read_once_size T: trace_20241114_141407_3_2_62.txt S: 62 I1: 3 I2: 2 IP1: c26c33ed IP2: c26c3fd1 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 303822 IC2: 298136
5	c26c3530 atomic_or T: trace_20241114_141407_3_2_49.txt S: 49 I1: 3 I2: 2 IP1: c26c3349 IP2: c26c3530 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 545979 IC2: 545953
5	c26c34b9 atomic_cmpxchg T: trace_20241114_141410_3_3_38.txt S: 38 I1: 3 I2: 3 IP1: c26c3456 IP2: c26c34b9 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 359627 IC2: 359531
6	c26c33b9 atomic_cmpxchg T: trace_20241114_141412_3_3_13.txt S: 13 I1: 3 I2: 3 IP1: c26c33b9 IP2: c26c366e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 314601 IC2: 314592
9	c26c3226 __read_once_size T: trace_20241114_141412_3_3_13.txt S: 13 I1: 3 I2: 3 IP1: c26c366e IP2: c26c3226 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 314592 IC2: 314591
10	c26c32a0 __read_once_size T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c32a0 IP2: c26c3f07 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1370262 IC2: 1370247
11	c26c3f07 atomic_cmpxchg T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c3499 IP2: c26c3f07 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1370271 IC2: 1370247
12	c26c3388 atomic_and T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c10ecfcd IP2: c26c3388 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1477317 IC2: 1461810
31	c26c3540 atomic_or T: trace_20241114_141413_3_3_30.txt S: 30 I1: 3 I2: 3 IP1: c26c3f07 IP2: c26c3540 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1370247 IC2: 1369634
127	c26c3349 atomic_cmpxchg T: trace_20241114_141519_3_3_51.txt S: 51 I1: 3 I2: 3 IP1: c26c31c2 IP2: c26c3349 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 276000 IC2: 275963
163	c10ecf7a __read_once_size T: trace_20241114_141519_3_3_53.txt S: 53 I1: 3 I2: 3 IP1: c10ecf7a IP2: c26c366e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 248143 IC2: 248017
198	c26c31c2 __read_once_size T: trace_20241114_141519_3_3_53.txt S: 53 I1: 3 I2: 3 IP1: c26c31c2 IP2: c26c366e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 248054 IC2: 248017
254	c26c3192 __read_once_size T: trace_20241114_141519_3_3_53.txt S: 53 I1: 3 I2: 3 IP1: c26c3192 IP2: c26c366e PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 248035 IC2: 248017
271	c10ecfcd __read_once_size T: trace_20241114_141719_3_2_42.txt S: 42 I1: 3 I2: 2 IP1: c10ecfcd IP2: c26c3fd1 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 184699 IC2: 184696
324	c26c3456 __read_once_size T: trace_20241114_141719_3_2_42.txt S: 42 I1: 3 I2: 2 IP1: c26c3456 IP2: c26c3fd1 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 184732 IC2: 184696
364	c26c346f atomic_cmpxchg T: trace_20241114_141719_3_2_42.txt S: 42 I1: 3 I2: 2 IP1: c26c346f IP2: c26c3fd1 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 184741 IC2: 184696
1902	c26c366e atomic_cmpxchg T: trace_20241114_141519_3_3_54.txt S: 54 I1: 3 I2: 3 IP1: c26c366e IP2: c26c346f PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 184461 IC2: 158063
1914	c26c3fd1 atomic_cmpxchg T: trace_20241114_141719_3_2_42.txt S: 42 I1: 3 I2: 2 IP1: c26c346f IP2: c26c3fd1 PMA1: 32eb86e0 PMA2: 32eb86e0 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 184741 IC2: 184696

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:196
       176	 * Give up ownership to a specific task, when @task = NULL, this is equivalent
       177	 * to a regular unlock. Sets PICKUP on a handoff, clears HANDOF, preserves
       178	 * WAITERS. Provides RELEASE semantics like a regular unlock, the
       179	 * __mutex_trylock() provides a matching ACQUIRE semantics for the handoff.
       180	 */
       181	static void __mutex_handoff(struct mutex *lock, struct task_struct *task)
       182	{
       183		unsigned long owner = atomic_long_read(&lock->owner);
       184	
       185		for (;;) {
       186			unsigned long old, new;
       187	
       188	#ifdef CONFIG_DEBUG_MUTEXES
       189			DEBUG_LOCKS_WARN_ON(__owner_task(owner) != current);
       190			DEBUG_LOCKS_WARN_ON(owner & MUTEX_FLAG_PICKUP);
       191	#endif
       192	
       193			new = (owner & MUTEX_FLAG_WAITERS);
       194			new |= (unsigned long)task;
       195			if (task)
==>    196				new |= MUTEX_FLAG_PICKUP;       
       197	
       198			old = atomic_long_cmpxchg_release(&lock->owner, owner, new);
       199			if (old == owner)
       200				break;
       201	
       202			owner = old;
       203		}
       204	}
       205	
       206	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       207	/*
       208	 * We split the mutex lock/unlock logic into separate fastpath and
       209	 * slowpath functions, to reduce the register pressure on the fastpath.
       210	 * We also put the fastpath first in the kernel image, to make sure the
       211	 * branch is predicted by the CPU as default-untaken.
       212	 */
       213	static void __sched __mutex_lock_slowpath(struct mutex *lock);
       214	
       215	/**
       216	 * mutex_lock - acquire the mutex

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:1015
       995		might_sleep();
       996		ret = __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE,
       997				      0, ctx ? &ctx->dep_map : NULL, _RET_IP_,
       998				      ctx);
       999	
      1000		if (!ret && ctx && ctx->acquired > 1)
      1001			return ww_mutex_deadlock_injection(lock, ctx);
      1002	
      1003		return ret;
      1004	}
      1005	EXPORT_SYMBOL_GPL(ww_mutex_lock_interruptible);
      1006	
      1007	#endif
      1008	
      1009	/*
      1010	 * Release the lock, slowpath:
      1011	 */
      1012	static noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip)
      1013	{
      1014		struct task_struct *next = NULL;
==>   1015		DEFINE_WAKE_Q(wake_q);       
      1016		unsigned long owner;
      1017	
      1018		mutex_release(&lock->dep_map, 1, ip);
      1019	
      1020		/*
      1021		 * Release the lock before (potentially) taking the spinlock such that
      1022		 * other contenders can get on with things ASAP.
      1023		 *
      1024		 * Except when HANDOFF, in that case we must not clear the owner field,
      1025		 * but instead set it to the top waiter.
      1026		 */
      1027		owner = atomic_long_read(&lock->owner);
      1028		for (;;) {
      1029			unsigned long old;
      1030	
      1031	#ifdef CONFIG_DEBUG_MUTEXES
      1032			DEBUG_LOCKS_WARN_ON(__owner_task(owner) != current);
      1033			DEBUG_LOCKS_WARN_ON(owner & MUTEX_FLAG_PICKUP);
      1034	#endif
      1035	

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:845
       825	
       826			if (use_ww_ctx && ww_ctx && ww_ctx->acquired > 0) {
       827				ret = __ww_mutex_lock_check_stamp(lock, &waiter, ww_ctx);
       828				if (ret)
       829					goto err;
       830			}
       831	
       832			spin_unlock(&lock->wait_lock);
       833			schedule_preempt_disabled();
       834	
       835			/*
       836			 * ww_mutex needs to always recheck its position since its waiter
       837			 * list is not FIFO ordered.
       838			 */
       839			if ((use_ww_ctx && ww_ctx) || !first) {
       840				first = __mutex_waiter_is_first(lock, &waiter);
       841				if (first)
       842					__mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);
       843			}
       844	
==>    845			set_current_state(state);       
       846			/*
       847			 * Here we order against unlock; we must either see it change
       848			 * state back to RUNNING and fall through the next schedule(),
       849			 * or we must see its unlock and acquire.
       850			 */
       851			if (__mutex_trylock(lock) ||
       852			    (first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))
       853				break;
       854	
       855			spin_lock(&lock->wait_lock);
       856		}
       857		spin_lock(&lock->wait_lock);
       858	acquired:
       859		__set_current_state(TASK_RUNNING);
       860	
       861		mutex_remove_waiter(lock, &waiter, current);
       862		if (likely(list_empty(&lock->wait_list)))
       863			__mutex_clear_flag(lock, MUTEX_FLAGS);
       864	
       865		debug_mutex_free_waiter(&waiter);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:764
       744		struct ww_mutex *ww;
       745		int ret;
       746	
       747		might_sleep();
       748	
       749		ww = container_of(lock, struct ww_mutex, base);
       750		if (use_ww_ctx && ww_ctx) {
       751			if (unlikely(ww_ctx == READ_ONCE(ww->ctx)))
       752				return -EALREADY;
       753		}
       754	
       755		preempt_disable();
       756		mutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);
       757	
       758		if (__mutex_trylock(lock) ||
       759		    mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) {
       760			/* got the lock, yay! */
       761			lock_acquired(&lock->dep_map, ip);
       762			if (use_ww_ctx && ww_ctx)
       763				ww_mutex_set_context_fastpath(ww, ww_ctx);
==>    764			preempt_enable();       
       765			return 0;
       766		}
       767	
       768		spin_lock(&lock->wait_lock);
       769		/*
       770		 * After waiting to acquire the wait_lock, try again.
       771		 */
       772		if (__mutex_trylock(lock)) {
       773			if (use_ww_ctx && ww_ctx)
       774				__ww_mutex_wakeup_for_backoff(lock, ww_ctx);
       775	
       776			goto skip_wait;
       777		}
       778	
       779		debug_mutex_lock_common(lock, &waiter);
       780		debug_mutex_add_waiter(lock, &waiter, current);
       781	
       782		lock_contended(&lock->dep_map, ip);
       783	
       784		if (!use_ww_ctx) {

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/spinlock.h:317
       297	# include <linux/spinlock_api_up.h>
       298	#endif
       299	
       300	/*
       301	 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
       302	 */
       303	
       304	static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
       305	{
       306		return &lock->rlock;
       307	}
       308	
       309	#define spin_lock_init(_lock)				\
       310	do {							\
       311		spinlock_check(_lock);				\
       312		raw_spin_lock_init(&(_lock)->rlock);		\
       313	} while (0)
       314	
       315	static __always_inline void spin_lock(spinlock_t *lock)
       316	{
==>    317		raw_spin_lock(&lock->rlock);       
       318	}
       319	
       320	static __always_inline void spin_lock_bh(spinlock_t *lock)
       321	{
       322		raw_spin_lock_bh(&lock->rlock);
       323	}
       324	
       325	static __always_inline int spin_trylock(spinlock_t *lock)
       326	{
       327		return raw_spin_trylock(&lock->rlock);
       328	}
       329	
       330	#define spin_lock_nested(lock, subclass)			\
       331	do {								\
       332		raw_spin_lock_nested(spinlock_check(lock), subclass);	\
       333	} while (0)
       334	
       335	#define spin_lock_nest_lock(lock, nest_lock)				\
       336	do {									\
       337		raw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);	\

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/sched/signal.h:335
       315		return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
       316	}
       317	
       318	static inline int __fatal_signal_pending(struct task_struct *p)
       319	{
       320		return unlikely(sigismember(&p->pending.signal, SIGKILL));
       321	}
       322	
       323	static inline int fatal_signal_pending(struct task_struct *p)
       324	{
       325		return signal_pending(p) && __fatal_signal_pending(p);
       326	}
       327	
       328	static inline int signal_pending_state(long state, struct task_struct *p)
       329	{
       330		if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
       331			return 0;
       332		if (!signal_pending(p))
       333			return 0;
       334	
==>    335		return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);       
       336	}
       337	
       338	/*
       339	 * Reevaluate whether the task has signals pending delivery.
       340	 * Wake the task if so.
       341	 * This is required every time the blocked sigset_t changes.
       342	 * callers must hold sighand->siglock.
       343	 */
       344	extern void recalc_sigpending_and_wake(struct task_struct *t);
       345	extern void recalc_sigpending(void);
       346	
       347	extern void signal_wake_up_state(struct task_struct *t, unsigned int state);
       348	
       349	static inline void signal_wake_up(struct task_struct *t, bool resume)
       350	{
       351		signal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);
       352	}
       353	static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)
       354	{
       355		signal_wake_up_state(t, resume ? __TASK_TRACED : 0);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:74
        54	 * NULL means not owned. Since task_struct pointers are aligned at
        55	 * at least L1_CACHE_BYTES, we have low bits to store extra state.
        56	 *
        57	 * Bit0 indicates a non-empty waiter list; unlock must issue a wakeup.
        58	 * Bit1 indicates unlock needs to hand the lock to the top-waiter
        59	 * Bit2 indicates handoff has been done and we're waiting for pickup.
        60	 */
        61	#define MUTEX_FLAG_WAITERS	0x01
        62	#define MUTEX_FLAG_HANDOFF	0x02
        63	#define MUTEX_FLAG_PICKUP	0x04
        64	
        65	#define MUTEX_FLAGS		0x07
        66	
        67	static inline struct task_struct *__owner_task(unsigned long owner)
        68	{
        69		return (struct task_struct *)(owner & ~MUTEX_FLAGS);
        70	}
        71	
        72	static inline unsigned long __owner_flags(unsigned long owner)
        73	{
==>     74		return owner & MUTEX_FLAGS;       
        75	}
        76	
        77	/*
        78	 * Trylock variant that retuns the owning task on failure.
        79	 */
        80	static inline struct task_struct *__mutex_trylock_or_owner(struct mutex *lock)
        81	{
        82		unsigned long owner, curr = (unsigned long)current;
        83	
        84		owner = atomic_long_read(&lock->owner);
        85		for (;;) { /* must loop, can race against a flag */
        86			unsigned long old, flags = __owner_flags(owner);
        87			unsigned long task = owner & ~MUTEX_FLAGS;
        88	
        89			if (task) {
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:862
       842					__mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);
       843			}
       844	
       845			set_current_state(state);
       846			/*
       847			 * Here we order against unlock; we must either see it change
       848			 * state back to RUNNING and fall through the next schedule(),
       849			 * or we must see its unlock and acquire.
       850			 */
       851			if (__mutex_trylock(lock) ||
       852			    (first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))
       853				break;
       854	
       855			spin_lock(&lock->wait_lock);
       856		}
       857		spin_lock(&lock->wait_lock);
       858	acquired:
       859		__set_current_state(TASK_RUNNING);
       860	
       861		mutex_remove_waiter(lock, &waiter, current);
==>    862		if (likely(list_empty(&lock->wait_list)))       
       863			__mutex_clear_flag(lock, MUTEX_FLAGS);
       864	
       865		debug_mutex_free_waiter(&waiter);
       866	
       867	skip_wait:
       868		/* got the lock - cleanup and rejoice! */
       869		lock_acquired(&lock->dep_map, ip);
       870	
       871		if (use_ww_ctx && ww_ctx)
       872			ww_mutex_set_context_slowpath(ww, ww_ctx);
       873	
       874		spin_unlock(&lock->wait_lock);
       875		preempt_enable();
       876		return 0;
       877	
       878	err:
       879		__set_current_state(TASK_RUNNING);
       880		mutex_remove_waiter(lock, &waiter, current);
       881	err_early_backoff:
       882		spin_unlock(&lock->wait_lock);

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:840
       820			 */
       821			if (unlikely(signal_pending_state(state, current))) {
       822				ret = -EINTR;
       823				goto err;
       824			}
       825	
       826			if (use_ww_ctx && ww_ctx && ww_ctx->acquired > 0) {
       827				ret = __ww_mutex_lock_check_stamp(lock, &waiter, ww_ctx);
       828				if (ret)
       829					goto err;
       830			}
       831	
       832			spin_unlock(&lock->wait_lock);
       833			schedule_preempt_disabled();
       834	
       835			/*
       836			 * ww_mutex needs to always recheck its position since its waiter
       837			 * list is not FIFO ordered.
       838			 */
       839			if ((use_ww_ctx && ww_ctx) || !first) {
==>    840				first = __mutex_waiter_is_first(lock, &waiter);       
       841				if (first)
       842					__mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);
       843			}
       844	
       845			set_current_state(state);
       846			/*
       847			 * Here we order against unlock; we must either see it change
       848			 * state back to RUNNING and fall through the next schedule(),
       849			 * or we must see its unlock and acquire.
       850			 */
       851			if (__mutex_trylock(lock) ||
       852			    (first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))
       853				break;
       854	
       855			spin_lock(&lock->wait_lock);
       856		}
       857		spin_lock(&lock->wait_lock);
       858	acquired:
       859		__set_current_state(TASK_RUNNING);
       860	

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/rcupdate.h:629
       609	 * RCU read-side critical sections may be nested.  Any deferred actions
       610	 * will be deferred until the outermost RCU read-side critical section
       611	 * completes.
       612	 *
       613	 * You can avoid reading and understanding the next paragraph by
       614	 * following this rule: don't put anything in an rcu_read_lock() RCU
       615	 * read-side critical section that would block in a !PREEMPT kernel.
       616	 * But if you want the full story, read on!
       617	 *
       618	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       619	 * it is illegal to block while in an RCU read-side critical section.
       620	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       621	 * kernel builds, RCU read-side critical sections may be preempted,
       622	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       623	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       624	 * read-side critical sections may be preempted and they may also block, but
       625	 * only when acquiring spinlocks that are subject to priority inheritance.
       626	 */
       627	static inline void rcu_read_lock(void)
       628	{
==>    629		__rcu_read_lock();       
       630		__acquire(RCU);
       631		rcu_lock_acquire(&rcu_lock_map);
       632		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       633				 "rcu_read_lock() used illegally while idle");
       634	}
       635	
       636	/*
       637	 * So where is rcu_write_lock()?  It does not exist, as there is no
       638	 * way for writers to lock out RCU readers.  This is a feature, not
       639	 * a bug -- this property is what provides RCU's performance benefits.
       640	 * Of course, writers must coordinate with each other.  The normal
       641	 * spinlock primitives work well for this, but any other technique may be
       642	 * used as well.  RCU does not care how the writers keep out of each
       643	 * others' way, as long as they do so.
       644	 */
       645	
       646	/**
       647	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       648	 *
       649	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./include/linux/rcupdate.h:629
       609	 * RCU read-side critical sections may be nested.  Any deferred actions
       610	 * will be deferred until the outermost RCU read-side critical section
       611	 * completes.
       612	 *
       613	 * You can avoid reading and understanding the next paragraph by
       614	 * following this rule: don't put anything in an rcu_read_lock() RCU
       615	 * read-side critical section that would block in a !PREEMPT kernel.
       616	 * But if you want the full story, read on!
       617	 *
       618	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       619	 * it is illegal to block while in an RCU read-side critical section.
       620	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       621	 * kernel builds, RCU read-side critical sections may be preempted,
       622	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       623	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       624	 * read-side critical sections may be preempted and they may also block, but
       625	 * only when acquiring spinlocks that are subject to priority inheritance.
       626	 */
       627	static inline void rcu_read_lock(void)
       628	{
==>    629		__rcu_read_lock();       
       630		__acquire(RCU);
       631		rcu_lock_acquire(&rcu_lock_map);
       632		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       633				 "rcu_read_lock() used illegally while idle");
       634	}
       635	
       636	/*
       637	 * So where is rcu_write_lock()?  It does not exist, as there is no
       638	 * way for writers to lock out RCU readers.  This is a feature, not
       639	 * a bug -- this property is what provides RCU's performance benefits.
       640	 * Of course, writers must coordinate with each other.  The normal
       641	 * spinlock primitives work well for this, but any other technique may be
       642	 * used as well.  RCU does not care how the writers keep out of each
       643	 * others' way, as long as they do so.
       644	 */
       645	
       646	/**
       647	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       648	 *
       649	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/processor.h:635
       615	{
       616		unsigned int eax, ebx, ecx, edx;
       617	
       618		cpuid(op, &eax, &ebx, &ecx, &edx);
       619	
       620		return ecx;
       621	}
       622	
       623	static inline unsigned int cpuid_edx(unsigned int op)
       624	{
       625		unsigned int eax, ebx, ecx, edx;
       626	
       627		cpuid(op, &eax, &ebx, &ecx, &edx);
       628	
       629		return edx;
       630	}
       631	
       632	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       633	static __always_inline void rep_nop(void)
       634	{
==>    635		asm volatile("rep; nop" ::: "memory");       
       636	}
       637	
       638	static __always_inline void cpu_relax(void)
       639	{
       640		rep_nop();
       641	}
       642	
       643	/*
       644	 * This function forces the icache and prefetched instruction stream to
       645	 * catch up with reality in two very specific cases:
       646	 *
       647	 *  a) Text was modified using one virtual address and is about to be executed
       648	 *     from the same physical page at a different virtual address.
       649	 *
       650	 *  b) Text was modified on a different CPU, may subsequently be
       651	 *     executed on this CPU, and you want to make sure the new version
       652	 *     gets executed.  This generally means you're calling this in a IPI.
       653	 *
       654	 * If you're calling this for a different reason, you're probably doing
       655	 * it wrong.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */


====================================================================================================================================================================================
Total: 16386	Addresses: c10d82cf c10dc6f3
8193	0xc10d82cf: update_min_vruntime at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:542
8193	0xc10dc6f3: migrate_task_rq_fair at /root/2017-17712-i386/linux-4.14/kernel/sched/fair.c:6051 (discriminator 1)

8193	c10d82cf update_min_vruntime T: trace_20241114_135838_3_3_36.txt S: 36 I1: 3 I2: 3 IP1: c10dc6f3 IP2: c10d82cf PMA1: 365ad41c PMA2: 365ad41c CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 199308 IC2: 191120
8193	c10dc6f3 migrate_task_rq_fair T: trace_20241114_135838_3_3_36.txt S: 36 I1: 3 I2: 3 IP1: c10dc6f3 IP2: c10d82cf PMA1: 365ad41c PMA2: 365ad41c CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 199308 IC2: 191120

++++++++++++++++++++++++
STATS: Distinct IPs: 83 Distinct pairs: 99 Distinct clusters: 24
++++++++++++++++++++++++
/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:542
       522			if (curr->on_rq)
       523				vruntime = curr->vruntime;
       524			else
       525				curr = NULL;
       526		}
       527	
       528		if (leftmost) { /* non-empty tree */
       529			struct sched_entity *se;
       530			se = rb_entry(leftmost, struct sched_entity, run_node);
       531	
       532			if (!curr)
       533				vruntime = se->vruntime;
       534			else
       535				vruntime = min_vruntime(vruntime, se->vruntime);
       536		}
       537	
       538		/* ensure we never gain time by being placed backwards. */
       539		cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);
       540	#ifndef CONFIG_64BIT
       541		smp_wmb();
==>    542		cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;       
       543	#endif
       544	}
       545	
       546	/*
       547	 * Enqueue an entity into the rb-tree:
       548	 */
       549	static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
       550	{
       551		struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
       552		struct rb_node *parent = NULL;
       553		struct sched_entity *entry;
       554		bool leftmost = true;
       555	
       556		/*
       557		 * Find the right place in the rbtree:
       558		 */
       559		while (*link) {
       560			parent = *link;
       561			entry = rb_entry(parent, struct sched_entity, run_node);
       562			/*

/root/snowboard-2017-17712/testsuite/kernel/2017-17712/source//kernel/sched/fair.c:6051
      6031	 * cfs_rq_of(p) references at time of call are still valid and identify the
      6032	 * previous cpu. The caller guarantees p->pi_lock or task_rq(p)->lock is held.
      6033	 */
      6034	static void migrate_task_rq_fair(struct task_struct *p)
      6035	{
      6036		/*
      6037		 * As blocked tasks retain absolute vruntime the migration needs to
      6038		 * deal with this by subtracting the old and adding the new
      6039		 * min_vruntime -- the latter is done by enqueue_entity() when placing
      6040		 * the task on the new runqueue.
      6041		 */
      6042		if (p->state == TASK_WAKING) {
      6043			struct sched_entity *se = &p->se;
      6044			struct cfs_rq *cfs_rq = cfs_rq_of(se);
      6045			u64 min_vruntime;
      6046	
      6047	#ifndef CONFIG_64BIT
      6048			u64 min_vruntime_copy;
      6049	
      6050			do {
==>   6051				min_vruntime_copy = cfs_rq->min_vruntime_copy;       
      6052				smp_rmb();
      6053				min_vruntime = cfs_rq->min_vruntime;
      6054			} while (min_vruntime != min_vruntime_copy);
      6055	#else
      6056			min_vruntime = cfs_rq->min_vruntime;
      6057	#endif
      6058	
      6059			se->vruntime -= min_vruntime;
      6060		}
      6061	
      6062		/*
      6063		 * We are supposed to update the task to "current" time, then its up to date
      6064		 * and ready to go to new CPU/cfs_rq. But we have difficulty in getting
      6065		 * what current time is, so simply throw away the out-of-date time. This
      6066		 * will result in the wakee task is less decayed, but giving the wakee more
      6067		 * load sounds not bad.
      6068		 */
      6069		remove_entity_load_avg(&p->se);
      6070	
      6071		/* Tell new CPU we are migrated */


