vmlinux map is loaded
Waiting for data race records

('Analyed ', 500, 727, ' data races in total')

====================================================================================================================================================================================
Total: 4	Addresses: c10fd0fa c10fd1da
2	0xc10fd0f8: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667
2	0xc10fd1d8: osq_unlock at /root/2018-12232-i386/linux-4.17.1/kernel/locking/osq_lock.c:223

2	c10fd0fa __read_once_size T: trace_20241115_161027_2_2_43.txt S: 43 I1: 2 I2: 2 IP1: c10fd0fa IP2: c10fd1da PMA1: 361b9608 PMA2: 361b9608 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 549440 IC2: 549438
2	c10fd1da __write_once_size T: trace_20241115_161027_2_2_43.txt S: 43 I1: 2 I2: 2 IP1: c10fd0fa IP2: c10fd1da PMA1: 361b9608 PMA2: 361b9608 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 549440 IC2: 549438

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/locking/osq_lock.c:223
       203		return false;
       204	}
       205	
       206	void osq_unlock(struct optimistic_spin_queue *lock)
       207	{
       208		struct optimistic_spin_node *node, *next;
       209		int curr = encode_cpu(smp_processor_id());
       210	
       211		/*
       212		 * Fast path for the uncontended case.
       213		 */
       214		if (likely(atomic_cmpxchg_release(&lock->tail, curr,
       215						  OSQ_UNLOCKED_VAL) == curr))
       216			return;
       217	
       218		/*
       219		 * Second most likely case.
       220		 */
       221		node = this_cpu_ptr(&osq_node);
       222		next = xchg(&node->next, NULL);
==>    223		if (next) {       
       224			WRITE_ONCE(next->locked, 1);
       225			return;
       226		}
       227	
       228		next = osq_wait_next(lock, node, NULL);
       229		if (next)
       230			WRITE_ONCE(next->locked, 1);
       231	}


====================================================================================================================================================================================
Total: 8	Addresses: c11378fb c1137b5a
4	0xc11378f9: csd_unlock at /root/2018-12232-i386/linux-4.17.1/kernel/smp.c:126
4	0xc1137b58: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667

4	c11378fb __write_once_size T: trace_20241115_161031_4_3_20.txt S: 20 I1: 4 I2: 3 IP1: c11378fb IP2: c1137b5a PMA1: 31b19c7c PMA2: 31b19c7c CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 4289346 IC2: 4283542
4	c1137b5a __read_once_size T: trace_20241115_161031_4_3_20.txt S: 20 I1: 4 I2: 3 IP1: c11378fb IP2: c1137b5a PMA1: 31b19c7c PMA2: 31b19c7c CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 4289346 IC2: 4283542

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/smp.c:126
       106	static __always_inline void csd_lock_wait(call_single_data_t *csd)
       107	{
       108		smp_cond_load_acquire(&csd->flags, !(VAL & CSD_FLAG_LOCK));
       109	}
       110	
       111	static __always_inline void csd_lock(call_single_data_t *csd)
       112	{
       113		csd_lock_wait(csd);
       114		csd->flags |= CSD_FLAG_LOCK;
       115	
       116		/*
       117		 * prevent CPU from reordering the above assignment
       118		 * to ->flags with any subsequent assignments to other
       119		 * fields of the specified call_single_data_t structure:
       120		 */
       121		smp_wmb();
       122	}
       123	
       124	static __always_inline void csd_unlock(call_single_data_t *csd)
       125	{
==>    126		WARN_ON(!(csd->flags & CSD_FLAG_LOCK));       
       127	
       128		/*
       129		 * ensure we're all done before releasing data:
       130		 */
       131		smp_store_release(&csd->flags, 0);
       132	}
       133	
       134	static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);
       135	
       136	/*
       137	 * Insert a previously allocated call_single_data_t element
       138	 * for execution on the given CPU. data must already have
       139	 * ->func, ->info, and ->flags set.
       140	 */
       141	static int generic_exec_single(int cpu, call_single_data_t *csd,
       142				       smp_call_func_t func, void *info)
       143	{
       144		if (cpu == smp_processor_id()) {
       145			unsigned long flags;
       146	

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.


====================================================================================================================================================================================
Total: 10	Addresses: c10f979e c18e7147 c10f91a0 c10f93fd c18e6ff4
1	0xc18e7147: copy_page_to_iter at /root/2018-12232-i386/linux-4.17.1/lib/iov_iter.c:713
1	0xc10f93fd: __wake_up_sync_key at /root/2018-12232-i386/linux-4.17.1/kernel/sched/wait.c:197
1	0xc18e6ff4: copy_page_to_iter at /root/2018-12232-i386/linux-4.17.1/lib/iov_iter.c:701
2	0xc10f918e: finish_wait at /root/2018-12232-i386/linux-4.17.1/kernel/sched/wait.c:368
5	0xc10f9789: autoremove_wake_function at /root/2018-12232-i386/linux-4.17.1/kernel/sched/wait.c:379

1	c18e7147 copy_page_to_iter T: trace_20241115_160928_3_3_17.txt S: 17 I1: 3 I2: 3 IP1: c18e7147 IP2: c10f979e PMA1: 31d8de98 PMA2: 31d8de98 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 3418096 IC2: 3413338
1	c10f93fd __wake_up_sync_key T: trace_20241115_160928_3_3_17.txt S: 17 I1: 3 I2: 3 IP1: c10f93fd IP2: c10f979e PMA1: 31d8de98 PMA2: 31d8de98 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 3418196 IC2: 3413338
1	c18e6ff4 copy_page_to_iter T: trace_20241115_160928_3_3_17.txt S: 17 I1: 3 I2: 3 IP1: c18e6ff4 IP2: c10f979e PMA1: 31d8de98 PMA2: 31d8de98 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 3417486 IC2: 3413338
2	c10f91a0 list_empty_careful T: trace_20241115_160955_2_4_39.txt S: 39 I1: 2 I2: 4 IP1: c10f91a0 IP2: c10f979e PMA1: 32efbe98 PMA2: 32efbe98 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1560803 IC2: 1556546
5	c10f979e INIT_LIST_HEAD T: trace_20241115_160955_2_4_39.txt S: 39 I1: 2 I2: 4 IP1: c10f91a0 IP2: c10f979e PMA1: 32efbe98 PMA2: 32efbe98 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1560803 IC2: 1556546

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//lib/iov_iter.c:713
       693		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       694			return true;
       695		WARN_ON(1);
       696		return false;
       697	}
       698	
       699	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       700				 struct iov_iter *i)
       701	{
       702		if (unlikely(!page_copy_sane(page, offset, bytes)))
       703			return 0;
       704		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       705			void *kaddr = kmap_atomic(page);
       706			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       707			kunmap_atomic(kaddr);
       708			return wanted;
       709		} else if (likely(!(i->type & ITER_PIPE)))
       710			return copy_page_to_iter_iovec(page, offset, bytes, i);
       711		else
       712			return copy_page_to_iter_pipe(page, offset, bytes, i);
==>    713	}       
       714	EXPORT_SYMBOL(copy_page_to_iter);
       715	
       716	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       717				 struct iov_iter *i)
       718	{
       719		if (unlikely(!page_copy_sane(page, offset, bytes)))
       720			return 0;
       721		if (unlikely(i->type & ITER_PIPE)) {
       722			WARN_ON(1);
       723			return 0;
       724		}
       725		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       726			void *kaddr = kmap_atomic(page);
       727			size_t wanted = _copy_from_iter(kaddr + offset, bytes, i);
       728			kunmap_atomic(kaddr);
       729			return wanted;
       730		} else
       731			return copy_page_from_iter_iovec(page, offset, bytes, i);
       732	}
       733	EXPORT_SYMBOL(copy_page_from_iter);

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/wait.c:197
       177	 * away soon, so while the target thread will be woken up, it will not
       178	 * be migrated to another CPU - ie. the two threads are 'synchronized'
       179	 * with each other. This can prevent needless bouncing between CPUs.
       180	 *
       181	 * On UP it can prevent extra preemption.
       182	 *
       183	 * It may be assumed that this function implies a write memory barrier before
       184	 * changing the task state if and only if any tasks are woken up.
       185	 */
       186	void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
       187				int nr_exclusive, void *key)
       188	{
       189		int wake_flags = 1; /* XXX WF_SYNC */
       190	
       191		if (unlikely(!wq_head))
       192			return;
       193	
       194		if (unlikely(nr_exclusive != 1))
       195			wake_flags = 0;
       196	
==>    197		__wake_up_common_lock(wq_head, mode, nr_exclusive, wake_flags, key);       
       198	}
       199	EXPORT_SYMBOL_GPL(__wake_up_sync_key);
       200	
       201	/*
       202	 * __wake_up_sync - see __wake_up_sync_key()
       203	 */
       204	void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode, int nr_exclusive)
       205	{
       206		__wake_up_sync_key(wq_head, mode, nr_exclusive, NULL);
       207	}
       208	EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
       209	
       210	/*
       211	 * Note: we use "set_current_state()" _after_ the wait-queue add,
       212	 * because we need a memory barrier there on SMP, so that any
       213	 * wake-function that tests for the wait-queue being active
       214	 * will be guaranteed to see waitqueue addition _or_ subsequent
       215	 * tests in this thread will see the wakeup having taken place.
       216	 *
       217	 * The spin_unlock() itself is semi-permeable and only protects

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//lib/iov_iter.c:701
       681		)
       682	
       683		iov_iter_advance(i, bytes);
       684		return true;
       685	}
       686	EXPORT_SYMBOL(_copy_from_iter_full_nocache);
       687	
       688	static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
       689	{
       690		struct page *head = compound_head(page);
       691		size_t v = n + offset + page_address(page) - page_address(head);
       692	
       693		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       694			return true;
       695		WARN_ON(1);
       696		return false;
       697	}
       698	
       699	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       700				 struct iov_iter *i)
==>    701	{       
       702		if (unlikely(!page_copy_sane(page, offset, bytes)))
       703			return 0;
       704		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       705			void *kaddr = kmap_atomic(page);
       706			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       707			kunmap_atomic(kaddr);
       708			return wanted;
       709		} else if (likely(!(i->type & ITER_PIPE)))
       710			return copy_page_to_iter_iovec(page, offset, bytes, i);
       711		else
       712			return copy_page_to_iter_pipe(page, offset, bytes, i);
       713	}
       714	EXPORT_SYMBOL(copy_page_to_iter);
       715	
       716	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       717				 struct iov_iter *i)
       718	{
       719		if (unlikely(!page_copy_sane(page, offset, bytes)))
       720			return 0;
       721		if (unlikely(i->type & ITER_PIPE)) {

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/wait.c:368
       348	
       349		__set_current_state(TASK_RUNNING);
       350		/*
       351		 * We can check for list emptiness outside the lock
       352		 * IFF:
       353		 *  - we use the "careful" check that verifies both
       354		 *    the next and prev pointers, so that there cannot
       355		 *    be any half-pending updates in progress on other
       356		 *    CPU's that we haven't seen yet (and that might
       357		 *    still change the stack area.
       358		 * and
       359		 *  - all other users take the lock (ie we can only
       360		 *    have _one_ other CPU that looks at or modifies
       361		 *    the list).
       362		 */
       363		if (!list_empty_careful(&wq_entry->entry)) {
       364			spin_lock_irqsave(&wq_head->lock, flags);
       365			list_del_init(&wq_entry->entry);
       366			spin_unlock_irqrestore(&wq_head->lock, flags);
       367		}
==>    368	}       
       369	EXPORT_SYMBOL(finish_wait);
       370	
       371	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       372	{
       373		int ret = default_wake_function(wq_entry, mode, sync, key);
       374	
       375		if (ret)
       376			list_del_init(&wq_entry->entry);
       377	
       378		return ret;
       379	}
       380	EXPORT_SYMBOL(autoremove_wake_function);
       381	
       382	static inline bool is_kthread_should_stop(void)
       383	{
       384		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       385	}
       386	
       387	/*
       388	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/wait.c:379
       359		 *  - all other users take the lock (ie we can only
       360		 *    have _one_ other CPU that looks at or modifies
       361		 *    the list).
       362		 */
       363		if (!list_empty_careful(&wq_entry->entry)) {
       364			spin_lock_irqsave(&wq_head->lock, flags);
       365			list_del_init(&wq_entry->entry);
       366			spin_unlock_irqrestore(&wq_head->lock, flags);
       367		}
       368	}
       369	EXPORT_SYMBOL(finish_wait);
       370	
       371	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       372	{
       373		int ret = default_wake_function(wq_entry, mode, sync, key);
       374	
       375		if (ret)
       376			list_del_init(&wq_entry->entry);
       377	
       378		return ret;
==>    379	}       
       380	EXPORT_SYMBOL(autoremove_wake_function);
       381	
       382	static inline bool is_kthread_should_stop(void)
       383	{
       384		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       385	}
       386	
       387	/*
       388	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       389	 *
       390	 * add_wait_queue(&wq_head, &wait);
       391	 * for (;;) {
       392	 *     if (condition)
       393	 *         break;
       394	 *
       395	 *     p->state = mode;				condition = true;
       396	 *     smp_mb(); // A				smp_wmb(); // C
       397	 *     if (!wq_entry->flags & WQ_FLAG_WOKEN)	wq_entry->flags |= WQ_FLAG_WOKEN;
       398	 *         schedule()				try_to_wake_up();
       399	 *     p->state = TASK_RUNNING;		    ~~~~~~~~~~~~~~~~~~


====================================================================================================================================================================================
Total: 12	Addresses: c113791d c1137b22
6	0xc113791b: csd_unlock at /root/2018-12232-i386/linux-4.17.1/kernel/smp.c:126
6	0xc1137b20: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667

6	c113791d __write_once_size T: trace_20241115_161039_2_4_3.txt S: 3 I1: 2 I2: 4 IP1: c113791d IP2: c1137b22 PMA1: 361b990c PMA2: 361b990c CPU1: 2 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 4465473 IC2: 4465158
6	c1137b22 __read_once_size T: trace_20241115_161039_2_4_3.txt S: 3 I1: 2 I2: 4 IP1: c113791d IP2: c1137b22 PMA1: 361b990c PMA2: 361b990c CPU1: 2 CPU2: 1 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 4465473 IC2: 4465158

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/smp.c:126
       106	static __always_inline void csd_lock_wait(call_single_data_t *csd)
       107	{
       108		smp_cond_load_acquire(&csd->flags, !(VAL & CSD_FLAG_LOCK));
       109	}
       110	
       111	static __always_inline void csd_lock(call_single_data_t *csd)
       112	{
       113		csd_lock_wait(csd);
       114		csd->flags |= CSD_FLAG_LOCK;
       115	
       116		/*
       117		 * prevent CPU from reordering the above assignment
       118		 * to ->flags with any subsequent assignments to other
       119		 * fields of the specified call_single_data_t structure:
       120		 */
       121		smp_wmb();
       122	}
       123	
       124	static __always_inline void csd_unlock(call_single_data_t *csd)
       125	{
==>    126		WARN_ON(!(csd->flags & CSD_FLAG_LOCK));       
       127	
       128		/*
       129		 * ensure we're all done before releasing data:
       130		 */
       131		smp_store_release(&csd->flags, 0);
       132	}
       133	
       134	static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);
       135	
       136	/*
       137	 * Insert a previously allocated call_single_data_t element
       138	 * for execution on the given CPU. data must already have
       139	 * ->func, ->info, and ->flags set.
       140	 */
       141	static int generic_exec_single(int cpu, call_single_data_t *csd,
       142				       smp_call_func_t func, void *info)
       143	{
       144		if (cpu == smp_processor_id()) {
       145			unsigned long flags;
       146	

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.


====================================================================================================================================================================================
Total: 12	Addresses: c11db56b c1134cea
6	0xc11db56b: set_bit at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/bitops.h:76
6	0xc1134cea: constant_test_bit at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/bitops.h:328

6	c11db56b set_bit T: trace_20241115_160750_4_4_49.txt S: 49 I1: 4 I2: 4 IP1: c11db56b IP2: c1134cea PMA1: 3633026c PMA2: 3633026c CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 134006 IC2: 111229
6	c1134cea constant_test_bit T: trace_20241115_160750_4_4_49.txt S: 49 I1: 4 I2: 4 IP1: c11db56b IP2: c1134cea PMA1: 3633026c PMA2: 3633026c CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 134006 IC2: 111229

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/bitops.h:76
        56	
        57	/**
        58	 * set_bit - Atomically set a bit in memory
        59	 * @nr: the bit to set
        60	 * @addr: the address to start counting from
        61	 *
        62	 * This function is atomic and may not be reordered.  See __set_bit()
        63	 * if you do not require the atomic guarantees.
        64	 *
        65	 * Note: there are no guarantees that this function will not be reordered
        66	 * on non x86 architectures, so if you are writing portable code,
        67	 * make sure not to rely on its reordering guarantees.
        68	 *
        69	 * Note that @nr may be almost arbitrarily large; this function is not
        70	 * restricted to acting on a single-word quantity.
        71	 */
        72	static __always_inline void
        73	set_bit(long nr, volatile unsigned long *addr)
        74	{
        75		if (IS_IMMEDIATE(nr)) {
==>     76			asm volatile(LOCK_PREFIX "orb %1,%0"       
        77				: CONST_MASK_ADDR(nr, addr)
        78				: "iq" ((u8)CONST_MASK(nr))
        79				: "memory");
        80		} else {
        81			asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"
        82				: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
        83		}
        84	}
        85	
        86	/**
        87	 * __set_bit - Set a bit in memory
        88	 * @nr: the bit to set
        89	 * @addr: the address to start counting from
        90	 *
        91	 * Unlike set_bit(), this function is non-atomic and may be reordered.
        92	 * If it's called on the same region of memory simultaneously, the effect
        93	 * may be that only one operation succeeds.
        94	 */
        95	static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
        96	{

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/bitops.h:328
       308		return oldbit;
       309	}
       310	
       311	/**
       312	 * test_and_change_bit - Change a bit and return its old value
       313	 * @nr: Bit to change
       314	 * @addr: Address to count from
       315	 *
       316	 * This operation is atomic and cannot be reordered.
       317	 * It also implies a memory barrier.
       318	 */
       319	static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
       320	{
       321		GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc),
       322		                 *addr, "Ir", nr, "%0", c);
       323	}
       324	
       325	static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
       326	{
       327		return ((1UL << (nr & (BITS_PER_LONG-1))) &
==>    328			(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;       
       329	}
       330	
       331	static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
       332	{
       333		bool oldbit;
       334	
       335		asm volatile(__ASM_SIZE(bt) " %2,%1"
       336			     CC_SET(c)
       337			     : CC_OUT(c) (oldbit)
       338			     : "m" (*(unsigned long *)addr), "Ir" (nr));
       339	
       340		return oldbit;
       341	}
       342	
       343	#if 0 /* Fool kernel-doc since it doesn't do macros yet */
       344	/**
       345	 * test_bit - Determine whether a bit is set
       346	 * @nr: bit number to test
       347	 * @addr: Address to start counting from
       348	 */


====================================================================================================================================================================================
Total: 12	Addresses: c10f979b c10f915a
6	0xc10f9789: autoremove_wake_function at /root/2018-12232-i386/linux-4.17.1/kernel/sched/wait.c:379
6	0xc10f9153: finish_wait at /root/2018-12232-i386/linux-4.17.1/kernel/sched/wait.c:349

6	c10f979b __write_once_size T: trace_20241115_160955_2_4_34.txt S: 34 I1: 2 I2: 4 IP1: c10f915a IP2: c10f979b PMA1: 31d8de94 PMA2: 31d8de94 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 3417572 IC2: 3413492
6	c10f915a list_empty_careful T: trace_20241115_160955_2_4_34.txt S: 34 I1: 2 I2: 4 IP1: c10f915a IP2: c10f979b PMA1: 31d8de94 PMA2: 31d8de94 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 3417572 IC2: 3413492

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/wait.c:379
       359		 *  - all other users take the lock (ie we can only
       360		 *    have _one_ other CPU that looks at or modifies
       361		 *    the list).
       362		 */
       363		if (!list_empty_careful(&wq_entry->entry)) {
       364			spin_lock_irqsave(&wq_head->lock, flags);
       365			list_del_init(&wq_entry->entry);
       366			spin_unlock_irqrestore(&wq_head->lock, flags);
       367		}
       368	}
       369	EXPORT_SYMBOL(finish_wait);
       370	
       371	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       372	{
       373		int ret = default_wake_function(wq_entry, mode, sync, key);
       374	
       375		if (ret)
       376			list_del_init(&wq_entry->entry);
       377	
       378		return ret;
==>    379	}       
       380	EXPORT_SYMBOL(autoremove_wake_function);
       381	
       382	static inline bool is_kthread_should_stop(void)
       383	{
       384		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       385	}
       386	
       387	/*
       388	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       389	 *
       390	 * add_wait_queue(&wq_head, &wait);
       391	 * for (;;) {
       392	 *     if (condition)
       393	 *         break;
       394	 *
       395	 *     p->state = mode;				condition = true;
       396	 *     smp_mb(); // A				smp_wmb(); // C
       397	 *     if (!wq_entry->flags & WQ_FLAG_WOKEN)	wq_entry->flags |= WQ_FLAG_WOKEN;
       398	 *         schedule()				try_to_wake_up();
       399	 *     p->state = TASK_RUNNING;		    ~~~~~~~~~~~~~~~~~~

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/wait.c:349
       329		schedule();
       330		spin_lock_irq(&wq->lock);
       331	
       332		return 0;
       333	}
       334	EXPORT_SYMBOL(do_wait_intr_irq);
       335	
       336	/**
       337	 * finish_wait - clean up after waiting in a queue
       338	 * @wq_head: waitqueue waited on
       339	 * @wq_entry: wait descriptor
       340	 *
       341	 * Sets current thread back to running state and removes
       342	 * the wait descriptor from the given waitqueue if still
       343	 * queued.
       344	 */
       345	void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
       346	{
       347		unsigned long flags;
       348	
==>    349		__set_current_state(TASK_RUNNING);       
       350		/*
       351		 * We can check for list emptiness outside the lock
       352		 * IFF:
       353		 *  - we use the "careful" check that verifies both
       354		 *    the next and prev pointers, so that there cannot
       355		 *    be any half-pending updates in progress on other
       356		 *    CPU's that we haven't seen yet (and that might
       357		 *    still change the stack area.
       358		 * and
       359		 *  - all other users take the lock (ie we can only
       360		 *    have _one_ other CPU that looks at or modifies
       361		 *    the list).
       362		 */
       363		if (!list_empty_careful(&wq_entry->entry)) {
       364			spin_lock_irqsave(&wq_head->lock, flags);
       365			list_del_init(&wq_entry->entry);
       366			spin_unlock_irqrestore(&wq_head->lock, flags);
       367		}
       368	}
       369	EXPORT_SYMBOL(finish_wait);


====================================================================================================================================================================================
Total: 20	Addresses: c276df31 c1134cc9 c1134cb6
5	0xc276df1f: cpumask_local_spread at ??:?
5	0xc1134cb4: get_futex_key at /root/2018-12232-i386/linux-4.17.1/kernel/futex.c:661 (discriminator 1)
10	0xc1134cb4: get_futex_key at /root/2018-12232-i386/linux-4.17.1/kernel/futex.c:661 (discriminator 1)

5	c276df31 arch_atomic_try_cmpxchg T: trace_20241115_160750_4_4_45.txt S: 45 I1: 4 I2: 4 IP1: c276df31 IP2: c1134cb6 PMA1: 31abae28 PMA2: 31abae28 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 134220 IC2: 111168
5	c1134cc9 arch_atomic_try_cmpxchg T: trace_20241115_160750_4_4_45.txt S: 45 I1: 4 I2: 4 IP1: c1134cc9 IP2: c1134cb6 PMA1: 31abae28 PMA2: 31abae28 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 134078 IC2: 111168
10	c1134cb6 __read_once_size T: trace_20241115_160750_4_4_45.txt S: 45 I1: 4 I2: 4 IP1: c276df31 IP2: c1134cb6 PMA1: 31abae28 PMA2: 31abae28 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 134220 IC2: 111168

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/futex.c:661
       641			 * The associated futex object in this case is the inode and
       642			 * the page->mapping must be traversed. Ordinarily this should
       643			 * be stabilised under page lock but it's not strictly
       644			 * necessary in this case as we just want to pin the inode, not
       645			 * update the radix tree or anything like that.
       646			 *
       647			 * The RCU read lock is taken as the inode is finally freed
       648			 * under RCU. If the mapping still matches expectations then the
       649			 * mapping->host can be safely accessed as being a valid inode.
       650			 */
       651			rcu_read_lock();
       652	
       653			if (READ_ONCE(page->mapping) != mapping) {
       654				rcu_read_unlock();
       655				put_page(page);
       656	
       657				goto again;
       658			}
       659	
       660			inode = READ_ONCE(mapping->host);
==>    661			if (!inode) {       
       662				rcu_read_unlock();
       663				put_page(page);
       664	
       665				goto again;
       666			}
       667	
       668			/*
       669			 * Take a reference unless it is about to be freed. Previously
       670			 * this reference was taken by ihold under the page lock
       671			 * pinning the inode in place so i_lock was unnecessary. The
       672			 * only way for this check to fail is if the inode was
       673			 * truncated in parallel which is almost certainly an
       674			 * application bug. In such a case, just retry.
       675			 *
       676			 * We are not calling into get_futex_key_refs() in file-backed
       677			 * cases, therefore a successful atomic_inc return below will
       678			 * guarantee that get_futex_key() will still imply smp_mb(); (B).
       679			 */
       680			if (!atomic_inc_not_zero(&inode->i_count)) {
       681				rcu_read_unlock();

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/futex.c:661
       641			 * The associated futex object in this case is the inode and
       642			 * the page->mapping must be traversed. Ordinarily this should
       643			 * be stabilised under page lock but it's not strictly
       644			 * necessary in this case as we just want to pin the inode, not
       645			 * update the radix tree or anything like that.
       646			 *
       647			 * The RCU read lock is taken as the inode is finally freed
       648			 * under RCU. If the mapping still matches expectations then the
       649			 * mapping->host can be safely accessed as being a valid inode.
       650			 */
       651			rcu_read_lock();
       652	
       653			if (READ_ONCE(page->mapping) != mapping) {
       654				rcu_read_unlock();
       655				put_page(page);
       656	
       657				goto again;
       658			}
       659	
       660			inode = READ_ONCE(mapping->host);
==>    661			if (!inode) {       
       662				rcu_read_unlock();
       663				put_page(page);
       664	
       665				goto again;
       666			}
       667	
       668			/*
       669			 * Take a reference unless it is about to be freed. Previously
       670			 * this reference was taken by ihold under the page lock
       671			 * pinning the inode in place so i_lock was unnecessary. The
       672			 * only way for this check to fail is if the inode was
       673			 * truncated in parallel which is almost certainly an
       674			 * application bug. In such a case, just retry.
       675			 *
       676			 * We are not calling into get_futex_key_refs() in file-backed
       677			 * cases, therefore a successful atomic_inc return below will
       678			 * guarantee that get_futex_key() will still imply smp_mb(); (B).
       679			 */
       680			if (!atomic_inc_not_zero(&inode->i_count)) {
       681				rcu_read_unlock();


====================================================================================================================================================================================
Total: 48	Addresses: c10fecb2 c278e9d3
24	0xc10fecb0: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667
24	0xc278e9d1: _raw_write_unlock_irq at /root/2018-12232-i386/linux-4.17.1/kernel/locking/spinlock.c:335

24	c10fecb2 __read_once_size T: trace_20241115_161057_2_2_34.txt S: 34 I1: 2 I2: 2 IP1: c278e9d3 IP2: c10fecb2 PMA1: 30170c0 PMA2: 30170c0 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 2310085 IC2: 2309983
24	c278e9d3 __write_once_size T: trace_20241115_161057_2_2_34.txt S: 34 I1: 2 I2: 2 IP1: c278e9d3 IP2: c10fecb2 PMA1: 30170c0 PMA2: 30170c0 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 2310085 IC2: 2309983

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/locking/spinlock.c:335
       315	#endif
       316	
       317	#ifndef CONFIG_INLINE_WRITE_UNLOCK
       318	void __lockfunc _raw_write_unlock(rwlock_t *lock)
       319	{
       320		__raw_write_unlock(lock);
       321	}
       322	EXPORT_SYMBOL(_raw_write_unlock);
       323	#endif
       324	
       325	#ifndef CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE
       326	void __lockfunc _raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
       327	{
       328		__raw_write_unlock_irqrestore(lock, flags);
       329	}
       330	EXPORT_SYMBOL(_raw_write_unlock_irqrestore);
       331	#endif
       332	
       333	#ifndef CONFIG_INLINE_WRITE_UNLOCK_IRQ
       334	void __lockfunc _raw_write_unlock_irq(rwlock_t *lock)
==>    335	{       
       336		__raw_write_unlock_irq(lock);
       337	}
       338	EXPORT_SYMBOL(_raw_write_unlock_irq);
       339	#endif
       340	
       341	#ifndef CONFIG_INLINE_WRITE_UNLOCK_BH
       342	void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)
       343	{
       344		__raw_write_unlock_bh(lock);
       345	}
       346	EXPORT_SYMBOL(_raw_write_unlock_bh);
       347	#endif
       348	
       349	#ifdef CONFIG_DEBUG_LOCK_ALLOC
       350	
       351	void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
       352	{
       353		preempt_disable();
       354		spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
       355		LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);


====================================================================================================================================================================================
Total: 60	Addresses: c10b32d9 c10b32d5
14	0xc10b32d9: set_bit at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/bitops.h:81
46	0xc10b32d5: clear_bit at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/bitops.h:118

14	c10b32d9 set_bit T: trace_20241115_160820_2_4_28.txt S: 28 I1: 2 I2: 4 IP1: c10b32d9 IP2: c10b32d5 PMA1: 305d794 PMA2: 305d794 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 2284470 IC2: 2086603
46	c10b32d5 clear_bit T: trace_20241115_160820_2_4_31.txt S: 31 I1: 2 I2: 4 IP1: c10b32d5 IP2: c10b32d5 PMA1: 305d794 PMA2: 305d794 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 2419092 IC2: 2416679

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/bitops.h:81
        61	 *
        62	 * This function is atomic and may not be reordered.  See __set_bit()
        63	 * if you do not require the atomic guarantees.
        64	 *
        65	 * Note: there are no guarantees that this function will not be reordered
        66	 * on non x86 architectures, so if you are writing portable code,
        67	 * make sure not to rely on its reordering guarantees.
        68	 *
        69	 * Note that @nr may be almost arbitrarily large; this function is not
        70	 * restricted to acting on a single-word quantity.
        71	 */
        72	static __always_inline void
        73	set_bit(long nr, volatile unsigned long *addr)
        74	{
        75		if (IS_IMMEDIATE(nr)) {
        76			asm volatile(LOCK_PREFIX "orb %1,%0"
        77				: CONST_MASK_ADDR(nr, addr)
        78				: "iq" ((u8)CONST_MASK(nr))
        79				: "memory");
        80		} else {
==>     81			asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"       
        82				: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
        83		}
        84	}
        85	
        86	/**
        87	 * __set_bit - Set a bit in memory
        88	 * @nr: the bit to set
        89	 * @addr: the address to start counting from
        90	 *
        91	 * Unlike set_bit(), this function is non-atomic and may be reordered.
        92	 * If it's called on the same region of memory simultaneously, the effect
        93	 * may be that only one operation succeeds.
        94	 */
        95	static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
        96	{
        97		asm volatile(__ASM_SIZE(bts) " %1,%0" : ADDR : "Ir" (nr) : "memory");
        98	}
        99	
       100	/**
       101	 * clear_bit - Clears a bit in memory

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/bitops.h:118
        98	}
        99	
       100	/**
       101	 * clear_bit - Clears a bit in memory
       102	 * @nr: Bit to clear
       103	 * @addr: Address to start counting from
       104	 *
       105	 * clear_bit() is atomic and may not be reordered.  However, it does
       106	 * not contain a memory barrier, so if it is used for locking purposes,
       107	 * you should call smp_mb__before_atomic() and/or smp_mb__after_atomic()
       108	 * in order to ensure changes are visible on other processors.
       109	 */
       110	static __always_inline void
       111	clear_bit(long nr, volatile unsigned long *addr)
       112	{
       113		if (IS_IMMEDIATE(nr)) {
       114			asm volatile(LOCK_PREFIX "andb %1,%0"
       115				: CONST_MASK_ADDR(nr, addr)
       116				: "iq" ((u8)~CONST_MASK(nr)));
       117		} else {
==>    118			asm volatile(LOCK_PREFIX __ASM_SIZE(btr) " %1,%0"       
       119				: BITOP_ADDR(addr)
       120				: "Ir" (nr));
       121		}
       122	}
       123	
       124	/*
       125	 * clear_bit_unlock - Clears a bit in memory
       126	 * @nr: Bit to clear
       127	 * @addr: Address to start counting from
       128	 *
       129	 * clear_bit() is atomic and implies release semantics before the memory
       130	 * operation. It can be used for an unlock.
       131	 */
       132	static __always_inline void clear_bit_unlock(long nr, volatile unsigned long *addr)
       133	{
       134		barrier();
       135		clear_bit(nr, addr);
       136	}
       137	
       138	static __always_inline void __clear_bit(long nr, volatile unsigned long *addr)


====================================================================================================================================================================================
Total: 148	Addresses: c10e0c02 c10dda69
74	0xc10e0c00: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667
74	0xc10dda61: arch_static_branch at /root/2018-12232-i386/linux-4.17.1/kernel/sched/core.c:2717

74	c10e0c02 __read_once_size T: trace_20241115_161038_4_2_59.txt S: 59 I1: 4 I2: 2 IP1: c10dda69 IP2: c10e0c02 PMA1: 32430a60 PMA2: 32430a60 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 807029 IC2: 805877
74	c10dda69 __write_once_size T: trace_20241115_161038_4_2_59.txt S: 59 I1: 4 I2: 2 IP1: c10dda69 IP2: c10e0c02 PMA1: 32430a60 PMA2: 32430a60 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 807029 IC2: 805877

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/sched/core.c:2717
      2697		 * Also, see FORK_PREEMPT_COUNT.
      2698		 */
      2699		if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
      2700			      "corrupted preempt_count: %s/%d/0x%x\n",
      2701			      current->comm, current->pid, preempt_count()))
      2702			preempt_count_set(FORK_PREEMPT_COUNT);
      2703	
      2704		rq->prev_mm = NULL;
      2705	
      2706		/*
      2707		 * A task struct has one reference for the use as "current".
      2708		 * If a task dies, then it sets TASK_DEAD in tsk->state and calls
      2709		 * schedule one last time. The schedule call will never return, and
      2710		 * the scheduled task must drop that reference.
      2711		 *
      2712		 * We must observe prev->state before clearing prev->on_cpu (in
      2713		 * finish_task), otherwise a concurrent wakeup can get prev
      2714		 * running on another CPU and we could rave with its RUNNING -> DEAD
      2715		 * transition, resulting in a double drop.
      2716		 */
==>   2717		prev_state = prev->state;       
      2718		vtime_task_switch(prev);
      2719		perf_event_task_sched_in(prev, current);
      2720		finish_task(prev);
      2721		finish_lock_switch(rq);
      2722		finish_arch_post_lock_switch();
      2723	
      2724		fire_sched_in_preempt_notifiers(current);
      2725		/*
      2726		 * When switching through a kernel thread, the loop in
      2727		 * membarrier_{private,global}_expedited() may have observed that
      2728		 * kernel thread and not issued an IPI. It is therefore possible to
      2729		 * schedule between user->kernel->user threads without passing though
      2730		 * switch_mm(). Membarrier requires a barrier after storing to
      2731		 * rq->curr, before returning to userspace, so provide them here:
      2732		 *
      2733		 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
      2734		 *   provided by mmdrop(),
      2735		 * - a sync_core for SYNC_CORE.
      2736		 */
      2737		if (mm) {


====================================================================================================================================================================================
Total: 234	Addresses: c278b3be c278b1bf c10fcaba c10fcb0d c278b1a6 c278bce1 c278af12 c278aee2
2	0xc10fcab5: rcu_read_lock at /root/2018-12232-i386/linux-4.17.1/./include/linux/rcupdate.h:630
3	0xc278aedb: __preempt_count_add at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/preempt.h:76
4	0xc278af0d: rcu_read_lock at /root/2018-12232-i386/linux-4.17.1/./include/linux/rcupdate.h:630
12	0xc10fcb0b: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667
16	0xc278b1a4: rep_nop at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/processor.h:667
27	0xc278b3b7: get_current at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/current.h:15
36	0xc278b1bd: __mutex_trylock_or_owner at /root/2018-12232-i386/linux-4.17.1/kernel/locking/mutex.c:110
134	0xc278bcd8: get_current at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/current.h:15

2	c10fcaba __read_once_size T: trace_20241115_161028_2_2_53.txt S: 53 I1: 2 I2: 2 IP1: c10fcaba IP2: c278bce1 PMA1: 31abb200 PMA2: 31abb200 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 656456 IC2: 656427
3	c278aee2 __read_once_size T: trace_20241115_161028_2_2_53.txt S: 53 I1: 2 I2: 2 IP1: c278aee2 IP2: c278b3be PMA1: 31abb200 PMA2: 31abb200 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 653393 IC2: 652720
4	c278af12 __read_once_size T: trace_20241115_161028_2_2_53.txt S: 53 I1: 2 I2: 2 IP1: c278af12 IP2: c278b3be PMA1: 31abb200 PMA2: 31abb200 CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 653412 IC2: 652720
12	c10fcb0d __read_once_size T: trace_20241115_161041_2_4_20.txt S: 20 I1: 2 I2: 4 IP1: c10fcb0d IP2: c278bce1 PMA1: 319d31e0 PMA2: 319d31e0 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1561076 IC2: 1561073
16	c278b1a6 __read_once_size T: trace_20241115_161041_2_4_20.txt S: 20 I1: 2 I2: 4 IP1: c278b1a6 IP2: c278bce1 PMA1: 319d31e0 PMA2: 319d31e0 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1561109 IC2: 1561073
27	c278b3be arch_atomic_cmpxchg T: trace_20241115_161041_2_4_20.txt S: 20 I1: 2 I2: 4 IP1: c278b3be IP2: c278bce1 PMA1: 319d31e0 PMA2: 319d31e0 CPU1: 3 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1563839 IC2: 1561073
36	c278b1bf arch_atomic_cmpxchg T: trace_20241115_161041_2_4_20.txt S: 20 I1: 2 I2: 4 IP1: c278b1bf IP2: c278bce1 PMA1: 319d31e0 PMA2: 319d31e0 CPU1: 3 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1561118 IC2: 1561073
134	c278bce1 arch_atomic_cmpxchg T: trace_20241115_161041_2_4_20.txt S: 20 I1: 2 I2: 4 IP1: c278bce1 IP2: c278bce1 PMA1: 319d31e0 PMA2: 319d31e0 CPU1: 3 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1563867 IC2: 1561073

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./include/linux/rcupdate.h:630
       610	 * RCU read-side critical sections may be nested.  Any deferred actions
       611	 * will be deferred until the outermost RCU read-side critical section
       612	 * completes.
       613	 *
       614	 * You can avoid reading and understanding the next paragraph by
       615	 * following this rule: don't put anything in an rcu_read_lock() RCU
       616	 * read-side critical section that would block in a !PREEMPT kernel.
       617	 * But if you want the full story, read on!
       618	 *
       619	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       620	 * it is illegal to block while in an RCU read-side critical section.
       621	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       622	 * kernel builds, RCU read-side critical sections may be preempted,
       623	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       624	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       625	 * read-side critical sections may be preempted and they may also block, but
       626	 * only when acquiring spinlocks that are subject to priority inheritance.
       627	 */
       628	static inline void rcu_read_lock(void)
       629	{
==>    630		__rcu_read_lock();       
       631		__acquire(RCU);
       632		rcu_lock_acquire(&rcu_lock_map);
       633		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       634				 "rcu_read_lock() used illegally while idle");
       635	}
       636	
       637	/*
       638	 * So where is rcu_write_lock()?  It does not exist, as there is no
       639	 * way for writers to lock out RCU readers.  This is a feature, not
       640	 * a bug -- this property is what provides RCU's performance benefits.
       641	 * Of course, writers must coordinate with each other.  The normal
       642	 * spinlock primitives work well for this, but any other technique may be
       643	 * used as well.  RCU does not care how the writers keep out of each
       644	 * others' way, as long as they do so.
       645	 */
       646	
       647	/**
       648	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       649	 *
       650	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./include/linux/rcupdate.h:630
       610	 * RCU read-side critical sections may be nested.  Any deferred actions
       611	 * will be deferred until the outermost RCU read-side critical section
       612	 * completes.
       613	 *
       614	 * You can avoid reading and understanding the next paragraph by
       615	 * following this rule: don't put anything in an rcu_read_lock() RCU
       616	 * read-side critical section that would block in a !PREEMPT kernel.
       617	 * But if you want the full story, read on!
       618	 *
       619	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       620	 * it is illegal to block while in an RCU read-side critical section.
       621	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       622	 * kernel builds, RCU read-side critical sections may be preempted,
       623	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       624	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       625	 * read-side critical sections may be preempted and they may also block, but
       626	 * only when acquiring spinlocks that are subject to priority inheritance.
       627	 */
       628	static inline void rcu_read_lock(void)
       629	{
==>    630		__rcu_read_lock();       
       631		__acquire(RCU);
       632		rcu_lock_acquire(&rcu_lock_map);
       633		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       634				 "rcu_read_lock() used illegally while idle");
       635	}
       636	
       637	/*
       638	 * So where is rcu_write_lock()?  It does not exist, as there is no
       639	 * way for writers to lock out RCU readers.  This is a feature, not
       640	 * a bug -- this property is what provides RCU's performance benefits.
       641	 * Of course, writers must coordinate with each other.  The normal
       642	 * spinlock primitives work well for this, but any other technique may be
       643	 * used as well.  RCU does not care how the writers keep out of each
       644	 * others' way, as long as they do so.
       645	 */
       646	
       647	/**
       648	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       649	 *
       650	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/processor.h:667
       647	{
       648		unsigned int eax, ebx, ecx, edx;
       649	
       650		cpuid(op, &eax, &ebx, &ecx, &edx);
       651	
       652		return ecx;
       653	}
       654	
       655	static inline unsigned int cpuid_edx(unsigned int op)
       656	{
       657		unsigned int eax, ebx, ecx, edx;
       658	
       659		cpuid(op, &eax, &ebx, &ecx, &edx);
       660	
       661		return edx;
       662	}
       663	
       664	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       665	static __always_inline void rep_nop(void)
       666	{
==>    667		asm volatile("rep; nop" ::: "memory");       
       668	}
       669	
       670	static __always_inline void cpu_relax(void)
       671	{
       672		rep_nop();
       673	}
       674	
       675	/*
       676	 * This function forces the icache and prefetched instruction stream to
       677	 * catch up with reality in two very specific cases:
       678	 *
       679	 *  a) Text was modified using one virtual address and is about to be executed
       680	 *     from the same physical page at a different virtual address.
       681	 *
       682	 *  b) Text was modified on a different CPU, may subsequently be
       683	 *     executed on this CPU, and you want to make sure the new version
       684	 *     gets executed.  This generally means you're calling this in a IPI.
       685	 *
       686	 * If you're calling this for a different reason, you're probably doing
       687	 * it wrong.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */


====================================================================================================================================================================================
Total: 886	Addresses: c278e3ef c10fd268 c278e72d c278e339 c10fd26e c278e43d
21	0xc10fd26c: virt_spin_lock at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/qspinlock.h:68
83	0xc278e43d: pv_queued_spin_unlock at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/paravirt.h:679
87	0xc278e72d: pv_queued_spin_unlock at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/paravirt.h:679
123	0xc278e329: __preempt_count_add at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/preempt.h:76
222	0xc278e3ef: pv_queued_spin_unlock at /root/2018-12232-i386/linux-4.17.1/./arch/x86/include/asm/paravirt.h:679
350	0xc10fd25b: arch_static_branch at /root/2018-12232-i386/linux-4.17.1/kernel/locking/qspinlock.c:295

21	c10fd26e arch_atomic_cmpxchg T: trace_20241115_161049_4_3_61.txt S: 61 I1: 4 I2: 3 IP1: c10fd268 IP2: c10fd26e PMA1: 338d188 PMA2: 338d188 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 2771425 IC2: 2771397
83	c278e43d pv_queued_spin_unlock T: trace_20241115_161056_2_2_18.txt S: 18 I1: 2 I2: 2 IP1: c278e43d IP2: c10fd268 PMA1: 3619be80 PMA2: 3619be80 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 3159530 IC2: 2448594
87	c278e72d pv_queued_spin_unlock T: trace_20241115_161059_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c278e72d IP2: c10fd268 PMA1: 3619be80 PMA2: 3619be80 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 3157971 IC2: 2447952
123	c278e339 arch_atomic_cmpxchg T: trace_20241115_161049_4_3_63.txt S: 63 I1: 4 I2: 3 IP1: c278e3ef IP2: c278e339 PMA1: 338d188 PMA2: 338d188 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 1 L2: 4 IC1: 2787389 IC2: 2787351
222	c278e3ef pv_queued_spin_unlock T: trace_20241115_161057_2_2_35.txt S: 35 I1: 2 I2: 2 IP1: c278e3ef IP2: c10fd268 PMA1: 319d31f4 PMA2: 319d31f4 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 4063008 IC2: 4062951
350	c10fd268 __read_once_size T: trace_20241115_161059_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c278e72d IP2: c10fd268 PMA1: 3619be80 PMA2: 3619be80 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 3157971 IC2: 2447952

++++++++++++++++++++++++
STATS: Distinct IPs: 38 Distinct pairs: 39 Distinct clusters: 12
++++++++++++++++++++++++
/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/qspinlock.h:68
        48	#endif
        49	
        50	#ifdef CONFIG_PARAVIRT
        51	DECLARE_STATIC_KEY_TRUE(virt_spin_lock_key);
        52	
        53	void native_pv_lock_init(void) __init;
        54	
        55	#define virt_spin_lock virt_spin_lock
        56	static inline bool virt_spin_lock(struct qspinlock *lock)
        57	{
        58		if (!static_branch_likely(&virt_spin_lock_key))
        59			return false;
        60	
        61		/*
        62		 * On hypervisors without PARAVIRT_SPINLOCKS support we fall
        63		 * back to a Test-and-Set spinlock, because fair locks have
        64		 * horrible lock 'holder' preemption issues.
        65		 */
        66	
        67		do {
==>     68			while (atomic_read(&lock->val) != 0)       
        69				cpu_relax();
        70		} while (atomic_cmpxchg(&lock->val, 0, _Q_LOCKED_VAL) != 0);
        71	
        72		return true;
        73	}
        74	#else
        75	static inline void native_pv_lock_init(void)
        76	{
        77	}
        78	#endif /* CONFIG_PARAVIRT */
        79	
        80	#include <asm-generic/qspinlock.h>
        81	
        82	#endif /* _ASM_X86_QSPINLOCK_H */

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/paravirt.h:679
       659	{
       660		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       661	}
       662	
       663	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       664					phys_addr_t phys, pgprot_t flags)
       665	{
       666		pv_mmu_ops.set_fixmap(idx, phys, flags);
       667	}
       668	
       669	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       670	
       671	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       672								u32 val)
       673	{
       674		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       675	}
       676	
       677	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       678	{
==>    679		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       680	}
       681	
       682	static __always_inline void pv_wait(u8 *ptr, u8 val)
       683	{
       684		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       685	}
       686	
       687	static __always_inline void pv_kick(int cpu)
       688	{
       689		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       690	}
       691	
       692	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       693	{
       694		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       695	}
       696	
       697	#endif /* SMP && PARAVIRT_SPINLOCKS */
       698	
       699	#ifdef CONFIG_X86_32

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/paravirt.h:679
       659	{
       660		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       661	}
       662	
       663	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       664					phys_addr_t phys, pgprot_t flags)
       665	{
       666		pv_mmu_ops.set_fixmap(idx, phys, flags);
       667	}
       668	
       669	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       670	
       671	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       672								u32 val)
       673	{
       674		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       675	}
       676	
       677	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       678	{
==>    679		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       680	}
       681	
       682	static __always_inline void pv_wait(u8 *ptr, u8 val)
       683	{
       684		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       685	}
       686	
       687	static __always_inline void pv_kick(int cpu)
       688	{
       689		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       690	}
       691	
       692	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       693	{
       694		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       695	}
       696	
       697	#endif /* SMP && PARAVIRT_SPINLOCKS */
       698	
       699	#ifdef CONFIG_X86_32

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//./arch/x86/include/asm/paravirt.h:679
       659	{
       660		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       661	}
       662	
       663	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       664					phys_addr_t phys, pgprot_t flags)
       665	{
       666		pv_mmu_ops.set_fixmap(idx, phys, flags);
       667	}
       668	
       669	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       670	
       671	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       672								u32 val)
       673	{
       674		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       675	}
       676	
       677	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       678	{
==>    679		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       680	}
       681	
       682	static __always_inline void pv_wait(u8 *ptr, u8 val)
       683	{
       684		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       685	}
       686	
       687	static __always_inline void pv_kick(int cpu)
       688	{
       689		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       690	}
       691	
       692	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       693	{
       694		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       695	}
       696	
       697	#endif /* SMP && PARAVIRT_SPINLOCKS */
       698	
       699	#ifdef CONFIG_X86_32

/root/snowboard-2018-12232/testsuite/kernel/2018-12232/source//kernel/locking/qspinlock.c:295
       275	 * @lock: Pointer to queued spinlock structure
       276	 * @val: Current value of the queued spinlock 32-bit word
       277	 *
       278	 * (queue tail, pending bit, lock value)
       279	 *
       280	 *              fast     :    slow                                  :    unlock
       281	 *                       :                                          :
       282	 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
       283	 *                       :       | ^--------.------.             /  :
       284	 *                       :       v           \      \            |  :
       285	 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
       286	 *                       :       | ^--'              |           |  :
       287	 *                       :       v                   |           |  :
       288	 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
       289	 *   queue               :       | ^--'                          |  :
       290	 *                       :       v                               |  :
       291	 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
       292	 *   queue               :         ^--'                             :
       293	 */
       294	void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
==>    295	{       
       296		struct mcs_spinlock *prev, *next, *node;
       297		u32 new, old, tail;
       298		int idx;
       299	
       300		BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
       301	
       302		if (pv_enabled())
       303			goto queue;
       304	
       305		if (virt_spin_lock(lock))
       306			return;
       307	
       308		/*
       309		 * wait for in-progress pending->locked hand-overs
       310		 *
       311		 * 0,1,0 -> 0,0,1
       312		 */
       313		if (val == _Q_PENDING_VAL) {
       314			while ((val = atomic_read(&lock->val)) == _Q_PENDING_VAL)
       315				cpu_relax();


