vmlinux map is loaded
Waiting for data race records

('Analyed ', 500, 749, ' data races in total')

====================================================================================================================================================================================
Total: 4	Addresses: c288e2f7 c10ea952
2	0xc288e2f7: sched_submit_work at /root/2019-6974-i386/linux-4.19/kernel/sched/core.c:3500
2	0xc10ea952: ttwu_do_wakeup at /root/2019-6974-i386/linux-4.19/kernel/sched/core.c:1653

2	c288e2f7 sched_submit_work T: trace_20241118_115909_4_4_19.txt S: 19 I1: 4 I2: 4 IP1: c10ea952 IP2: c288e2f7 PMA1: 34861388 PMA2: 34861388 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 577992 IC2: 577159
2	c10ea952 ttwu_do_wakeup T: trace_20241118_115909_4_4_19.txt S: 19 I1: 4 I2: 4 IP1: c10ea952 IP2: c288e2f7 PMA1: 34861388 PMA2: 34861388 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 577992 IC2: 577159

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/core.c:3500
      3480	}
      3481	
      3482	void __noreturn do_task_dead(void)
      3483	{
      3484		/* Causes final put_task_struct in finish_task_switch(): */
      3485		set_special_state(TASK_DEAD);
      3486	
      3487		/* Tell freezer to ignore us: */
      3488		current->flags |= PF_NOFREEZE;
      3489	
      3490		__schedule(false);
      3491		BUG();
      3492	
      3493		/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
      3494		for (;;)
      3495			cpu_relax();
      3496	}
      3497	
      3498	static inline void sched_submit_work(struct task_struct *tsk)
      3499	{
==>   3500		if (!tsk->state || tsk_is_pi_blocked(tsk))       
      3501			return;
      3502		/*
      3503		 * If we are going to sleep and we have plugged IO queued,
      3504		 * make sure to submit it to avoid deadlocks.
      3505		 */
      3506		if (blk_needs_flush_plug(tsk))
      3507			blk_schedule_flush_plug(tsk);
      3508	}
      3509	
      3510	asmlinkage __visible void __sched schedule(void)
      3511	{
      3512		struct task_struct *tsk = current;
      3513	
      3514		sched_submit_work(tsk);
      3515		do {
      3516			preempt_disable();
      3517			__schedule(false);
      3518			sched_preempt_enable_no_resched();
      3519		} while (need_resched());
      3520	}

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/core.c:1653
      1633			__schedstat_inc(p->se.statistics.nr_wakeups_sync);
      1634	}
      1635	
      1636	static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags)
      1637	{
      1638		activate_task(rq, p, en_flags);
      1639		p->on_rq = TASK_ON_RQ_QUEUED;
      1640	
      1641		/* If a worker is waking up, notify the workqueue: */
      1642		if (p->flags & PF_WQ_WORKER)
      1643			wq_worker_waking_up(p, cpu_of(rq));
      1644	}
      1645	
      1646	/*
      1647	 * Mark the task runnable and perform wakeup-preemption.
      1648	 */
      1649	static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
      1650				   struct rq_flags *rf)
      1651	{
      1652		check_preempt_curr(rq, p, wake_flags);
==>   1653		p->state = TASK_RUNNING;       
      1654		trace_sched_wakeup(p);
      1655	
      1656	#ifdef CONFIG_SMP
      1657		if (p->sched_class->task_woken) {
      1658			/*
      1659			 * Our task @p is fully woken up and running; so its safe to
      1660			 * drop the rq->lock, hereafter rq is only used for statistics.
      1661			 */
      1662			rq_unpin_lock(rq, rf);
      1663			p->sched_class->task_woken(rq, p);
      1664			rq_repin_lock(rq, rf);
      1665		}
      1666	
      1667		if (rq->idle_stamp) {
      1668			u64 delta = rq_clock(rq) - rq->idle_stamp;
      1669			u64 max = 2*rq->max_idle_balance_cost;
      1670	
      1671			update_avg(&rq->avg_idle, delta);
      1672	
      1673			if (rq->avg_idle > max)


====================================================================================================================================================================================
Total: 12	Addresses: c11f536b c11449ca
2	0xc11449ca: constant_test_bit at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/bitops.h:328
10	0xc11f536b: set_bit at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/bitops.h:76

2	c11449ca constant_test_bit T: trace_20241118_115903_2_3_19.txt S: 19 I1: 2 I2: 3 IP1: c11449ca IP2: c11f536b PMA1: 358962c4 PMA2: 358962c4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 1 IC1: 138220 IC2: 109799
10	c11f536b set_bit T: trace_20241118_115903_2_3_19.txt S: 19 I1: 2 I2: 3 IP1: c11449ca IP2: c11f536b PMA1: 358962c4 PMA2: 358962c4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 1 IC1: 138220 IC2: 109799

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/bitops.h:328
       308		return oldbit;
       309	}
       310	
       311	/**
       312	 * test_and_change_bit - Change a bit and return its old value
       313	 * @nr: Bit to change
       314	 * @addr: Address to count from
       315	 *
       316	 * This operation is atomic and cannot be reordered.
       317	 * It also implies a memory barrier.
       318	 */
       319	static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
       320	{
       321		GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc),
       322		                 *addr, "Ir", nr, "%0", c);
       323	}
       324	
       325	static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
       326	{
       327		return ((1UL << (nr & (BITS_PER_LONG-1))) &
==>    328			(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;       
       329	}
       330	
       331	static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
       332	{
       333		bool oldbit;
       334	
       335		asm volatile(__ASM_SIZE(bt) " %2,%1"
       336			     CC_SET(c)
       337			     : CC_OUT(c) (oldbit)
       338			     : "m" (*(unsigned long *)addr), "Ir" (nr));
       339	
       340		return oldbit;
       341	}
       342	
       343	#if 0 /* Fool kernel-doc since it doesn't do macros yet */
       344	/**
       345	 * test_bit - Determine whether a bit is set
       346	 * @nr: bit number to test
       347	 * @addr: Address to start counting from
       348	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/bitops.h:76
        56	
        57	/**
        58	 * set_bit - Atomically set a bit in memory
        59	 * @nr: the bit to set
        60	 * @addr: the address to start counting from
        61	 *
        62	 * This function is atomic and may not be reordered.  See __set_bit()
        63	 * if you do not require the atomic guarantees.
        64	 *
        65	 * Note: there are no guarantees that this function will not be reordered
        66	 * on non x86 architectures, so if you are writing portable code,
        67	 * make sure not to rely on its reordering guarantees.
        68	 *
        69	 * Note that @nr may be almost arbitrarily large; this function is not
        70	 * restricted to acting on a single-word quantity.
        71	 */
        72	static __always_inline void
        73	set_bit(long nr, volatile unsigned long *addr)
        74	{
        75		if (IS_IMMEDIATE(nr)) {
==>     76			asm volatile(LOCK_PREFIX "orb %1,%0"       
        77				: CONST_MASK_ADDR(nr, addr)
        78				: "iq" ((u8)CONST_MASK(nr))
        79				: "memory");
        80		} else {
        81			asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"
        82				: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
        83		}
        84	}
        85	
        86	/**
        87	 * __set_bit - Set a bit in memory
        88	 * @nr: the bit to set
        89	 * @addr: the address to start counting from
        90	 *
        91	 * Unlike set_bit(), this function is non-atomic and may be reordered.
        92	 * If it's called on the same region of memory simultaneously, the effect
        93	 * may be that only one operation succeeds.
        94	 */
        95	static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
        96	{


====================================================================================================================================================================================
Total: 60	Addresses: c11038da c1103f5b
30	0xc11038d3: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:351
30	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

30	c11038da list_empty_careful T: trace_20241118_115859_2_5_33.txt S: 33 I1: 2 I2: 5 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 132013 IC2: 127963
30	c1103f5b __write_once_size T: trace_20241118_115859_2_5_33.txt S: 33 I1: 2 I2: 5 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 132013 IC2: 127963

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:351
       331		schedule();
       332		spin_lock_irq(&wq->lock);
       333	
       334		return 0;
       335	}
       336	EXPORT_SYMBOL(do_wait_intr_irq);
       337	
       338	/**
       339	 * finish_wait - clean up after waiting in a queue
       340	 * @wq_head: waitqueue waited on
       341	 * @wq_entry: wait descriptor
       342	 *
       343	 * Sets current thread back to running state and removes
       344	 * the wait descriptor from the given waitqueue if still
       345	 * queued.
       346	 */
       347	void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
       348	{
       349		unsigned long flags;
       350	
==>    351		__set_current_state(TASK_RUNNING);       
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 80	Addresses: c10e82c9 c10eb2ba
40	0xc10e82c1: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/sched/core.c:2671
40	0xc10eb2b8: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665

40	c10e82c9 __write_once_size T: trace_20241118_115920_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 141906 IC2: 141294
40	c10eb2ba __read_once_size T: trace_20241118_115920_2_3_63.txt S: 63 I1: 2 I2: 3 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 141906 IC2: 141294

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/core.c:2671
      2651		 * Also, see FORK_PREEMPT_COUNT.
      2652		 */
      2653		if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
      2654			      "corrupted preempt_count: %s/%d/0x%x\n",
      2655			      current->comm, current->pid, preempt_count()))
      2656			preempt_count_set(FORK_PREEMPT_COUNT);
      2657	
      2658		rq->prev_mm = NULL;
      2659	
      2660		/*
      2661		 * A task struct has one reference for the use as "current".
      2662		 * If a task dies, then it sets TASK_DEAD in tsk->state and calls
      2663		 * schedule one last time. The schedule call will never return, and
      2664		 * the scheduled task must drop that reference.
      2665		 *
      2666		 * We must observe prev->state before clearing prev->on_cpu (in
      2667		 * finish_task), otherwise a concurrent wakeup can get prev
      2668		 * running on another CPU and we could rave with its RUNNING -> DEAD
      2669		 * transition, resulting in a double drop.
      2670		 */
==>   2671		prev_state = prev->state;       
      2672		vtime_task_switch(prev);
      2673		perf_event_task_sched_in(prev, current);
      2674		finish_task(prev);
      2675		finish_lock_switch(rq);
      2676		finish_arch_post_lock_switch();
      2677		kcov_finish_switch(current);
      2678	
      2679		fire_sched_in_preempt_notifiers(current);
      2680		/*
      2681		 * When switching through a kernel thread, the loop in
      2682		 * membarrier_{private,global}_expedited() may have observed that
      2683		 * kernel thread and not issued an IPI. It is therefore possible to
      2684		 * schedule between user->kernel->user threads without passing though
      2685		 * switch_mm(). Membarrier requires a barrier after storing to
      2686		 * rq->curr, before returning to userspace, so provide them here:
      2687		 *
      2688		 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
      2689		 *   provided by mmdrop(),
      2690		 * - a sync_core for SYNC_CORE.
      2691		 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.


====================================================================================================================================================================================
Total: 120	Addresses: c1108c1d c288f36e c288f721 c288f0cd c1108bca c288f231 c288f387 c288f0a0 c288f53e
1	0xc1108c1b: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665
1	0xc288f0c8: rcu_read_lock at /root/2019-6974-i386/linux-4.19/./include/linux/rcupdate.h:627
2	0xc288f36c: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665
2	0xc1108bc5: rcu_read_lock at /root/2019-6974-i386/linux-4.19/./include/linux/rcupdate.h:627
8	0xc288f385: __mutex_trylock_or_owner at /root/2019-6974-i386/linux-4.19/kernel/locking/mutex.c:110
10	0xc288f22f: __mutex_trylock_or_owner at /root/2019-6974-i386/linux-4.19/kernel/locking/mutex.c:110
12	0xc288f099: __preempt_count_add at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/preempt.h:76
36	0xc288f537: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15
48	0xc288f718: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15

1	c1108c1d __read_once_size T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c1108c1d IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1854374 IC2: 1854351
1	c288f0cd __read_once_size T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f721 IP2: c288f0cd PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 1806814 IC2: 1806812
2	c288f36e __read_once_size T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f36e IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 1854407 IC2: 1854351
2	c1108bca __read_once_size T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f721 IP2: c1108bca PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 1854351 IC2: 1854349
8	c288f387 arch_atomic_cmpxchg T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f387 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1854416 IC2: 1854351
10	c288f231 arch_atomic_cmpxchg T: trace_20241118_115901_5_3_21.txt S: 21 I1: 5 I2: 3 IP1: c288f231 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 746270 IC2: 746241
12	c288f0a0 __read_once_size T: trace_20241118_115901_5_3_21.txt S: 21 I1: 5 I2: 3 IP1: c288f0a0 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 746261 IC2: 746241
36	c288f53e arch_atomic_try_cmpxchg T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f53e IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 445051 IC2: 416028
48	c288f721 arch_atomic_cmpxchg T: trace_20241118_115904_2_3_52.txt S: 52 I1: 2 I2: 3 IP1: c288f387 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 1854416 IC2: 1854351

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./include/linux/rcupdate.h:627
       607	 * RCU read-side critical sections may be nested.  Any deferred actions
       608	 * will be deferred until the outermost RCU read-side critical section
       609	 * completes.
       610	 *
       611	 * You can avoid reading and understanding the next paragraph by
       612	 * following this rule: don't put anything in an rcu_read_lock() RCU
       613	 * read-side critical section that would block in a !PREEMPT kernel.
       614	 * But if you want the full story, read on!
       615	 *
       616	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       617	 * it is illegal to block while in an RCU read-side critical section.
       618	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       619	 * kernel builds, RCU read-side critical sections may be preempted,
       620	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       621	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       622	 * read-side critical sections may be preempted and they may also block, but
       623	 * only when acquiring spinlocks that are subject to priority inheritance.
       624	 */
       625	static inline void rcu_read_lock(void)
       626	{
==>    627		__rcu_read_lock();       
       628		__acquire(RCU);
       629		rcu_lock_acquire(&rcu_lock_map);
       630		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       631				 "rcu_read_lock() used illegally while idle");
       632	}
       633	
       634	/*
       635	 * So where is rcu_write_lock()?  It does not exist, as there is no
       636	 * way for writers to lock out RCU readers.  This is a feature, not
       637	 * a bug -- this property is what provides RCU's performance benefits.
       638	 * Of course, writers must coordinate with each other.  The normal
       639	 * spinlock primitives work well for this, but any other technique may be
       640	 * used as well.  RCU does not care how the writers keep out of each
       641	 * others' way, as long as they do so.
       642	 */
       643	
       644	/**
       645	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       646	 *
       647	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./include/linux/rcupdate.h:627
       607	 * RCU read-side critical sections may be nested.  Any deferred actions
       608	 * will be deferred until the outermost RCU read-side critical section
       609	 * completes.
       610	 *
       611	 * You can avoid reading and understanding the next paragraph by
       612	 * following this rule: don't put anything in an rcu_read_lock() RCU
       613	 * read-side critical section that would block in a !PREEMPT kernel.
       614	 * But if you want the full story, read on!
       615	 *
       616	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       617	 * it is illegal to block while in an RCU read-side critical section.
       618	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       619	 * kernel builds, RCU read-side critical sections may be preempted,
       620	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       621	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       622	 * read-side critical sections may be preempted and they may also block, but
       623	 * only when acquiring spinlocks that are subject to priority inheritance.
       624	 */
       625	static inline void rcu_read_lock(void)
       626	{
==>    627		__rcu_read_lock();       
       628		__acquire(RCU);
       629		rcu_lock_acquire(&rcu_lock_map);
       630		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       631				 "rcu_read_lock() used illegally while idle");
       632	}
       633	
       634	/*
       635	 * So where is rcu_write_lock()?  It does not exist, as there is no
       636	 * way for writers to lock out RCU readers.  This is a feature, not
       637	 * a bug -- this property is what provides RCU's performance benefits.
       638	 * Of course, writers must coordinate with each other.  The normal
       639	 * spinlock primitives work well for this, but any other technique may be
       640	 * used as well.  RCU does not care how the writers keep out of each
       641	 * others' way, as long as they do so.
       642	 */
       643	
       644	/**
       645	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       646	 *
       647	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */


====================================================================================================================================================================================
Total: 184	Addresses: c1957205 c1103bb3 c1103f5e c1103920 c1957356
23	0xc1957205: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:831
23	0xc1103bb3: __wake_up_sync_key at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:199
23	0xc110390e: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:370
23	0xc1957356: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:843
92	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

23	c1957205 copy_page_to_iter T: trace_20241118_115852_4_5_63.txt S: 63 I1: 4 I2: 5 IP1: c1957205 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115118 IC2: 110997
23	c1103bb3 __wake_up_sync_key T: trace_20241118_115852_4_5_63.txt S: 63 I1: 4 I2: 5 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115825 IC2: 110997
23	c1103920 list_empty_careful T: trace_20241118_115852_4_5_63.txt S: 63 I1: 4 I2: 5 IP1: c1103920 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115049 IC2: 110997
23	c1957356 copy_page_to_iter T: trace_20241118_115852_4_5_63.txt S: 63 I1: 4 I2: 5 IP1: c1957356 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115722 IC2: 110997
92	c1103f5e INIT_LIST_HEAD T: trace_20241118_115852_4_5_63.txt S: 63 I1: 4 I2: 5 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115825 IC2: 110997

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:831
       811		)
       812	
       813		iov_iter_advance(i, bytes);
       814		return true;
       815	}
       816	EXPORT_SYMBOL(_copy_from_iter_full_nocache);
       817	
       818	static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
       819	{
       820		struct page *head = compound_head(page);
       821		size_t v = n + offset + page_address(page) - page_address(head);
       822	
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
==>    831	{       
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
       843	}
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:199
       179	 * away soon, so while the target thread will be woken up, it will not
       180	 * be migrated to another CPU - ie. the two threads are 'synchronized'
       181	 * with each other. This can prevent needless bouncing between CPUs.
       182	 *
       183	 * On UP it can prevent extra preemption.
       184	 *
       185	 * If this function wakes up a task, it executes a full memory barrier before
       186	 * accessing the task state.
       187	 */
       188	void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
       189				int nr_exclusive, void *key)
       190	{
       191		int wake_flags = 1; /* XXX WF_SYNC */
       192	
       193		if (unlikely(!wq_head))
       194			return;
       195	
       196		if (unlikely(nr_exclusive != 1))
       197			wake_flags = 0;
       198	
==>    199		__wake_up_common_lock(wq_head, mode, nr_exclusive, wake_flags, key);       
       200	}
       201	EXPORT_SYMBOL_GPL(__wake_up_sync_key);
       202	
       203	/*
       204	 * __wake_up_sync - see __wake_up_sync_key()
       205	 */
       206	void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode, int nr_exclusive)
       207	{
       208		__wake_up_sync_key(wq_head, mode, nr_exclusive, NULL);
       209	}
       210	EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
       211	
       212	/*
       213	 * Note: we use "set_current_state()" _after_ the wait-queue add,
       214	 * because we need a memory barrier there on SMP, so that any
       215	 * wake-function that tests for the wait-queue being active
       216	 * will be guaranteed to see waitqueue addition _or_ subsequent
       217	 * tests in this thread will see the wakeup having taken place.
       218	 *
       219	 * The spin_unlock() itself is semi-permeable and only protects

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:370
       350	
       351		__set_current_state(TASK_RUNNING);
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
==>    370	}       
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
       381	}
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:843
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
       831	{
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
==>    843	}       
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {
       852			WARN_ON(1);
       853			return 0;
       854		}
       855		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       856			void *kaddr = kmap_atomic(page);
       857			size_t wanted = _copy_from_iter(kaddr + offset, bytes, i);
       858			kunmap_atomic(kaddr);
       859			return wanted;
       860		} else
       861			return copy_page_from_iter_iovec(page, offset, bytes, i);
       862	}
       863	EXPORT_SYMBOL(copy_page_from_iter);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 1038	Addresses: c2892c8d c11093c8 c289299d c289294f
5	0xc2892c8d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
21	0xc289299d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
493	0xc289294f: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
519	0xc11093b8: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/locking/qspinlock.c:295

5	c2892c8d pv_queued_spin_unlock T: trace_20241118_115920_2_3_47.txt S: 47 I1: 2 I2: 3 IP1: c2892c8d IP2: c11093c8 PMA1: 34815b44 PMA2: 34815b44 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 138487 IC2: 138451
21	c289299d pv_queued_spin_unlock T: trace_20241118_115920_2_3_56.txt S: 56 I1: 2 I2: 3 IP1: c289299d IP2: c11093c8 PMA1: 355d8200 PMA2: 355d8200 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 354045 IC2: 352582
493	c289294f pv_queued_spin_unlock T: trace_20241118_115922_3_3_43.txt S: 43 I1: 3 I2: 3 IP1: c289294f IP2: c11093c8 PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 214726 IC2: 205083
519	c11093c8 __read_once_size T: trace_20241118_115922_3_3_43.txt S: 43 I1: 3 I2: 3 IP1: c289294f IP2: c11093c8 PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 214726 IC2: 205083

++++++++++++++++++++++++
STATS: Distinct IPs: 26 Distinct pairs: 25 Distinct clusters: 7
++++++++++++++++++++++++
/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/qspinlock.c:295
       275	 * @lock: Pointer to queued spinlock structure
       276	 * @val: Current value of the queued spinlock 32-bit word
       277	 *
       278	 * (queue tail, pending bit, lock value)
       279	 *
       280	 *              fast     :    slow                                  :    unlock
       281	 *                       :                                          :
       282	 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
       283	 *                       :       | ^--------.------.             /  :
       284	 *                       :       v           \      \            |  :
       285	 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
       286	 *                       :       | ^--'              |           |  :
       287	 *                       :       v                   |           |  :
       288	 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
       289	 *   queue               :       | ^--'                          |  :
       290	 *                       :       v                               |  :
       291	 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
       292	 *   queue               :         ^--'                             :
       293	 */
       294	void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
==>    295	{       
       296		struct mcs_spinlock *prev, *next, *node;
       297		u32 old, tail;
       298		int idx;
       299	
       300		BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
       301	
       302		if (pv_enabled())
       303			goto pv_queue;
       304	
       305		if (virt_spin_lock(lock))
       306			return;
       307	
       308		/*
       309		 * Wait for in-progress pending->locked hand-overs with a bounded
       310		 * number of spins so that we guarantee forward progress.
       311		 *
       312		 * 0,1,0 -> 0,0,1
       313		 */
       314		if (val == _Q_PENDING_VAL) {
       315			int cnt = _Q_PENDING_LOOPS;


