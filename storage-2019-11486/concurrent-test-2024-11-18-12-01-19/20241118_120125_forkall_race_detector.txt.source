vmlinux map is loaded
Waiting for data race records

('Analyed ', 500, 1281, ' data races in total')
('Analyed ', 1000, 1281, ' data races in total')

====================================================================================================================================================================================
Total: 24	Addresses: c11f536b c11449ca
4	0xc11449ca: constant_test_bit at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/bitops.h:328
20	0xc11f536b: set_bit at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/bitops.h:76

4	c11449ca constant_test_bit T: trace_20241118_120300_3_3_19.txt S: 19 I1: 3 I2: 3 IP1: c11449ca IP2: c11f536b PMA1: 358962c4 PMA2: 358962c4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 1 IC1: 137587 IC2: 109320
20	c11f536b set_bit T: trace_20241118_120300_3_3_19.txt S: 19 I1: 3 I2: 3 IP1: c11449ca IP2: c11f536b PMA1: 358962c4 PMA2: 358962c4 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 1 IC1: 137587 IC2: 109320

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/bitops.h:328
       308		return oldbit;
       309	}
       310	
       311	/**
       312	 * test_and_change_bit - Change a bit and return its old value
       313	 * @nr: Bit to change
       314	 * @addr: Address to count from
       315	 *
       316	 * This operation is atomic and cannot be reordered.
       317	 * It also implies a memory barrier.
       318	 */
       319	static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
       320	{
       321		GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc),
       322		                 *addr, "Ir", nr, "%0", c);
       323	}
       324	
       325	static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
       326	{
       327		return ((1UL << (nr & (BITS_PER_LONG-1))) &
==>    328			(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;       
       329	}
       330	
       331	static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)
       332	{
       333		bool oldbit;
       334	
       335		asm volatile(__ASM_SIZE(bt) " %2,%1"
       336			     CC_SET(c)
       337			     : CC_OUT(c) (oldbit)
       338			     : "m" (*(unsigned long *)addr), "Ir" (nr));
       339	
       340		return oldbit;
       341	}
       342	
       343	#if 0 /* Fool kernel-doc since it doesn't do macros yet */
       344	/**
       345	 * test_bit - Determine whether a bit is set
       346	 * @nr: bit number to test
       347	 * @addr: Address to start counting from
       348	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/bitops.h:76
        56	
        57	/**
        58	 * set_bit - Atomically set a bit in memory
        59	 * @nr: the bit to set
        60	 * @addr: the address to start counting from
        61	 *
        62	 * This function is atomic and may not be reordered.  See __set_bit()
        63	 * if you do not require the atomic guarantees.
        64	 *
        65	 * Note: there are no guarantees that this function will not be reordered
        66	 * on non x86 architectures, so if you are writing portable code,
        67	 * make sure not to rely on its reordering guarantees.
        68	 *
        69	 * Note that @nr may be almost arbitrarily large; this function is not
        70	 * restricted to acting on a single-word quantity.
        71	 */
        72	static __always_inline void
        73	set_bit(long nr, volatile unsigned long *addr)
        74	{
        75		if (IS_IMMEDIATE(nr)) {
==>     76			asm volatile(LOCK_PREFIX "orb %1,%0"       
        77				: CONST_MASK_ADDR(nr, addr)
        78				: "iq" ((u8)CONST_MASK(nr))
        79				: "memory");
        80		} else {
        81			asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"
        82				: BITOP_ADDR(addr) : "Ir" (nr) : "memory");
        83		}
        84	}
        85	
        86	/**
        87	 * __set_bit - Set a bit in memory
        88	 * @nr: the bit to set
        89	 * @addr: the address to start counting from
        90	 *
        91	 * Unlike set_bit(), this function is non-atomic and may be reordered.
        92	 * If it's called on the same region of memory simultaneously, the effect
        93	 * may be that only one operation succeeds.
        94	 */
        95	static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
        96	{


====================================================================================================================================================================================
Total: 118	Addresses: c10e82c9 c10eb2ba
59	0xc10e82c1: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/sched/core.c:2671
59	0xc10eb2b8: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665

59	c10e82c9 __write_once_size T: trace_20241118_120313_4_2_37.txt S: 37 I1: 4 I2: 2 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 173897 IC2: 173888
59	c10eb2ba __read_once_size T: trace_20241118_120313_4_2_37.txt S: 37 I1: 4 I2: 2 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 173897 IC2: 173888

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/core.c:2671
      2651		 * Also, see FORK_PREEMPT_COUNT.
      2652		 */
      2653		if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
      2654			      "corrupted preempt_count: %s/%d/0x%x\n",
      2655			      current->comm, current->pid, preempt_count()))
      2656			preempt_count_set(FORK_PREEMPT_COUNT);
      2657	
      2658		rq->prev_mm = NULL;
      2659	
      2660		/*
      2661		 * A task struct has one reference for the use as "current".
      2662		 * If a task dies, then it sets TASK_DEAD in tsk->state and calls
      2663		 * schedule one last time. The schedule call will never return, and
      2664		 * the scheduled task must drop that reference.
      2665		 *
      2666		 * We must observe prev->state before clearing prev->on_cpu (in
      2667		 * finish_task), otherwise a concurrent wakeup can get prev
      2668		 * running on another CPU and we could rave with its RUNNING -> DEAD
      2669		 * transition, resulting in a double drop.
      2670		 */
==>   2671		prev_state = prev->state;       
      2672		vtime_task_switch(prev);
      2673		perf_event_task_sched_in(prev, current);
      2674		finish_task(prev);
      2675		finish_lock_switch(rq);
      2676		finish_arch_post_lock_switch();
      2677		kcov_finish_switch(current);
      2678	
      2679		fire_sched_in_preempt_notifiers(current);
      2680		/*
      2681		 * When switching through a kernel thread, the loop in
      2682		 * membarrier_{private,global}_expedited() may have observed that
      2683		 * kernel thread and not issued an IPI. It is therefore possible to
      2684		 * schedule between user->kernel->user threads without passing though
      2685		 * switch_mm(). Membarrier requires a barrier after storing to
      2686		 * rq->curr, before returning to userspace, so provide them here:
      2687		 *
      2688		 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
      2689		 *   provided by mmdrop(),
      2690		 * - a sync_core for SYNC_CORE.
      2691		 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.


====================================================================================================================================================================================
Total: 140	Addresses: c11038da c1103f5b
70	0xc11038d3: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:351
70	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

70	c11038da list_empty_careful T: trace_20241118_120314_4_2_54.txt S: 54 I1: 4 I2: 2 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115658 IC2: 111608
70	c1103f5b __write_once_size T: trace_20241118_120314_4_2_54.txt S: 54 I1: 4 I2: 2 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115658 IC2: 111608

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:351
       331		schedule();
       332		spin_lock_irq(&wq->lock);
       333	
       334		return 0;
       335	}
       336	EXPORT_SYMBOL(do_wait_intr_irq);
       337	
       338	/**
       339	 * finish_wait - clean up after waiting in a queue
       340	 * @wq_head: waitqueue waited on
       341	 * @wq_entry: wait descriptor
       342	 *
       343	 * Sets current thread back to running state and removes
       344	 * the wait descriptor from the given waitqueue if still
       345	 * queued.
       346	 */
       347	void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
       348	{
       349		unsigned long flags;
       350	
==>    351		__set_current_state(TASK_RUNNING);       
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 190	Addresses: c1108c1d c288f36e c288f721 c288f0cd c1108bca c288f231 c288f387 c288f0a0 c288f53e
4	0xc288f385: __mutex_trylock_or_owner at /root/2019-6974-i386/linux-4.19/kernel/locking/mutex.c:110
6	0xc288f22f: __mutex_trylock_or_owner at /root/2019-6974-i386/linux-4.19/kernel/locking/mutex.c:110
7	0xc288f0c8: rcu_read_lock at /root/2019-6974-i386/linux-4.19/./include/linux/rcupdate.h:627
8	0xc1108bc5: rcu_read_lock at /root/2019-6974-i386/linux-4.19/./include/linux/rcupdate.h:627
9	0xc1108c1b: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665
9	0xc288f36c: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665
10	0xc288f099: __preempt_count_add at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/preempt.h:76
58	0xc288f718: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15
79	0xc288f537: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15

4	c288f387 arch_atomic_cmpxchg T: trace_20241118_120258_2_5_48.txt S: 48 I1: 2 I2: 5 IP1: c288f387 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 990451 IC2: 990408
6	c288f231 arch_atomic_cmpxchg T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f231 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 1 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 666069 IC2: 666040
7	c288f0cd __read_once_size T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f0cd IP2: c288f53e PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 693514 IC2: 693475
8	c1108bca __read_once_size T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f721 IP2: c1108bca PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 694210 IC2: 693603
9	c1108c1d __read_once_size T: trace_20241118_120259_2_5_63.txt S: 63 I1: 2 I2: 5 IP1: c1108c1d IP2: c288f53e PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 797463 IC2: 797378
9	c288f36e __read_once_size T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f36e IP2: c288f53e PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 693574 IC2: 693475
10	c288f0a0 __read_once_size T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f0a0 IP2: c288f53e PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 693496 IC2: 693475
58	c288f721 arch_atomic_cmpxchg T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c288f721 IP2: c1108bca PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 694210 IC2: 693603
79	c288f53e arch_atomic_try_cmpxchg T: trace_20241118_120300_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c1108bca IP2: c288f53e PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 693603 IC2: 693475

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/mutex.c:110
        90				if (likely(task != curr))
        91					break;
        92	
        93				if (likely(!(flags & MUTEX_FLAG_PICKUP)))
        94					break;
        95	
        96				flags &= ~MUTEX_FLAG_PICKUP;
        97			} else {
        98	#ifdef CONFIG_DEBUG_MUTEXES
        99				DEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);
       100	#endif
       101			}
       102	
       103			/*
       104			 * We set the HANDOFF bit, we must make sure it doesn't live
       105			 * past the point where we acquire it. This would be possible
       106			 * if we (accidentally) set the bit on an unlocked mutex.
       107			 */
       108			flags &= ~MUTEX_FLAG_HANDOFF;
       109	
==>    110			old = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);       
       111			if (old == owner)
       112				return NULL;
       113	
       114			owner = old;
       115		}
       116	
       117		return __owner_task(owner);
       118	}
       119	
       120	/*
       121	 * Actual trylock that will work on any unlocked state.
       122	 */
       123	static inline bool __mutex_trylock(struct mutex *lock)
       124	{
       125		return !__mutex_trylock_or_owner(lock);
       126	}
       127	
       128	#ifndef CONFIG_DEBUG_LOCK_ALLOC
       129	/*
       130	 * Lockdep annotations are contained to the slow paths for simplicity.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./include/linux/rcupdate.h:627
       607	 * RCU read-side critical sections may be nested.  Any deferred actions
       608	 * will be deferred until the outermost RCU read-side critical section
       609	 * completes.
       610	 *
       611	 * You can avoid reading and understanding the next paragraph by
       612	 * following this rule: don't put anything in an rcu_read_lock() RCU
       613	 * read-side critical section that would block in a !PREEMPT kernel.
       614	 * But if you want the full story, read on!
       615	 *
       616	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       617	 * it is illegal to block while in an RCU read-side critical section.
       618	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       619	 * kernel builds, RCU read-side critical sections may be preempted,
       620	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       621	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       622	 * read-side critical sections may be preempted and they may also block, but
       623	 * only when acquiring spinlocks that are subject to priority inheritance.
       624	 */
       625	static inline void rcu_read_lock(void)
       626	{
==>    627		__rcu_read_lock();       
       628		__acquire(RCU);
       629		rcu_lock_acquire(&rcu_lock_map);
       630		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       631				 "rcu_read_lock() used illegally while idle");
       632	}
       633	
       634	/*
       635	 * So where is rcu_write_lock()?  It does not exist, as there is no
       636	 * way for writers to lock out RCU readers.  This is a feature, not
       637	 * a bug -- this property is what provides RCU's performance benefits.
       638	 * Of course, writers must coordinate with each other.  The normal
       639	 * spinlock primitives work well for this, but any other technique may be
       640	 * used as well.  RCU does not care how the writers keep out of each
       641	 * others' way, as long as they do so.
       642	 */
       643	
       644	/**
       645	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       646	 *
       647	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./include/linux/rcupdate.h:627
       607	 * RCU read-side critical sections may be nested.  Any deferred actions
       608	 * will be deferred until the outermost RCU read-side critical section
       609	 * completes.
       610	 *
       611	 * You can avoid reading and understanding the next paragraph by
       612	 * following this rule: don't put anything in an rcu_read_lock() RCU
       613	 * read-side critical section that would block in a !PREEMPT kernel.
       614	 * But if you want the full story, read on!
       615	 *
       616	 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
       617	 * it is illegal to block while in an RCU read-side critical section.
       618	 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
       619	 * kernel builds, RCU read-side critical sections may be preempted,
       620	 * but explicit blocking is illegal.  Finally, in preemptible RCU
       621	 * implementations in real-time (with -rt patchset) kernel builds, RCU
       622	 * read-side critical sections may be preempted and they may also block, but
       623	 * only when acquiring spinlocks that are subject to priority inheritance.
       624	 */
       625	static inline void rcu_read_lock(void)
       626	{
==>    627		__rcu_read_lock();       
       628		__acquire(RCU);
       629		rcu_lock_acquire(&rcu_lock_map);
       630		RCU_LOCKDEP_WARN(!rcu_is_watching(),
       631				 "rcu_read_lock() used illegally while idle");
       632	}
       633	
       634	/*
       635	 * So where is rcu_write_lock()?  It does not exist, as there is no
       636	 * way for writers to lock out RCU readers.  This is a feature, not
       637	 * a bug -- this property is what provides RCU's performance benefits.
       638	 * Of course, writers must coordinate with each other.  The normal
       639	 * spinlock primitives work well for this, but any other technique may be
       640	 * used as well.  RCU does not care how the writers keep out of each
       641	 * others' way, as long as they do so.
       642	 */
       643	
       644	/**
       645	 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
       646	 *
       647	 * In most situations, rcu_read_unlock() is immune from deadlock.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */


====================================================================================================================================================================================
Total: 224	Addresses: c1957205 c1103bb3 c1103f5e c289308d c1103920 c1957356
1	0xc289308d: resume_kernel at /root/2019-6974-i386/linux-4.19/arch/x86/entry/entry_32.S:769
27	0xc1103bb3: __wake_up_sync_key at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:199
28	0xc1957205: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:831
28	0xc110390e: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:370
28	0xc1957356: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:843
112	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

1	c289308d resume_kernel T: trace_20241118_120254_5_3_17.txt S: 17 I1: 5 I2: 3 IP1: c289308d IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 1 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 198448 IC2: 187186
27	c1103bb3 __wake_up_sync_key T: trace_20241118_120256_5_3_63.txt S: 63 I1: 5 I2: 3 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115825 IC2: 110997
28	c1957205 copy_page_to_iter T: trace_20241118_120256_5_3_63.txt S: 63 I1: 5 I2: 3 IP1: c1957205 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115118 IC2: 110997
28	c1103920 list_empty_careful T: trace_20241118_120256_5_3_63.txt S: 63 I1: 5 I2: 3 IP1: c1103920 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115049 IC2: 110997
28	c1957356 copy_page_to_iter T: trace_20241118_120256_5_3_63.txt S: 63 I1: 5 I2: 3 IP1: c1957356 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 115722 IC2: 110997
112	c1103f5e INIT_LIST_HEAD T: trace_20241118_120256_5_3_63.txt S: 63 I1: 5 I2: 3 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 115825 IC2: 110997

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//arch/x86/entry/entry_32.S:769
       749		movl	PT_CS(%esp), %eax
       750		andl	$SEGMENT_RPL_MASK, %eax
       751	#endif
       752		cmpl	$USER_RPL, %eax
       753		jb	resume_kernel			# not returning to v8086 or userspace
       754	
       755	ENTRY(resume_userspace)
       756		DISABLE_INTERRUPTS(CLBR_ANY)
       757		TRACE_IRQS_OFF
       758		movl	%esp, %eax
       759		call	prepare_exit_to_usermode
       760		jmp	restore_all
       761	END(ret_from_exception)
       762	
       763	#ifdef CONFIG_PREEMPT
       764	ENTRY(resume_kernel)
       765		DISABLE_INTERRUPTS(CLBR_ANY)
       766	.Lneed_resched:
       767		cmpl	$0, PER_CPU_VAR(__preempt_count)
       768		jnz	restore_all_kernel
==>    769		testl	$X86_EFLAGS_IF, PT_EFLAGS(%esp)	# interrupts off (exception path) ?       
       770		jz	restore_all_kernel
       771		call	preempt_schedule_irq
       772		jmp	.Lneed_resched
       773	END(resume_kernel)
       774	#endif
       775	
       776	GLOBAL(__begin_SYSENTER_singlestep_region)
       777	/*
       778	 * All code from here through __end_SYSENTER_singlestep_region is subject
       779	 * to being single-stepped if a user program sets TF and executes SYSENTER.
       780	 * There is absolutely nothing that we can do to prevent this from happening
       781	 * (thanks Intel!).  To keep our handling of this situation as simple as
       782	 * possible, we handle TF just like AC and NT, except that our #DB handler
       783	 * will ignore all of the single-step traps generated in this range.
       784	 */
       785	
       786	#ifdef CONFIG_XEN
       787	/*
       788	 * Xen doesn't set %esp to be precisely what the normal SYSENTER
       789	 * entry point expects, so fix it up before using the normal path.

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:199
       179	 * away soon, so while the target thread will be woken up, it will not
       180	 * be migrated to another CPU - ie. the two threads are 'synchronized'
       181	 * with each other. This can prevent needless bouncing between CPUs.
       182	 *
       183	 * On UP it can prevent extra preemption.
       184	 *
       185	 * If this function wakes up a task, it executes a full memory barrier before
       186	 * accessing the task state.
       187	 */
       188	void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
       189				int nr_exclusive, void *key)
       190	{
       191		int wake_flags = 1; /* XXX WF_SYNC */
       192	
       193		if (unlikely(!wq_head))
       194			return;
       195	
       196		if (unlikely(nr_exclusive != 1))
       197			wake_flags = 0;
       198	
==>    199		__wake_up_common_lock(wq_head, mode, nr_exclusive, wake_flags, key);       
       200	}
       201	EXPORT_SYMBOL_GPL(__wake_up_sync_key);
       202	
       203	/*
       204	 * __wake_up_sync - see __wake_up_sync_key()
       205	 */
       206	void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode, int nr_exclusive)
       207	{
       208		__wake_up_sync_key(wq_head, mode, nr_exclusive, NULL);
       209	}
       210	EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
       211	
       212	/*
       213	 * Note: we use "set_current_state()" _after_ the wait-queue add,
       214	 * because we need a memory barrier there on SMP, so that any
       215	 * wake-function that tests for the wait-queue being active
       216	 * will be guaranteed to see waitqueue addition _or_ subsequent
       217	 * tests in this thread will see the wakeup having taken place.
       218	 *
       219	 * The spin_unlock() itself is semi-permeable and only protects

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:831
       811		)
       812	
       813		iov_iter_advance(i, bytes);
       814		return true;
       815	}
       816	EXPORT_SYMBOL(_copy_from_iter_full_nocache);
       817	
       818	static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
       819	{
       820		struct page *head = compound_head(page);
       821		size_t v = n + offset + page_address(page) - page_address(head);
       822	
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
==>    831	{       
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
       843	}
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:370
       350	
       351		__set_current_state(TASK_RUNNING);
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
==>    370	}       
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
       381	}
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:843
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
       831	{
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
==>    843	}       
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {
       852			WARN_ON(1);
       853			return 0;
       854		}
       855		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       856			void *kaddr = kmap_atomic(page);
       857			size_t wanted = _copy_from_iter(kaddr + offset, bytes, i);
       858			kunmap_atomic(kaddr);
       859			return wanted;
       860		} else
       861			return copy_page_from_iter_iovec(page, offset, bytes, i);
       862	}
       863	EXPORT_SYMBOL(copy_page_from_iter);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 1866	Addresses: c2892c8d c11093ce c2892899 c2892c36 c289299d c11093c8 c289294f
1	0xc2892c26: __preempt_count_add at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/preempt.h:76
6	0xc2892c8d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
9	0xc11093cc: virt_spin_lock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/qspinlock.h:65
42	0xc289299d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
52	0xc2892889: __preempt_count_add at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/preempt.h:76
843	0xc11093b8: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/locking/qspinlock.c:295
913	0xc289294f: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684

1	c2892c36 arch_atomic_cmpxchg T: trace_20241118_120256_5_3_61.txt S: 61 I1: 5 I2: 3 IP1: c2892c36 IP2: c11093c8 PMA1: 35614200 PMA2: 35614200 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 187482 IC2: 156136
6	c2892c8d pv_queued_spin_unlock T: trace_20241118_120256_5_3_61.txt S: 61 I1: 5 I2: 3 IP1: c2892c8d IP2: c11093c8 PMA1: 35614200 PMA2: 35614200 CPU1: 1 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 187594 IC2: 187497
9	c11093ce arch_atomic_cmpxchg T: trace_20241118_120318_2_5_26.txt S: 26 I1: 2 I2: 5 IP1: c11093ce IP2: c289294f PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 1 IC1: 131894 IC2: 131889
42	c289299d pv_queued_spin_unlock T: trace_20241118_120308_4_2_56.txt S: 56 I1: 4 I2: 2 IP1: c289299d IP2: c11093c8 PMA1: 355d8200 PMA2: 355d8200 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 353985 IC2: 352522
52	c2892899 arch_atomic_cmpxchg T: trace_20241118_120320_2_5_59.txt S: 59 I1: 2 I2: 5 IP1: c2892899 IP2: c289294f PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 3 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 1 IC1: 193780 IC2: 171960
843	c11093c8 __read_once_size T: trace_20241118_120318_2_5_31.txt S: 31 I1: 2 I2: 5 IP1: c289294f IP2: c11093c8 PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 131889 IC2: 131838
913	c289294f pv_queued_spin_unlock T: trace_20241118_120320_2_5_59.txt S: 59 I1: 2 I2: 5 IP1: c289294f IP2: c289294f PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 3 CPU2: 1 R1: 0 R2: 0 L1: 1 L2: 1 IC1: 193845 IC2: 171960

++++++++++++++++++++++++
STATS: Distinct IPs: 28 Distinct pairs: 33 Distinct clusters: 6
++++++++++++++++++++++++
/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/qspinlock.h:65
        45	#endif
        46	
        47	#ifdef CONFIG_PARAVIRT
        48	DECLARE_STATIC_KEY_TRUE(virt_spin_lock_key);
        49	
        50	void native_pv_lock_init(void) __init;
        51	
        52	#define virt_spin_lock virt_spin_lock
        53	static inline bool virt_spin_lock(struct qspinlock *lock)
        54	{
        55		if (!static_branch_likely(&virt_spin_lock_key))
        56			return false;
        57	
        58		/*
        59		 * On hypervisors without PARAVIRT_SPINLOCKS support we fall
        60		 * back to a Test-and-Set spinlock, because fair locks have
        61		 * horrible lock 'holder' preemption issues.
        62		 */
        63	
        64		do {
==>     65			while (atomic_read(&lock->val) != 0)       
        66				cpu_relax();
        67		} while (atomic_cmpxchg(&lock->val, 0, _Q_LOCKED_VAL) != 0);
        68	
        69		return true;
        70	}
        71	#else
        72	static inline void native_pv_lock_init(void)
        73	{
        74	}
        75	#endif /* CONFIG_PARAVIRT */
        76	
        77	#include <asm-generic/qspinlock.h>
        78	
        79	#endif /* _ASM_X86_QSPINLOCK_H */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/qspinlock.c:295
       275	 * @lock: Pointer to queued spinlock structure
       276	 * @val: Current value of the queued spinlock 32-bit word
       277	 *
       278	 * (queue tail, pending bit, lock value)
       279	 *
       280	 *              fast     :    slow                                  :    unlock
       281	 *                       :                                          :
       282	 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
       283	 *                       :       | ^--------.------.             /  :
       284	 *                       :       v           \      \            |  :
       285	 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
       286	 *                       :       | ^--'              |           |  :
       287	 *                       :       v                   |           |  :
       288	 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
       289	 *   queue               :       | ^--'                          |  :
       290	 *                       :       v                               |  :
       291	 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
       292	 *   queue               :         ^--'                             :
       293	 */
       294	void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
==>    295	{       
       296		struct mcs_spinlock *prev, *next, *node;
       297		u32 old, tail;
       298		int idx;
       299	
       300		BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
       301	
       302		if (pv_enabled())
       303			goto pv_queue;
       304	
       305		if (virt_spin_lock(lock))
       306			return;
       307	
       308		/*
       309		 * Wait for in-progress pending->locked hand-overs with a bounded
       310		 * number of spins so that we guarantee forward progress.
       311		 *
       312		 * 0,1,0 -> 0,0,1
       313		 */
       314		if (val == _Q_PENDING_VAL) {
       315			int cnt = _Q_PENDING_LOOPS;

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32


