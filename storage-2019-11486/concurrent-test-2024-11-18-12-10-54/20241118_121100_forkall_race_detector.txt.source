vmlinux map is loaded
Waiting for data race records

('Analyed ', 500, 566, ' data races in total')

====================================================================================================================================================================================
Total: 12	Addresses: c10e82c9 c10eb2ba
6	0xc10e82c1: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/sched/core.c:2671
6	0xc10eb2b8: rep_nop at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/processor.h:665

6	c10e82c9 __write_once_size T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 142170 IC2: 141553
6	c10eb2ba __read_once_size T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c10e82c9 IP2: c10eb2ba PMA1: 2f588a20 PMA2: 2f588a20 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 4 L2: 4 IC1: 142170 IC2: 141553

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/core.c:2671
      2651		 * Also, see FORK_PREEMPT_COUNT.
      2652		 */
      2653		if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
      2654			      "corrupted preempt_count: %s/%d/0x%x\n",
      2655			      current->comm, current->pid, preempt_count()))
      2656			preempt_count_set(FORK_PREEMPT_COUNT);
      2657	
      2658		rq->prev_mm = NULL;
      2659	
      2660		/*
      2661		 * A task struct has one reference for the use as "current".
      2662		 * If a task dies, then it sets TASK_DEAD in tsk->state and calls
      2663		 * schedule one last time. The schedule call will never return, and
      2664		 * the scheduled task must drop that reference.
      2665		 *
      2666		 * We must observe prev->state before clearing prev->on_cpu (in
      2667		 * finish_task), otherwise a concurrent wakeup can get prev
      2668		 * running on another CPU and we could rave with its RUNNING -> DEAD
      2669		 * transition, resulting in a double drop.
      2670		 */
==>   2671		prev_state = prev->state;       
      2672		vtime_task_switch(prev);
      2673		perf_event_task_sched_in(prev, current);
      2674		finish_task(prev);
      2675		finish_lock_switch(rq);
      2676		finish_arch_post_lock_switch();
      2677		kcov_finish_switch(current);
      2678	
      2679		fire_sched_in_preempt_notifiers(current);
      2680		/*
      2681		 * When switching through a kernel thread, the loop in
      2682		 * membarrier_{private,global}_expedited() may have observed that
      2683		 * kernel thread and not issued an IPI. It is therefore possible to
      2684		 * schedule between user->kernel->user threads without passing though
      2685		 * switch_mm(). Membarrier requires a barrier after storing to
      2686		 * rq->curr, before returning to userspace, so provide them here:
      2687		 *
      2688		 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
      2689		 *   provided by mmdrop(),
      2690		 * - a sync_core for SYNC_CORE.
      2691		 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/processor.h:665
       645	{
       646		unsigned int eax, ebx, ecx, edx;
       647	
       648		cpuid(op, &eax, &ebx, &ecx, &edx);
       649	
       650		return ecx;
       651	}
       652	
       653	static inline unsigned int cpuid_edx(unsigned int op)
       654	{
       655		unsigned int eax, ebx, ecx, edx;
       656	
       657		cpuid(op, &eax, &ebx, &ecx, &edx);
       658	
       659		return edx;
       660	}
       661	
       662	/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
       663	static __always_inline void rep_nop(void)
       664	{
==>    665		asm volatile("rep; nop" ::: "memory");       
       666	}
       667	
       668	static __always_inline void cpu_relax(void)
       669	{
       670		rep_nop();
       671	}
       672	
       673	/*
       674	 * This function forces the icache and prefetched instruction stream to
       675	 * catch up with reality in two very specific cases:
       676	 *
       677	 *  a) Text was modified using one virtual address and is about to be executed
       678	 *     from the same physical page at a different virtual address.
       679	 *
       680	 *  b) Text was modified on a different CPU, may subsequently be
       681	 *     executed on this CPU, and you want to make sure the new version
       682	 *     gets executed.  This generally means you're calling this in a IPI.
       683	 *
       684	 * If you're calling this for a different reason, you're probably doing
       685	 * it wrong.


====================================================================================================================================================================================
Total: 16	Addresses: c288f53e c288f721
4	0xc288f537: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15
12	0xc288f718: get_current at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/current.h:15

4	c288f53e arch_atomic_try_cmpxchg T: trace_20241118_121201_4_3_61.txt S: 61 I1: 4 I2: 3 IP1: c288f53e IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 604747 IC2: 558638
12	c288f721 arch_atomic_cmpxchg T: trace_20241118_121201_4_3_61.txt S: 61 I1: 4 I2: 3 IP1: c288f721 IP2: c288f721 PMA1: 2f447f80 PMA2: 2f447f80 CPU1: 0 CPU2: 1 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 604935 IC2: 558638

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/current.h:15
         1	/* SPDX-License-Identifier: GPL-2.0 */
         2	#ifndef _ASM_X86_CURRENT_H
         3	#define _ASM_X86_CURRENT_H
         4	
         5	#include <linux/compiler.h>
         6	#include <asm/percpu.h>
         7	
         8	#ifndef __ASSEMBLY__
         9	struct task_struct;
        10	
        11	DECLARE_PER_CPU(struct task_struct *, current_task);
        12	
        13	static __always_inline struct task_struct *get_current(void)
        14	{
==>     15		return this_cpu_read_stable(current_task);       
        16	}
        17	
        18	#define current get_current()
        19	
        20	#endif /* __ASSEMBLY__ */
        21	
        22	#endif /* _ASM_X86_CURRENT_H */


====================================================================================================================================================================================
Total: 50	Addresses: c11038da c1103f5b
25	0xc11038d3: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:351
25	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

25	c11038da list_empty_careful T: trace_20241118_121157_5_5_38.txt S: 38 I1: 5 I2: 5 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 131312 IC2: 127262
25	c1103f5b __write_once_size T: trace_20241118_121157_5_5_38.txt S: 38 I1: 5 I2: 5 IP1: c11038da IP2: c1103f5b PMA1: 307d3e88 PMA2: 307d3e88 CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 131312 IC2: 127262

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:351
       331		schedule();
       332		spin_lock_irq(&wq->lock);
       333	
       334		return 0;
       335	}
       336	EXPORT_SYMBOL(do_wait_intr_irq);
       337	
       338	/**
       339	 * finish_wait - clean up after waiting in a queue
       340	 * @wq_head: waitqueue waited on
       341	 * @wq_entry: wait descriptor
       342	 *
       343	 * Sets current thread back to running state and removes
       344	 * the wait descriptor from the given waitqueue if still
       345	 * queued.
       346	 */
       347	void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
       348	{
       349		unsigned long flags;
       350	
==>    351		__set_current_state(TASK_RUNNING);       
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 104	Addresses: c1957205 c1103bb3 c1103f5e c1103920 c1957356
13	0xc1957205: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:831
13	0xc1103bb3: __wake_up_sync_key at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:199
13	0xc110390e: finish_wait at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:370
13	0xc1957356: copy_page_to_iter at /root/2019-6974-i386/linux-4.19/lib/iov_iter.c:843
52	0xc1103f49: autoremove_wake_function at /root/2019-6974-i386/linux-4.19/kernel/sched/wait.c:381

13	c1957205 copy_page_to_iter T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c1957205 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 116104 IC2: 111983
13	c1103bb3 __wake_up_sync_key T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 116809 IC2: 111983
13	c1103920 list_empty_careful T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c1103920 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 116035 IC2: 111983
13	c1957356 copy_page_to_iter T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c1957356 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 1 R2: 0 L1: 4 L2: 4 IC1: 116706 IC2: 111983
52	c1103f5e INIT_LIST_HEAD T: trace_20241118_121144_2_2_63.txt S: 63 I1: 2 I2: 2 IP1: c1103bb3 IP2: c1103f5e PMA1: 307d3e8c PMA2: 307d3e8c CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 4 IC1: 116809 IC2: 111983

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:831
       811		)
       812	
       813		iov_iter_advance(i, bytes);
       814		return true;
       815	}
       816	EXPORT_SYMBOL(_copy_from_iter_full_nocache);
       817	
       818	static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
       819	{
       820		struct page *head = compound_head(page);
       821		size_t v = n + offset + page_address(page) - page_address(head);
       822	
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
==>    831	{       
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
       843	}
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:199
       179	 * away soon, so while the target thread will be woken up, it will not
       180	 * be migrated to another CPU - ie. the two threads are 'synchronized'
       181	 * with each other. This can prevent needless bouncing between CPUs.
       182	 *
       183	 * On UP it can prevent extra preemption.
       184	 *
       185	 * If this function wakes up a task, it executes a full memory barrier before
       186	 * accessing the task state.
       187	 */
       188	void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
       189				int nr_exclusive, void *key)
       190	{
       191		int wake_flags = 1; /* XXX WF_SYNC */
       192	
       193		if (unlikely(!wq_head))
       194			return;
       195	
       196		if (unlikely(nr_exclusive != 1))
       197			wake_flags = 0;
       198	
==>    199		__wake_up_common_lock(wq_head, mode, nr_exclusive, wake_flags, key);       
       200	}
       201	EXPORT_SYMBOL_GPL(__wake_up_sync_key);
       202	
       203	/*
       204	 * __wake_up_sync - see __wake_up_sync_key()
       205	 */
       206	void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode, int nr_exclusive)
       207	{
       208		__wake_up_sync_key(wq_head, mode, nr_exclusive, NULL);
       209	}
       210	EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
       211	
       212	/*
       213	 * Note: we use "set_current_state()" _after_ the wait-queue add,
       214	 * because we need a memory barrier there on SMP, so that any
       215	 * wake-function that tests for the wait-queue being active
       216	 * will be guaranteed to see waitqueue addition _or_ subsequent
       217	 * tests in this thread will see the wakeup having taken place.
       218	 *
       219	 * The spin_unlock() itself is semi-permeable and only protects

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:370
       350	
       351		__set_current_state(TASK_RUNNING);
       352		/*
       353		 * We can check for list emptiness outside the lock
       354		 * IFF:
       355		 *  - we use the "careful" check that verifies both
       356		 *    the next and prev pointers, so that there cannot
       357		 *    be any half-pending updates in progress on other
       358		 *    CPU's that we haven't seen yet (and that might
       359		 *    still change the stack area.
       360		 * and
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
==>    370	}       
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
       381	}
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//lib/iov_iter.c:843
       823		if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
       824			return true;
       825		WARN_ON(1);
       826		return false;
       827	}
       828	
       829	size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
       830				 struct iov_iter *i)
       831	{
       832		if (unlikely(!page_copy_sane(page, offset, bytes)))
       833			return 0;
       834		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       835			void *kaddr = kmap_atomic(page);
       836			size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
       837			kunmap_atomic(kaddr);
       838			return wanted;
       839		} else if (likely(!(i->type & ITER_PIPE)))
       840			return copy_page_to_iter_iovec(page, offset, bytes, i);
       841		else
       842			return copy_page_to_iter_pipe(page, offset, bytes, i);
==>    843	}       
       844	EXPORT_SYMBOL(copy_page_to_iter);
       845	
       846	size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
       847				 struct iov_iter *i)
       848	{
       849		if (unlikely(!page_copy_sane(page, offset, bytes)))
       850			return 0;
       851		if (unlikely(i->type & ITER_PIPE)) {
       852			WARN_ON(1);
       853			return 0;
       854		}
       855		if (i->type & (ITER_BVEC|ITER_KVEC)) {
       856			void *kaddr = kmap_atomic(page);
       857			size_t wanted = _copy_from_iter(kaddr + offset, bytes, i);
       858			kunmap_atomic(kaddr);
       859			return wanted;
       860		} else
       861			return copy_page_from_iter_iovec(page, offset, bytes, i);
       862	}
       863	EXPORT_SYMBOL(copy_page_from_iter);

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/sched/wait.c:381
       361		 *  - all other users take the lock (ie we can only
       362		 *    have _one_ other CPU that looks at or modifies
       363		 *    the list).
       364		 */
       365		if (!list_empty_careful(&wq_entry->entry)) {
       366			spin_lock_irqsave(&wq_head->lock, flags);
       367			list_del_init(&wq_entry->entry);
       368			spin_unlock_irqrestore(&wq_head->lock, flags);
       369		}
       370	}
       371	EXPORT_SYMBOL(finish_wait);
       372	
       373	int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
       374	{
       375		int ret = default_wake_function(wq_entry, mode, sync, key);
       376	
       377		if (ret)
       378			list_del_init(&wq_entry->entry);
       379	
       380		return ret;
==>    381	}       
       382	EXPORT_SYMBOL(autoremove_wake_function);
       383	
       384	static inline bool is_kthread_should_stop(void)
       385	{
       386		return (current->flags & PF_KTHREAD) && kthread_should_stop();
       387	}
       388	
       389	/*
       390	 * DEFINE_WAIT_FUNC(wait, woken_wake_func);
       391	 *
       392	 * add_wait_queue(&wq_head, &wait);
       393	 * for (;;) {
       394	 *     if (condition)
       395	 *         break;
       396	 *
       397	 *     // in wait_woken()			// in woken_wake_function()
       398	 *
       399	 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;
       400	 *     smp_mb(); // A				try_to_wake_up():
       401	 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>


====================================================================================================================================================================================
Total: 950	Addresses: c2892c8d c11093ce c2892899 c289299d c11093c8 c289294f
3	0xc2892c8d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
7	0xc289299d: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684
8	0xc11093cc: virt_spin_lock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/qspinlock.h:65
96	0xc2892889: __preempt_count_add at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/preempt.h:76
319	0xc11093b8: arch_static_branch at /root/2019-6974-i386/linux-4.19/kernel/locking/qspinlock.c:295
517	0xc289294f: pv_queued_spin_unlock at /root/2019-6974-i386/linux-4.19/./arch/x86/include/asm/paravirt.h:684

3	c2892c8d pv_queued_spin_unlock T: trace_20241118_121132_2_5_47.txt S: 47 I1: 2 I2: 5 IP1: c2892c8d IP2: c11093c8 PMA1: 34815b44 PMA2: 34815b44 CPU1: 0 CPU2: 1 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 138487 IC2: 138451
7	c289299d pv_queued_spin_unlock T: trace_20241118_121144_2_2_58.txt S: 58 I1: 2 I2: 2 IP1: c289299d IP2: c11093c8 PMA1: 355d8200 PMA2: 355d8200 CPU1: 1 CPU2: 0 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 354516 IC2: 353048
8	c11093ce arch_atomic_cmpxchg T: trace_20241118_121134_3_3_18.txt S: 18 I1: 3 I2: 3 IP1: c11093ce IP2: c289294f PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 1 IC1: 131641 IC2: 131636
96	c2892899 arch_atomic_cmpxchg T: trace_20241118_121136_3_3_60.txt S: 60 I1: 3 I2: 3 IP1: c2892899 IP2: c289294f PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 3 CPU2: 0 R1: 0 R2: 0 L1: 4 L2: 1 IC1: 149129 IC2: 127309
319	c11093c8 __read_once_size T: trace_20241118_121203_5_2_38.txt S: 38 I1: 5 I2: 2 IP1: c289294f IP2: c11093c8 PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 131921 IC2: 131870
517	c289294f pv_queued_spin_unlock T: trace_20241118_121203_5_2_38.txt S: 38 I1: 5 I2: 2 IP1: c289294f IP2: c11093c8 PMA1: 2f447f94 PMA2: 2f447f94 CPU1: 0 CPU2: 3 R1: 0 R2: 1 L1: 1 L2: 4 IC1: 131921 IC2: 131870

++++++++++++++++++++++++
STATS: Distinct IPs: 17 Distinct pairs: 14 Distinct clusters: 5
++++++++++++++++++++++++
/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/qspinlock.h:65
        45	#endif
        46	
        47	#ifdef CONFIG_PARAVIRT
        48	DECLARE_STATIC_KEY_TRUE(virt_spin_lock_key);
        49	
        50	void native_pv_lock_init(void) __init;
        51	
        52	#define virt_spin_lock virt_spin_lock
        53	static inline bool virt_spin_lock(struct qspinlock *lock)
        54	{
        55		if (!static_branch_likely(&virt_spin_lock_key))
        56			return false;
        57	
        58		/*
        59		 * On hypervisors without PARAVIRT_SPINLOCKS support we fall
        60		 * back to a Test-and-Set spinlock, because fair locks have
        61		 * horrible lock 'holder' preemption issues.
        62		 */
        63	
        64		do {
==>     65			while (atomic_read(&lock->val) != 0)       
        66				cpu_relax();
        67		} while (atomic_cmpxchg(&lock->val, 0, _Q_LOCKED_VAL) != 0);
        68	
        69		return true;
        70	}
        71	#else
        72	static inline void native_pv_lock_init(void)
        73	{
        74	}
        75	#endif /* CONFIG_PARAVIRT */
        76	
        77	#include <asm-generic/qspinlock.h>
        78	
        79	#endif /* _ASM_X86_QSPINLOCK_H */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/preempt.h:76
        56	{
        57		raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);
        58	}
        59	
        60	static __always_inline void clear_preempt_need_resched(void)
        61	{
        62		raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);
        63	}
        64	
        65	static __always_inline bool test_preempt_need_resched(void)
        66	{
        67		return !(raw_cpu_read_4(__preempt_count) & PREEMPT_NEED_RESCHED);
        68	}
        69	
        70	/*
        71	 * The various preempt_count add/sub methods
        72	 */
        73	
        74	static __always_inline void __preempt_count_add(int val)
        75	{
==>     76		raw_cpu_add_4(__preempt_count, val);       
        77	}
        78	
        79	static __always_inline void __preempt_count_sub(int val)
        80	{
        81		raw_cpu_add_4(__preempt_count, -val);
        82	}
        83	
        84	/*
        85	 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
        86	 * a decrement which hits zero means we have no preempt_count and should
        87	 * reschedule.
        88	 */
        89	static __always_inline bool __preempt_count_dec_and_test(void)
        90	{
        91		GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), e);
        92	}
        93	
        94	/*
        95	 * Returns true when we need to resched and can (barring IRQ state).
        96	 */

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//kernel/locking/qspinlock.c:295
       275	 * @lock: Pointer to queued spinlock structure
       276	 * @val: Current value of the queued spinlock 32-bit word
       277	 *
       278	 * (queue tail, pending bit, lock value)
       279	 *
       280	 *              fast     :    slow                                  :    unlock
       281	 *                       :                                          :
       282	 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
       283	 *                       :       | ^--------.------.             /  :
       284	 *                       :       v           \      \            |  :
       285	 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
       286	 *                       :       | ^--'              |           |  :
       287	 *                       :       v                   |           |  :
       288	 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
       289	 *   queue               :       | ^--'                          |  :
       290	 *                       :       v                               |  :
       291	 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
       292	 *   queue               :         ^--'                             :
       293	 */
       294	void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
==>    295	{       
       296		struct mcs_spinlock *prev, *next, *node;
       297		u32 old, tail;
       298		int idx;
       299	
       300		BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
       301	
       302		if (pv_enabled())
       303			goto pv_queue;
       304	
       305		if (virt_spin_lock(lock))
       306			return;
       307	
       308		/*
       309		 * Wait for in-progress pending->locked hand-overs with a bounded
       310		 * number of spins so that we guarantee forward progress.
       311		 *
       312		 * 0,1,0 -> 0,0,1
       313		 */
       314		if (val == _Q_PENDING_VAL) {
       315			int cnt = _Q_PENDING_LOOPS;

/root/snowboard-2019-6974/testsuite/kernel/2019-6974/source//./arch/x86/include/asm/paravirt.h:684
       664	{
       665		PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
       666	}
       667	
       668	static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
       669					phys_addr_t phys, pgprot_t flags)
       670	{
       671		pv_mmu_ops.set_fixmap(idx, phys, flags);
       672	}
       673	
       674	#if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
       675	
       676	static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
       677								u32 val)
       678	{
       679		PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
       680	}
       681	
       682	static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
       683	{
==>    684		PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);       
       685	}
       686	
       687	static __always_inline void pv_wait(u8 *ptr, u8 val)
       688	{
       689		PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
       690	}
       691	
       692	static __always_inline void pv_kick(int cpu)
       693	{
       694		PVOP_VCALL1(pv_lock_ops.kick, cpu);
       695	}
       696	
       697	static __always_inline bool pv_vcpu_is_preempted(long cpu)
       698	{
       699		return PVOP_CALLEE1(bool, pv_lock_ops.vcpu_is_preempted, cpu);
       700	}
       701	
       702	#endif /* SMP && PARAVIRT_SPINLOCKS */
       703	
       704	#ifdef CONFIG_X86_32


